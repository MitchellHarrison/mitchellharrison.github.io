[
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Here is a place to browse my tutorials! Feel free to browse or sort by category on the right side of the page. If you have any questions or ideas for new topics, let me know on Discord!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHello, statistics. | Mathematical Statistics 0\n\n\n\nStatistics\n\n\nMathematical Statistics\n\n\n\n\n\n\n\nMitch Harrison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to Estimators! | Mathematical Statistics 1\n\n\n\nStatistics\n\n\nMathematical Statistics\n\n\n\n\n\n\n\nMitch Harrison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Bias-Variance Tradeoff | Mathematical Statistics 2\n\n\n\nStatistics\n\n\nMathematical Statistics\n\n\n\n\n\n\n\nMitch Harrison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJensen’s Inequality | Mathematical Statistics 3\n\n\n\nStatistics\n\n\nMathematical Statistics\n\n\n\n\n\n\n\nMitch Harrison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur World in Emissions | TidyTutorial\n\n\n\nData Viz\n\n\n\n\n\n\n\nMitch Harrison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCampus Pride Index | TidyTutorial\n\n\n\nData Viz\n\n\n\n\n\n\n\nMitch Harrison\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/tidytuesday_05212024.html",
    "href": "tutorials/tidytuesday_05212024.html",
    "title": "Our World in Emissions | TidyTutorial",
    "section": "",
    "text": "Welcome! If you saw my post for this week’s TidyTuesday, I’m glad you liked it enough to learn from it! If not, you can either scroll to the bottom to see the final product or click here. For this plot, we will use an area plot to visualize the global emissions by type going back to 1900. To start, we will use a bare-bones ggplot2 area chart with no bells or whistles to see what we are working with.\n\n\nClick here for code\nlibrary(tidyverse)\n\n# read data and rename an ugly column ------------------------------------------\nemis&lt;- read_csv(paste0(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/\",\n  \"data/2024/2024-05-21/emissions.csv\"\n  )\n)\n\nemis &lt;- emis |&gt;\n  rename(emissions = \"total_emissions_MtCO2e\")\n\nemis |&gt;\n  group_by(year, commodity) |&gt;\n  summarise(emissions = sum(emissions), .groups = \"drop\") |&gt;\n  \n  # start of plot --------------------------------------------------------------\n  ggplot(aes(x = year, y = emissions, fill = commodity)) +\n  geom_area(alpha = 0.9)\n\n\n\n\n\n\n\n\n\nOkay, we’ve learned a lot. First, there are a lot of categories. A good rule of thumb is that once you get to about seven colors, even non-colorblind humans struggle to differentiate. But there is hope! Notice that there are several types of coal production. Let’s aggregate them. Second, there is a long tail on the left because of near-zero data. Let’s bring our limit to the right to get a better look.\n\n\nClick here for code\nemis |&gt;\n  filter(year &gt;= 1900) |&gt; # get rid of that tail\n  mutate(\n    # aggregate coal\n    commodity = if_else(str_detect(commodity, \"Coal\"), \"Coal\", commodity),\n  ) |&gt;\n  group_by(year, commodity) |&gt;\n  summarise(emissions = sum(emissions), .groups = \"drop\") |&gt;\n  \n  # start of plot --------------------------------------------------------------\n  ggplot(aes(x = year, y = emissions, fill = commodity)) +\n  geom_area(alpha = 0.9)\n\n\n\n\n\n\n\n\n\nMuch better! But to me, having the smallest category (cement) on top feels awkward. Let’s reorder the categories! I’ll do so in descending order of emissions in the last year.\n\n\nClick here for code\nLEVS &lt;- c(\"Coal\", \"Oil & NGL\", \"Natural Gas\", \"Cement\") # our desired order\n\nemis |&gt;\n  filter(year &gt;= 1900) |&gt;\n  mutate(\n    commodity = if_else(str_detect(commodity, \"Coal\"), \"Coal\", commodity),\n    commodity = factor(commodity, levels = LEVS) # re-order \n  ) |&gt;\n  group_by(year, commodity) |&gt;\n  summarise(emissions = sum(emissions), .groups = \"drop\") |&gt;\n  \n  # start of plot --------------------------------------------------------------\n  ggplot(aes(x = year, y = emissions, fill = commodity)) +\n  geom_area(alpha = 0.9)\n\n\n\n\n\n\n\n\n\nNow we’re cooking! It’s time for some style points. I’ll use my favorite aesthetic cheat code: ggthemes. Let’s add a theme and color scheme. I’m going with the FiveThirtyEight theme and a colorblind-friendly palette. I’ll also take this opportunity to adjust the opacity down just a touch. This is a personal choice, but I find it nice to be able to see the grid behind such ink-heavy plots as area plots.\n\n\n\n\n\n\nImportant\n\n\n\nRemember: unless you are making plots for a very small number of people and you know for certain that none are colorblind, making inaccessible plots is inexcusable. Of course, we all make mistakes, so if you ever notice an accessibility issue on my site, reach out and let me know on Discord or via a GitHub issue so I can improve for next time!\n\n\n\n\nClick here for code\nLEVS &lt;- c(\"Coal\", \"Oil & NGL\", \"Natural Gas\", \"Cement\") # our desired order\n\nemis |&gt;\n  filter(year &gt;= 1900) |&gt;\n  mutate(\n    commodity = if_else(str_detect(commodity, \"Coal\"), \"Coal\", commodity),\n    commodity = factor(commodity, levels = LEVS) # re-order \n  ) |&gt;\n  group_by(year, commodity) |&gt;\n  summarise(emissions = sum(emissions), .groups = \"drop\") |&gt;\n  \n  # start of plot --------------------------------------------------------------\n  ggplot(aes(x = year, y = emissions, fill = commodity)) +\n  geom_area(alpha = 0.9) + # drop the opacity just a touch\n\n  # add theme and colors (love you, ggthemes) \n  ggthemes::scale_fill_colorblind() +\n  ggthemes::theme_fivethirtyeight()\n\n\n\n\n\n\n\n\n\nAnd just like that, it feels like we are almost there! Let’s change a few things at once. We will change the background color, add the title/subtitle/axis labels/caption, and format the \\(y\\)-axis to read 30k instead of 30000. That will give us a feel for the final color scheme and how the fonts feel on the page. Because of the subscript “2” in \\(CO_2\\), I will use the latex2exp package use \\(\\LaTeX\\) typesetting in the plot.\n\n\n\n\n\n\nNote\n\n\n\nOne note that is unique to this plot. When we use theme_fivethirtyeight, it removes the \\(y\\)-axis title. So, although we normally wouldn’t have to explicitly set the axis title to element_text in the theme function, we will here.\n\n\n\n\nClick here for code\nLEVS &lt;- c(\"Coal\", \"Oil & NGL\", \"Natural Gas\", \"Cement\")\nBG_COLOR &lt;- \"#F0F0F0\" # this will be our background color\n\nemis |&gt;\n  filter(year &gt;= 1900) |&gt;\n  mutate(\n    commodity = if_else(str_detect(commodity, \"Coal\"), \"Coal\", commodity),\n    commodity = factor(commodity, levels = LEVS) \n  ) |&gt;\n  group_by(year, commodity) |&gt;\n  summarise(emissions = sum(emissions), .groups = \"drop\") |&gt;\n  \n  # start of plot --------------------------------------------------------------\n  ggplot(aes(x = year, y = emissions, fill = commodity)) +\n  geom_area(alpha = 0.9) +\n\n  ggthemes::scale_fill_colorblind() +\n  ggthemes::theme_fivethirtyeight() +\n\n  # abbreviate the y axis labels using the scales package\n  scale_y_continuous(label = scales::label_number(scale = 1e-3, suffix = \"k\")) +\n\n  # add labels to the plot -----------------------------------------------------\n  labs(\n    x = element_blank(),\n    y = latex2exp::TeX(\"Emissions ($MtCO_2e$)\"), # LaTeX typesetting with TeX()\n    title = \"Our World in Emissions\",\n    subtitle = latex2exp::TeX(\n      paste(\n        \"Emissions are measured in Millions of Tons of $CO_2$ equivalent\",\n        \"($MtCO_2e$)\"\n      )\n    ),\n    caption = paste(\n      \"Made with love by Mitch Harrison\",\n      # long blank line to \"hack\" a an annotation in the bottom-left corner\n      \"                                                                       \",\n      \"Source: Carbon Majors database and TidyTuesday\"\n    )\n  ) +\n  theme(\n    axis.title.y = element_text(size = 10),\n    plot.background = element_rect(fill = BG_COLOR) # change background color\n  ) \n\n\n\n\n\n\n\n\n\nYou could submit this plot for public consumption without shame, but we can do better! For example, I think we could safely remove the legend by annotating the colors directly on the plot. Let’s use a geom_text to do just that. While this entire process has been creative, we are getting into highly subjective territory here. So if you don’t like these changes, do something else! I would love to see your ideas.\nTo make the annotations, I want the text to be right-justified and directly atop one another. To accomplish that, I will give geom_text a single \\(x\\) value but several \\(y\\) values (one for each category).\n\n\nClick here for code\nLEVS &lt;- c(\"Coal\", \"Oil & NGL\", \"Natural Gas\", \"Cement\")\nBG_COLOR &lt;- \"#F0F0F0\"\n\nemis |&gt;\n  filter(year &gt;= 1900) |&gt;\n  mutate(\n    commodity = if_else(str_detect(commodity, \"Coal\"), \"Coal\", commodity),\n    commodity = factor(commodity, levels = LEVS) \n  ) |&gt;\n  group_by(year, commodity) |&gt;\n  summarise(emissions = sum(emissions), .groups = \"drop\") |&gt;\n  \n  # start of plot --------------------------------------------------------------\n  ggplot(aes(x = year, y = emissions, fill = commodity)) +\n  geom_area(alpha = 0.9) +\n\n  ggthemes::scale_fill_colorblind() +\n  ggthemes::theme_fivethirtyeight() +\n  scale_y_continuous(label = scales::label_number(scale = 1e-3, suffix = \"k\")) +\n\n  # add annotation text to replace the legend ----------------------------------\n  annotate(\n    geom = \"text\",\n    color = \"white\",\n    x = 2020,\n    y = c(1000, 4700, 13000, 26000),\n    label = c(\"Cement\", \"Natural Gas\", \"Oil & NGL\", \"Coal\"),\n    hjust = \"right\",\n    fontface = \"bold\"\n  ) +\n\n  labs(\n    x = element_blank(),\n    y = latex2exp::TeX(\"Emissions ($MtCO_2e$)\"),\n    title = \"Our World in Emissions\",\n    subtitle = latex2exp::TeX(\n      paste(\n        \"Emissions are measured in Millions of Tons of $CO_2$ equivalent\",\n        \"($MtCO_2e$)\"\n      )\n    ),\n    caption = paste(\n      \"Made with love by Mitch Harrison\",\n      \"                                                                       \",\n      \"Source: Carbon Majors database and TidyTuesday\"\n    )\n  ) +\n\n  theme(\n    legend.position = \"none\", # hide the legend\n    axis.title.y = element_text(size = 10),\n    plot.background = element_rect(fill = BG_COLOR)\n  ) \n\n\n\n\n\n\n\n\n\nNailed it. Now, I will happily take criticism here. I don’t love that the “Cement” label isn’t entirely encompassed by its data. But I think it’s much cleaner than having a legend drawing our eye away from the plot, so I’ll keep it.\nThe last thing we have to do before we can worry about the big annotation in the middle of the plot is change where the axes break. That is, set the years and emission amount displayed on the x and y axes, respectively. And while I’m at it, I will use a geom_hline to make the \\(x\\)-axis a bit bolder since it melts into the background a little bit too much for my liking.\n\n\nClick here for code\nLEVS &lt;- c(\"Coal\", \"Oil & NGL\", \"Natural Gas\", \"Cement\")\nBG_COLOR &lt;- \"#F0F0F0\"\nGRAY &lt;- \"gray35\"\n\nemis |&gt;\n  filter(year &gt;= 1900) |&gt;\n  mutate(\n    commodity = if_else(str_detect(commodity, \"Coal\"), \"Coal\", commodity),\n    commodity = factor(commodity, levels = LEVS) \n  ) |&gt;\n  group_by(year, commodity) |&gt;\n  summarise(emissions = sum(emissions), .groups = \"drop\") |&gt;\n  \n  # start of plot --------------------------------------------------------------\n  ggplot(aes(x = year, y = emissions, fill = commodity)) +\n  geom_area(alpha = 0.9) +\n\n  ggthemes::scale_fill_colorblind() +\n  ggthemes::theme_fivethirtyeight() +\n\n  # change where the axis breaks occur -----------------------------------------\n  scale_x_continuous(breaks = seq(1900, 2020, 20)) +\n  scale_y_continuous(\n    breaks = seq(0, 40000, 5000), \n    label = scales::label_number(scale = 1e-3, suffix = \"k\")\n  ) +\n\n  annotate(\n    geom = \"text\",\n    color = \"white\",\n    x = 2020,\n    y = c(1000, 4700, 13000, 26000),\n    label = c(\"Cement\", \"Natural Gas\", \"Oil & NGL\", \"Coal\"),\n    hjust = \"right\",\n    fontface = \"bold\"\n  ) +\n\n  labs(\n    x = element_blank(),\n    y = latex2exp::TeX(\"Emissions ($MtCO_2e$)\"),\n    title = \"Our World in Emissions\",\n    subtitle = latex2exp::TeX(\n      paste(\n        \"Emissions are measured in Millions of Tons of $CO_2$ equivalent\",\n        \"($MtCO_2e$)\"\n      )\n    ),\n    caption = paste(\n      \"Made with love by Mitch Harrison\",\n      \"                                                                       \",\n      \"Source: Carbon Majors database and TidyTuesday\"\n    )\n  ) +\n\n  geom_hline(yintercept = 0, linewidth = 0.7, color = GRAY) + # bold axis\n  theme(\n    legend.position = \"none\", # hide the legend\n    axis.title.y = element_text(size = 10),\n    plot.background = element_rect(fill = BG_COLOR)\n  ) \n\n\n\n\n\n\n\n\n\nOnce I write-in the line breaks, I’ll use the annotate function as before. But that’s not all. By default, there is no background with text annotations, so the grid overlaps the text and decreases legibility. To fix this, I’ll use annotate to put a rectangle the same color as the plot background behind the text, which “removes” the grid lines behind the text.\nFinally, to accomplish the arrow, we will use our final annotate to draw a line segment and put an arrowhead at the end.\n\n\n\n\n\n\nNote\n\n\n\nNormally, the order that we put things in a ggplot2 pipeline doesn’t matter. But here, if you put the background rectangle after the text annotation, it will cover the text, rendering it invisible.\n\n\nBecause this is our last edit, I will take this opportunity to make one very oft-forgotten change: write my alt text. Since you’re here, I know you respect the power of data communication. Alt text lets us communicate with those who sometimes miss out on learning from plots online. As our color palette did for colorblind viewers, we owe it to our non-sighted friends to let them participate.\nAnd finally, I’ll change the aspect ratio of the plot. You may have heard of the golden ratio, which is a ratio that many humans find inherently satisfying to look at. That ratio is approximately 1.618:1. The inverse of that number is 0.618, which will be our horizontal aspect ratio (1.618 is vertical). Because the quarto headers won’t render with the document, my final header is below:\n#| label: plt-final\n#| fig-width: 8\n#| fig-align: \"center\"\n#| fig-asp: 0.618\n#| fig-alt: |\n#|   This plot is titled Our World in Emissions. It is an area plot that shows\n#|   global emissions over time by type. The types are coal, natural gas,\n#|   cement, and oil and NGL. The plot notes that in 1995, the UN first met to\n#|   discuss the climate threat. The plot shows near-zero emissions from 1900 to\n#|   1920, when a slow increase begins. From there, emission growth seems to be\n#|   exponentially increasing, with no decline since the UN first met. Coal is\n#|   the largest emitter, then oil and NGL, then natural gas, and finally,\n#|   cement.\nNow, let’s see the plot!\n\n\nClick here for code\n# constants for ease of code legibility ----------------------------------------\nLEVS &lt;- c(\"Coal\", \"Oil & NGL\", \"Natural Gas\", \"Cement\")\nBG_COLOR &lt;- \"#F0F0F0\"\nUN_TEXT &lt;- paste(\n  \"In 1995, the United Nations\\nConference of the Parties met for\\nthe first\", \n  \"time to discuss the looming\\nthreat of climate change. The COP\\nhas\",\n  \"met twenty-eight times since.\"\n)\n\n# data cleanup -----------------------------------------------------------------\nemis |&gt;\n  filter(year &gt;= 1900) |&gt; # lots of near-zero space without this filter\n  mutate(\n    commodity = if_else(str_detect(commodity, \"Coal\"), \"Coal\", commodity),\n    commodity = factor(commodity, levels = LEVS) # re-order areas\n  ) |&gt;\n  group_by(year, commodity) |&gt;\n  summarise(emissions = sum(emissions), .groups = \"drop\") |&gt;\n  \n  # start of plot --------------------------------------------------------------\n  ggplot(aes(x = year, y = emissions, fill = commodity)) +\n  geom_area(alpha = 0.9) +\n  \n  # UN COP annotation text box -------------------------------------------------\n\n  # the arrow\n  annotate(\n    geom = \"segment\",\n    x = 1995,\n    xend = 1995,\n    y = 35500,\n    yend = 20500,\n    linetype = \"solid\",\n    linejoin = \"round\",\n    linewidth = 1,\n    color = \"grey35\",\n    arrow = arrow(type = \"closed\", length = unit(0.2, \"cm\"))\n  ) +\n\n  # the background rectangle (must be before the text)\n  annotate(\n    geom = \"rect\",\n    xmin = 1945.5,\n    xmax = 1993.5,\n    ymin = 23500,\n    ymax = 35800,\n    fill = BG_COLOR\n  ) +\n\n  # annotation text\n  annotate(\n    geom = \"text\",\n    x = 1992,\n    y = 30000,\n    label = UN_TEXT,\n    color = GRAY,\n    fontface = \"italic\",\n    hjust = \"right\"\n  ) +\n  \n  # replace legend with annotation text ----------------------------------------\n  annotate(\n    geom = \"text\",\n    color = \"white\",\n    x = 2020,\n    y = c(1000, 4700, 13000, 26000),\n    label = c(\"Cement\", \"Natural Gas\", \"Oil & NGL\", \"Coal\"),\n    hjust = \"right\",\n    fontface = \"bold\"\n  ) +\n  \n  # visual style elements (love you, ggthemes) ---------------------------------\n  ggthemes::scale_fill_colorblind() +\n  ggthemes::theme_fivethirtyeight() +\n  \n  # customize axis breaks and labels -------------------------------------------\n  scale_x_continuous(breaks = seq(1900, 2020, 20)) +\n  scale_y_continuous(\n    breaks = seq(0, 40000, 5000), \n    label = scales::label_number(scale = 1e-3, suffix = \"k\")\n  ) +\n  labs(\n    x = element_blank(),\n    y = latex2exp::TeX(\"Emissions ($MtCO_2e$)\"),\n    title = \"Our World in Emissions\",\n    subtitle = latex2exp::TeX(\n      paste(\n        \"Emissions are measured in Millions of Tons of $CO_2$ equivalent\",\n        \"($MtCO_2e$)\"\n      )\n    ),\n    caption = paste(\n      \"Made with love by Mitch Harrison\",\n      \"                                                                       \",\n      \"Source: Carbon Majors database and TidyTuesday\"\n    )\n  ) + \n  \n  # theme cleanup --------------------------------------------------------------\n  geom_hline(yintercept = 0, linewidth = 0.7, color = GRAY) + # bold axis\n  theme(\n    legend.position = \"none\", \n    axis.title.y = element_text(size = 10),\n    plot.background = element_rect(fill = BG_COLOR)\n  ) \n\n\n\n\n\n\n\n\n\nNo plot is perfect, but I am happy with what we have accomplished, and I hope you are too! If you have any questions or corrections, feel free to reach out on Discord, and I’ll be happy to help. And, of course, if you want to contribute to this effort financially, you are more than welcome to buy me a coffee.\nThanks for sticking around, and good luck with your TidyTuesday adventures!"
  },
  {
    "objectID": "tutorials/math_stat_2.html",
    "href": "tutorials/math_stat_2.html",
    "title": "The Bias-Variance Tradeoff | Mathematical Statistics 2",
    "section": "",
    "text": "Last time, we noted that we cannot precisely calculate bias or variance since they are conditional on the parameter we are estimating in the first place. If we knew that value, we wouldn’t need an estimator. Ideally, our estimators would be as close as possible to the true value of our unknown parameter \\(\\theta\\), but there may be infinitely many possible values for \\(\\theta\\). Instead, we will seek to be close to a range of possible values of \\(\\theta\\).\n\n\n\n\n\n\nDefinition\n\n\n\nCloseness describes how much we are willing to “pay” to be some “distance” away from the true value of \\(\\theta\\). We will measure this distance with a function (called a loss function), which is denoted as:\n\\[\n\\ell(\\theta, a).\n\\] This notation is somewhat confusing, because we are now using \\(a\\) to denote our estimate instead of \\(\\hat{\\theta}\\). So you can think of it as \\(\\ell(\\theta, \\hat{\\theta})\\) if that is more helpful (as it is for me).\n\n\nAs an example, if we choose a loss function,\n\\[\n\\ell(\\theta, \\hat{\\theta}) = (\\theta - \\hat{\\theta})^2,\n\\] then we have arrived at the squared-error loss function. Observe that the difference between our guess and the true value is the error, and we are squaring that value. If we take the mean of that function, we arrive at a critical value in mathematical statistics that you may have heard of: the mean-squared error (MSE).\n\n\n\n\n\n\nDefinition\n\n\n\nRisk is the expected loss for a given loss function \\(\\ell\\). Mathematically, it is denoted as \\[\nR_{\\delta}(\\theta) = \\mathbb{E}_{X|\\theta}[\\ell(\\theta, \\delta(\\mathbf{X}))],\n\\] where \\(\\delta(\\mathbf{X})\\) is the function that gives us our estimator. In effect, the risk is the expected loss given some loss function \\(\\ell(\\theta,\\hat{\\theta})\\).\n\n\nWe will get back to loss next time."
  },
  {
    "objectID": "tutorials/math_stat_2.html#worked-example-bias",
    "href": "tutorials/math_stat_2.html#worked-example-bias",
    "title": "The Bias-Variance Tradeoff | Mathematical Statistics 2",
    "section": "Worked example: bias",
    "text": "Worked example: bias\nLet’s work on our first example problem to nail some things down. Let our data \\(X_1, \\cdots, X_n \\sim N(\\mu, \\sigma^2)\\), and let each data point be independent and identically distributed (iid). Like last time, \\(\\sigma^2\\) is fixed and known. We are interested in estimating \\(\\mu\\). Recall our two estimators from the previous article:\n\\[\n\\begin{align*}\n\\delta_1(\\mathbf{X}) &= \\overline{X} \\\\\n\\delta_2(\\mathbf{X}) &= 5.\n\\end{align*}\n\\]\nThe expected value of \\(\\delta_2\\) is 5. Let’s find the expected value of \\(\\delta_1\\). Recall our generic formula for the expectation of an estimator is \\(\\mathbb{E}(\\delta(\\mathbf{X}) | \\theta)\\) for some estimator \\(\\delta\\) and true value \\(\\theta\\). In this case, our estimator is \\(\\overline{X}\\) and our parameter is \\(\\mu\\). Thus: \\[\n\\begin{align*}\n\\mathbb{E}(\\delta_1(\\mathbf{X})|\\theta) &= \\mathbb{E}(\\overline{X} | \\mu) \\\\\n&= \\mathbb{E}\\left[\\frac{\\sum_{i=1}^nX_i}{n}\\right]\n& \\text{expanding out }\\overline{X} \\\\\n&= \\frac{1}{n}\\sum_{i=1}^n\\mathbb{E}(X_i|\\mu)\n& \\text{expectation can be distributed into sums} \\\\\n&= \\frac{1}{n}\\sum_{i=1}^n\\mu\n& \\text{because } \\mathbb{E}(X_i) = \\mu \\text{ for any }i\\\\\n&= \\frac{n\\mu}{n} \\\\\n&= \\mu\n\\end{align*}\n\\]\nWe have a bias of zero! That means that the expected value of our estimator is precisely equivalent to our unknown parameter. It feels like we have solved statistics, right? Well… no. But we will get to that in a moment. For now, let’s visualize bias.\nLet’s say that the true (but unknown to us) value of \\(\\mu\\) is 1. One of our estimators was the constant 5. If we let 5 be our guess for the mean, we are saying, “I think the distribution is centered on 5,” when in reality, it is centered on 1. That difference is shown below.\n\n\nClick here for code\nlibrary(tidyverse)\n\nSIGMA &lt;- 1 # this is our fixed and known standard deviation\nMU &lt;- 1 # this is our true but unknown value\nMU_HAT &lt;- 5 # this is our estimator\n\nggplot() + \n  \n  # plot the true distribution\n  stat_function(\n    fun = dnorm,\n    args = list(mean = MU, sd = SIGMA),\n    color = \"coral3\",\n    linewidth = 2\n  ) +\n  geom_vline(xintercept = MU, color = \"coral3\", size = 1) +\n  annotate(\n    geom = \"text\",\n    hjust = \"right\",\n    x = -.1,\n    y = .3,\n    label = latex2exp::TeX(\"$N(1, \\\\sigma^2)$\"),\n    color = \"coral3\"\n  ) +\n  \n  # plot the estimated distribution\n  stat_function(\n    fun = dnorm,\n    args = list(mean = MU_HAT, sd = SIGMA),\n    color = \"cyan4\",\n    linewidth = 2\n  ) +\n  geom_vline(xintercept = MU_HAT, color = \"cyan4\", size = 1) +\n  annotate(\n    geom = \"text\",\n    hjust = \"right\",\n    x = 6.8,\n    y = .3,\n    label = latex2exp::TeX(\"$N(5, \\\\sigma^2)$\"),\n    color = \"cyan4\"\n  ) +\n  \n  # annotation arrow\n  annotate(\n    geom = \"segment\",\n    x = MU,\n    xend = MU_HAT,\n    y = .19,\n    yend = .19,\n    arrow = arrow(length = unit(0.3, \"cm\"), ends = \"both\", type = \"closed\")\n  ) +\n  annotate(\n    geom = \"text\",\n    x = 3,\n    y = 0.2,\n    label = \"bias\",\n    fontface = \"bold\"\n  ) +\n  \n  # aesthetic fixes\n  theme_minimal() +\n  labs(x = latex2exp::TeX(\"\\\\mu\")) +\n  scale_x_continuous(limits = c(-2, 8), breaks = -3:8) +\n  theme(\n    axis.title.y = element_blank(),\n    axis.line.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank(),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    axis.line.x = element_line()\n  )\n\n\n\n\n\n\n\n\n\nAs we can see, the smaller our bias, the closer our guess becomes to the true value of our parameter. We can also see that a bias of zero is ideal. But there is more to an estimator than bias. Let’s move on to variance."
  },
  {
    "objectID": "tutorials/math_stat_2.html#worked-example-variance",
    "href": "tutorials/math_stat_2.html#worked-example-variance",
    "title": "The Bias-Variance Tradeoff | Mathematical Statistics 2",
    "section": "Worked example: variance",
    "text": "Worked example: variance\nFormally, the variance of an estimator is the expected value of the squared sampling deviations. But that explanation might not be conceptually helpful. So, let’s see an example.\nSay a city has a population of 1,000,000. We want to get the average height of everyone in town. So we sample 1,000 people, measure their height, and use some function to calculate an estimator. We have used the same two estimators so far, \\(\\delta_1 = \\overline{X}\\) and \\(\\delta_2 = 5\\), so we will continue with those.\nDepending on the height of everyone in our 1,000-person sample, we will get a different \\(\\overline{X}\\) each time we take a sample. If we take 100 samples of 1,000 people per sample, we will (probably) arrive at 100 distinct values of \\(\\overline{X}\\). But if we use 5 as our estimator (perhaps to mean 5 feet tall), there will be no such variation in our estimator.\nThe measure of how much our estimator fluctuates as we take more and more samples is a helpful, intuitive understanding of variance. Ideally, we want to have as little variance as possible. We know our estimator has no variance if we use a constant (a number is always just that number), but what about \\(\\overline{X}\\)? Let’s calculate.\n\\[\n\\begin{align*}\nVar[\\delta_1(\\mathbf{X})|\\mu] &= Var(\\overline{X}) \\\\\n&= Var\\left[\\frac{1}{n}\\sum_{i=1}^nX_i\\right]\n& \\text{expanding out } \\overline{X} \\\\\n&= \\frac{1}{n^2}\\sum_{i=1}^nVar(X_i)\n& \\text{must square constants when factoring out of variance} \\\\\n&= \\frac{1}{n^2}\\sum_{i=1}^n\\sigma^2\n& Var(X_i) = \\sigma^2 \\text{ is fixed and known} \\\\\n&= \\frac{n\\sigma^2}{n^2} \\\\\n&= \\frac{\\sigma^2}{n}\n\\end{align*}\n\\]\nNotice that this value is a positive number, which means the variance of \\(\\delta_1 = \\overline{X}\\) is higher than the variance of \\(\\delta_2 = 5\\), which is zero."
  },
  {
    "objectID": "tutorials/math_stat_2.html#the-tradeoff",
    "href": "tutorials/math_stat_2.html#the-tradeoff",
    "title": "The Bias-Variance Tradeoff | Mathematical Statistics 2",
    "section": "The Tradeoff",
    "text": "The Tradeoff\nThis phenomenon, where one estimator has better bias and the other has better variance, is an example of a central balancing act that statisticians have to perform: the bias-variance tradeoff. We have seen an estimator with no bias but positive variance (\\(\\overline{X}\\)) and another with no variance but non-zero bias (5). How do we know which is better? We will explore that question in the following article when we explore this tradeoff more deeply."
  },
  {
    "objectID": "tutorials/math_stat_0.html",
    "href": "tutorials/math_stat_0.html",
    "title": "Hello, statistics. | Mathematical Statistics 0",
    "section": "",
    "text": "Background\nI recently finished STA 432 at Duke. For my fellow Duke students looking to take it: yes, it was that hard. But you’re smarter than me, so you will be fine! For the mathematically curious, the course is (somewhat longwindedly) called “Theory and Methods of Statistical Learning and Inference.” If, like me, you prefer helpful course names, call it “Mathematical Statistics.”\nWhile in STA 432, I kept finding myself in mathematics far beyond the “Google the topic and find it on 100 websites” stage. In fact, the topics covered in the course were typically covered by sources that assumed a level of mathematical maturity that I did not possess. I kept thinking that I needed to pre-understand the material to understand the explanation of that same material. I longed for a plain-language introduction to mathematically rigorous subjects.\nThis series is my humble attempt to communicate the beauty of mathematical statistics in comprehensible English to my fellow students (at Duke or at home). To facilitate this, we will take a “trust my word for it” approach, with proofs being in separate articles. However, I recommend at least reading through any proofs that I post. Struggling through those should prepare you for problems involving those same concepts. Even in the proofs articles, I’ll go step-by-step (very granularly) for easy comprehension. That is not a luxury that professors get in a short lecture period, but I’ll take my time to make sure every step is annotated, which I wish I had when I was in 432.\n\n\nCourse Outline\nThis course is broadly broken into three sections. First, we will study point estimators. If you’ve never heard of estimators, don’t worry. They’re just educated guesses at unknown distribution parameters (like the mean of a normal distribution). We will calculate estimators, study them, and choose between them.\nIn the middle portion of the course, we will look at confidence intervals. Again, no background knowledge is necessary here. In effect, confidence intervals tell us how confident we are that our chosen estimator falls into a particular range. Any statisticians reading just cringed at my description because it isn’t technically accurate, but we will discuss why that’s the case when we get to confidence intervals. Don’t worry; it’ll be pretty intuitive once we get there.\nLast but certainly not least, we will conduct hypothesis tests. There, we will choose two possible options for our estimator. For example, maybe we want to choose between X &gt; 5 or X &lt;= 5. We will use our data to decide which option is more likely to be correct. Of course, we will never be 100% of our guess, but we will get close.\nAnd that’s the course! It sounds nice and clean, and in many ways it is. However, like with much of statistics, the findings are the easy part. Finding the findings is the challenge. But I’ll try my best to get us through it so your GPA can be better than mine.\n\n\nCourse Materials\nMy course, which was run by Professor Alexander Volfovsky, had very few requirements. There was a recommended textbook (found here), which I found helpful to get a second perspective on topics. But if the price is off-putting, don’t worry. We only loosely followed it; apart from some homework questions, it was never explicitly required. So, for our purposes, I won’t reference the book directly. If you want a second perspective and don’t mind spending some coin, you have the Amazon link and can search the ISBN elsewhere.\nBesides that, you’ll only need a willingness to learn and a statistical curiosity!\n\n\nConclusion\nTo get started with the course, click here to go to the first article in the series, where we begin our discussion of estimators! Of course, if you want to ask me anything about this course, my time at Duke, or anything else, the best place to reach me is on Discord! And if you really want to support this project, you can buy me a coffee. Thank you for reading, and I can’t wait to get started!"
  },
  {
    "objectID": "projects/tidytuesday_06112024/pride.html",
    "href": "projects/tidytuesday_06112024/pride.html",
    "title": "Campus Pride Index",
    "section": "",
    "text": "Welcome!\nHappy pride month! On this fine TidyTuesday afternoon, we will see how different types of colleges and universities handle LGBTQ+ inclusion! The Campus Pride Index tracks safety, inclusivity, and LGBTQ+ policies/programs at universities across the United States. Results are on a 1-5 scale (with higher numbers being most inclusive), and colleges are grouped by various discrete categories. Today, we’ll build a stacked horizontal bar chart to see the distribution of scores for some of those categories. I’ll use the ggchicklet package and some custom fonts for easy aesthetic changes, and we’ll be done! If you want to see a step-by-step tutorial explaining the code, click here.\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nThere we have it! Seeing very few 1s is a good sign, but there is always room for progress. The top three types of colleges by number (public, private, and liberal arts) are highly inclusive, while community colleges (where I started my educational journey) have a long way to go.\nIf you have any questions or suggestions for improvements, the best way to reach me is on Discord! And, of course, if you want to support this work financially, you can buy me a coffee!\nThanks for reading, and I’ll see you next week!"
  },
  {
    "objectID": "projects/tidytuesday_05212024/emissions.html",
    "href": "projects/tidytuesday_05212024/emissions.html",
    "title": "Our World in Emissions",
    "section": "",
    "text": "Hello, all! Welcome to TidyTuesday. This week, as climate analysts often do, we are going to get mildly depressing in pursuit of a pretty graph. This time, we will look at emissions from various actors’ coal, natural gas, and cement production. Spoiler: it’s not good.\nThe data for this week are brought to us by Carbon Majors, who have compiled a database going all the way back to the 1850’s! The dataset contains emission data for 75 state and non-state actors, but we will aggregate into total emissions by type for the plot. If you want to get more granular in your own plot, check out the data on the TidyTuesday GitHub repository here!\n\n\nClick here for code\nlibrary(tidyverse)\n\n# read data and rename an ugly column ------------------------------------------\nemis&lt;- read_csv(paste0(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/\",\n  \"data/2024/2024-05-21/emissions.csv\"\n  )\n)\n\nemis &lt;- emis |&gt;\n  rename(emissions = \"total_emissions_MtCO2e\")\n\n# constants for ease of code legibility ----------------------------------------\nLEVS &lt;- c(\"Coal\", \"Oil & NGL\", \"Natural Gas\", \"Cement\")\nBG_COLOR &lt;- \"#F0F0F0\"\nGRAY &lt;- \"gray35\"\nUN_TEXT &lt;- paste(\n  \"In 1995, the United Nations\\nConference of the Parties met for\\nthe first\", \n  \"time to discuss the looming\\nthreat of climate change. The COP\\nhas\",\n  \"met twenty-eight times since.\"\n)\n\n# data cleanup -----------------------------------------------------------------\nemis |&gt;\n  filter(year &gt;= 1900) |&gt; # lots of near-zero space without this filter\n  mutate(\n    commodity = if_else(str_detect(commodity, \"Coal\"), \"Coal\", commodity),\n    commodity = factor(commodity, levels = LEVS) # re-order areas\n  ) |&gt;\n  group_by(year, commodity) |&gt;\n  summarise(emissions = sum(emissions), .groups = \"drop\") |&gt;\n  \n  # start of plot --------------------------------------------------------------\n  ggplot(aes(x = year, y = emissions, fill = commodity)) +\n  geom_area(alpha = 0.9) +\n  \n  # UN COP annotation text box -------------------------------------------------\n  annotate(\n    geom = \"segment\",\n    x = 1995,\n    xend = 1995,\n    y = 35500,\n    yend = 20500,\n    linetype = \"solid\",\n    linejoin = \"round\",\n    linewidth = 1,\n    color = \"grey35\",\n    arrow = arrow(type = \"closed\", length = unit(0.2, \"cm\"))\n  ) +\n  annotate(\n    geom = \"rect\",\n    xmin = 1950.5,\n    xmax = 1993.5,\n    ymin = 23500,\n    ymax = 35800,\n    fill = BG_COLOR\n  ) +\n  annotate(\n    geom = \"text\",\n    x = 1992,\n    y = 30000,\n    label = UN_TEXT,\n    color = GRAY,\n    fontface = \"italic\",\n    hjust = \"right\"\n  ) +\n  \n  # replace legend with annotation text ----------------------------------------\n  annotate(\n    geom = \"text\",\n    color = \"white\",\n    x = 2020,\n    y = c(1000, 4700, 13000, 26000),\n    label = c(\"Cement\", \"Natural Gas\", \"Oil & NGL\", \"Coal\"),\n    hjust = \"right\",\n    fontface = \"bold\"\n  ) +\n  \n  # visual style elements (love you, ggthemes) ---------------------------------\n  ggthemes::scale_fill_colorblind() +\n  ggthemes::theme_fivethirtyeight() +\n  \n  # customize axis breaks and labels -------------------------------------------\n  scale_x_continuous(breaks = seq(1900, 2020, 20)) +\n  scale_y_continuous(\n    breaks = seq(0, 40000, 5000), \n    label = scales::label_number(scale = 1e-3, suffix = \"k\")\n  ) +\n  labs(\n    x = element_blank(),\n    y = latex2exp::TeX(\"Emissions ($MtCO_2e$)\"),\n    title = \"Our World in Emissions\",\n    subtitle = latex2exp::TeX(\n      paste(\n        \"Emissions are measured in Millions of Tons of $CO_2$ equivalent\",\n        \"($MtCO_2e$)\"\n      )\n    ),\n    caption = paste(\n      \"Made with love by Mitch Harrison\",\n      \"                                                                       \",\n      \"Source: Carbon Majors database and TidyTuesday\"\n    )\n  ) + \n  \n  # theme cleanup --------------------------------------------------------------\n  geom_hline(yintercept = 0, linewidth = 0.7, color = GRAY) + # bold axis\n  theme(\n    legend.position = \"none\", # hide legend\n    axis.title.y = element_text(size = 10),\n    plot.background = element_rect(fill = BG_COLOR)\n  ) \n\n\n\n\n\n\n\n\n\nSo there she is! As we can see, the UN COP seems to be fighting an uphill battle. Emissions are rising, but a good analyst must note the limitations of the data. What jumps out to me is that renewables aren’t listed here because it’s only a graph of emissions. For all we know (from this graph), these emissions only produce a small portion of the world’s energy, and we are arguing about a couple of percentage points. Maybe we have defeated climate change after all!\nOf course, that’s not the case, but proving that point will require outside data. So, I welcome everyone reading to write a fuller report using more evidence. If nothing else, it would make for some fun data viz practice!\nIf you want a step-by-step guide to how I made this plot, there is a tutorial page here, or even stop by my Discord server and ask me! And, of course, if you appreciate my work enough to buy me a coffee, you can do so here. Thank you for reading, and see you next week!"
  },
  {
    "objectID": "projects/tidytuesday_01142025/talks.html",
    "href": "projects/tidytuesday_01142025/talks.html",
    "title": "Posit::Conf 2023",
    "section": "",
    "text": "Click here for code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(ggthemes)\n\nconf23 &lt;- read_csv(\"data/conf2023.csv\")\nconf24 &lt;- read_csv(\"data/conf2024.csv\")\n\n#conf23 &lt;- read_csv(\"projects/tidytuesday_01142025/data/conf2023.csv\")\n#conf24 &lt;- read_csv(\"projects/tidytuesday_01142025/data/conf2024.csv\")\n\n\n\nThe Viz\nHello all! This week’s TidyTuesday brings us the event schedule from the posit::conf events of 2023 and 2024. Although there isn’t always much in the way of data analysis that we can do with a simple list of conference events, I did find a phenomenon that amused me.\nAt the conference, there are three types of talks: keynote talks to open and close both days of the conference, regular talks that occur during the day, and very quick lightning talks that cover smaller topics in a very short time window. For some reason, all of the lightning talks were scheduled to overlap with the only pharma-related main talks at the entire event. So people had to choose whether to attend the pharma talks or the lightning talks. I’m not sure what the rationale for scheduling it that was is, but here’s a plot to demonstrate!\n\n\nClick here for code\nset.seed(103)\n\nnew_23 &lt;- conf23 |&gt;\n  distinct(session_type, session_start, .keep_all = TRUE) |&gt;\n  mutate(\n    session_start_time = format(session_start, \"%H:%M:%S\"),\n    session_start_time = as.POSIXct(session_start_time, format = \"%H:%M:%S\"),\n    session_end_time = session_start_time + minutes(session_length),\n    date_encoded = if_else(session_date == min(session_date), 1, 2),\n    session_date2 = if_else(session_date == as.POSIXct(\"2023-09-19\"), 0, 1)\n  )\n\nlightning &lt;- new_23 |&gt;\n  filter(session_type == \"lightning\")\n\nnon_lightning &lt;- new_23 |&gt;\n  filter(session_type != \"lightning\")\n\nnew_23 |&gt;\n  ggplot(aes(color = session_type, x = session_start_time, y = session_date2)) + \n  geom_jitter(data = lightning, size = 2, height = 0.1, width = 0) +\n  geom_point(data = non_lightning, size = 3) +\n  geom_segment(\n    aes(xend = session_end_time),\n    data = non_lightning,\n    linewidth = 1.5\n  ) +\n  geom_rect(\n    xmin = as.POSIXct(\"17:00:00\", format = \"%H:%M:%S\"),\n    xmax = as.POSIXct(\"19:20:00\", format = \"%H:%M:%S\"),\n    ymin = 0.33,\n    ymax = 0.67,\n    fill = \"#f0f0f0\", # theme background color\n    color = NA\n  ) +\n  annotate(\n    geom = \"text\",\n    x = as.POSIXct(\"17:00:00\", format = \"%H:%M:%S\"),\n    y = 0.5,\n    label = paste(\n      \"Lightning talks took place\\nexclusively during these\\nPharma-related\",\n      \"main talks.\\nSorry, doc!\"\n    ),\n    hjust = \"left\",\n    color = \"#016392\",\n    fontface = \"bold\"\n  ) +\n  annotate(\n    geom = \"segment\",\n    x = as.POSIXct(\"16:20:00\", format = \"%H:%M:%S\"),\n    xend = as.POSIXct(\"16:55:00\", format = \"%H:%M:%S\"),\n    y = 0.14,\n    yend = 0.34,\n    color = \"#016392\",\n    linewidth = 1\n  ) +\n  theme_fivethirtyeight() +\n  scale_color_wsj() +\n  scale_y_continuous(\n    breaks = c(0, 1),\n    labels = c(\"Sep 19\", \"Sep 20\")\n  ) +\n  labs(\n    title = \"Event schedule - posit::conf(2023)\",\n    subtitle = \"Hopefully there were no impatient pharmacists there.\",\n    color = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\n\nChallenges\nAlthough the plot is pretty simple, I used some non-intuitive techniques to place the text and line annotations when the data are dates instead of regular continuous values. I encoded the y axis as a continuous one and used POSIXct objects to place them on the x axis. Feel free to check the code to see how that worked!\n\n\nConclusion\nAs always, thanks for reading! Feel free to reach out to me on LinkedIn if you want to connect or ask questions about my work. See you next week!"
  },
  {
    "objectID": "projects/crickets/crickets.html",
    "href": "projects/crickets/crickets.html",
    "title": "Cricket Crackers: A Bayesian Approach",
    "section": "",
    "text": "Click here for code\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ggthemes)\nlibrary(gt)\nlibrary(bayesplot)\nlibrary(rstanarm)\n\ngrowth &lt;- read_csv(\"data/growth.csv\") |&gt; \n#growth &lt;- read_csv(\"projects/crickets/data/growth.csv\") |&gt;\n  janitor::clean_names() |&gt;\n  mutate(\n    date_of_hatching = dmy(date_of_hatching),\n    date_of_measurement = dmy(date_of_measurement)\n  )\n\nrepro &lt;- read_csv(\"data/reproduction.csv\") |&gt; \n#repro &lt;- read_csv(\"projects/crickets/data/reproduction.csv\") |&gt; \n  janitor::clean_names() |&gt;\n  mutate(\n    date_of_hatching = dmy(date_of_hatching),\n    date_of_laying = dmy(date_of_laying)\n  )\n\ntemp &lt;- read_csv(\"data/temp_humid.csv\") |&gt;\n#temp &lt;- read_csv(\"projects/crickets/data/temp_humid.csv\") |&gt;\n  janitor::clean_names()\nClick here for code\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618,\n  fit.retina = 3,\n  fig.align = \"center\",\n  dpi = 300\n)\n\nset.seed(1337)\ntheme_set(theme_fivethirtyeight())\n\nLIGHT_BLUE &lt;- \"#32a4a8\""
  },
  {
    "objectID": "projects/crickets/crickets.html#the-experiment",
    "href": "projects/crickets/crickets.html#the-experiment",
    "title": "Cricket Crackers: A Bayesian Approach",
    "section": "The Experiment",
    "text": "The Experiment\nResearchers are comparing four species of crickets: Teleogryllus lemus, Gryllodes sigillatus, Teleogryllus afer, and Gryllus madagascariensis. In this experiment, 150 crickets from each species are incubated in a lab-controlled setting. Temperature and humidity were controlled, and each species had 12 hours of light and dark per day. Unlimited food, in the form of chicken feed, and unlimited water were provided to each cricket. Researchers allowed each cricket to grow individually inside a ventilated plastic box. They measured each cricket’s body length and weight measurements weekly and conducted checks every three days to see if a cricket had molted (that is, shed its exoskeleton and entered a new growth stage).\nUpon reaching adulthood, 20 females and 20 males from each species were selected at random and paired together to breed eggs. Every 24 hours, researchers check for eggs, count them, and separate them to allow them to hatch. They then record the number of eggs laid and the number successfully hatched. We are left with a pair of datasets: one for reproduction data about the eggs and the other about the growth rates of the crickets when they hatch!"
  },
  {
    "objectID": "projects/crickets/crickets.html#our-plan",
    "href": "projects/crickets/crickets.html#our-plan",
    "title": "Cricket Crackers: A Bayesian Approach",
    "section": "Our Plan",
    "text": "Our Plan\nGiven these data, our job as an analyst is to help decide which species is best for future cultivation. Ideally, we want to maximize the amount of cricket flour generated per unit of time (in our case, per day). Since the entire cricket is ground into flour, this is equivalent to trying to find the maximum amount of body weight added per cricket per day. With that number, any future cricket farmer could simply multiply by the number of crickets they expect to have to arrive at total flour production estimates per day.\nLet’s take a Bayesian approach. We will build two models: the first will give a posterior distribution of the number of eggs laid per species, and the second will be a posterior distribution of growth per day by species. Finally, we can multiply the posterior samples of each model together to arrive at a final distribution that is a combination of the number of eggs laid and the amount of growth each cricket will experience per day. In other words, total meat per day! Once we find the best species under these ideal conditions, future research can vary the breeding conditions to find optimal conditions for that species."
  },
  {
    "objectID": "projects/crickets/crickets.html#the-egg-production-model",
    "href": "projects/crickets/crickets.html#the-egg-production-model",
    "title": "Cricket Crackers: A Bayesian Approach",
    "section": "The Egg Production Model",
    "text": "The Egg Production Model\nLet’s take a sample of our reproduction dataset to see what we are working with.\n\n\nClick here for code\nrepro |&gt;\n  select(-c(temperature_c, mortality_of_adults_y_n)) |&gt;\n  slice_sample(n = 5) |&gt;\n  gt() |&gt;\n  tab_header(md(\"**Random sample: reproduction dataset**\"))\n\n\n\n\n\n\n\n\nRandom sample: reproduction dataset\n\n\nspecies\npaired_adults\ndate_of_laying\neggs_number\ndate_of_hatching\nhatched_eggs\n\n\n\n\nGryllodes sigillatus\n19\n2023-12-23\n102\n2024-01-03\n66\n\n\nTeleogryllus lemur\n6\n2023-11-07\nNA\nNA\nNA\n\n\nTeleogryllus afer\n15\n2024-01-13\n103\n2024-01-20\n72\n\n\nTeleogryllus afer\n10\n2024-01-13\n113\n2024-01-20\n79\n\n\nTeleogryllus lemur\n7\n2023-11-01\n116\n2023-11-08\n81\n\n\n\n\n\n\n\nWe can start to see the structure of the data. Each row represents a single mating pair (paired_adults) on a specific date (date_of_laying). The eggs laid that day hatch at a later date (date_of_hatching), and the number of those eggs that successfully hatch are provided as well (hatched_eggs). The first question that comes to my mind is whether any of the four species has a significantly higher hatch rate than the others. Let’s have a look.\n\n\nClick here for code\np_success &lt;- repro |&gt;\n  group_by(paired_adults, species) |&gt;\n  summarise(\n    eggs_laid = sum(eggs_number, na.rm = T),\n    hatched = sum(hatched_eggs, na.rm = T)  \n  ) |&gt;\n  mutate(succ_prop = hatched / eggs_laid)\n\np_success$id &lt;- 1:nrow(p_success)\n\np_success |&gt;\n  ggplot(aes(x = succ_prop, color = species)) +\n  geom_density(linewidth = 2) +\n  scale_color_wsj() +\n  labs(\n    x = \"Proportion of eggs hatched\",\n    y = \"Density\",\n    color = \"Species\",\n    title = \"Successful eggs distribution\",\n    subtitle = \"The proportion of laid eggs successfully hatched by species\"\n  )\n\n\n\n\n\n\n\n\n\nThe four species seem to neatly stick to one of two modes. But again, we have to observe the \\(x\\) axis: there is next to no variance among any of the four species! The two modes are just about \\(0.0672\\) and \\(0.0676\\). Either we have a set of four strangely uniform cricket species, or there is a date collection error somewhere. Either way, hatch rate probably isn’t going to help us determine the best species. Since all four species have an approximately equal hatch rate, all that matters to us is the absolute number of eggs hatched. Let’s model it!\n\nBuilding the model\nThe only variable left to use as a predictor is the species of each mating pair. We have already eliminated hatch rate and both environmental factors that we were given by researchers, so we won’t be able to correct for other predictors. That’s a shame, but a univariate model is better than none. To maintain the independence of our observations, we don’t want to include every pair multiple times. If we did, then we would have reason to believe that the eggs hatched by one row is impacted by the eggs of another row. After all, they are the same breeding pair!\nTo maintain independence, let’s take the mean of each mating pair’s reproductive life. In doing so, we lose some data but gain statistical independence, without which we can’t do a model at all. Once we take that mean, we can run a Bayesian regression model with our species predictor and arrive at a per-species posterior distribution of the number of eggs successfully hatched per day. To make life easier, let’s see if we can use a common prior. The easiest one to check is the normal prior. Let’s plot our observed density against a normal distribution with the sample mean and standard deviation.\n\n\nClick here for code\nrepro_b &lt;- repro |&gt;\n  group_by(species, paired_adults) |&gt;\n  summarise(hatched_eggs = floor(mean(hatched_eggs, na.rm = TRUE)))\n\nrepro_b |&gt;\n  ggplot(aes(x = hatched_eggs)) +\n  geom_density(\n    aes(color = \"Observed distribution\"),\n    linewidth = 2,\n    color = \"darkgray\"\n  ) +\n  stat_function(\n    aes(color = \"Normal distribution\"),\n    fun = dnorm, \n    args = list(mean = mean(repro_b$hatched_eggs), \n                sd = sd(repro_b$hatched_eggs)), \n    linewidth = 2,\n    color = LIGHT_BLUE\n  ) +\n  annotate(\n    geom = \"text\",\n    label = \"Normal distribution\",\n    x = 67,\n    y = 0.035,\n    hjust = \"right\",\n    vjust = \"bottom\",\n    fontface = \"bold\",\n    color = LIGHT_BLUE\n  ) +\n  annotate(\n    geom = \"text\",\n    label = \"Observed distribution\",\n    x = 73,\n    y = 0.077,\n    hjust = \"right\",\n    vjust = \"bottom\",\n    fontface = \"bold\",\n    color = \"darkgray\"\n  ) +\n  labs(\n    x = \"Eggs hatched\",\n    title = \"Observed eggs hatched vs normal distribution\",\n    subtitle = paste(\"The observed egg hatch rate is similar enough to use a\",\n                     \"normal prior.\")\n  ) +\n  theme(axis.title.x = element_text())\n\n\n\n\n\n\n\n\n\nLooks good enough to me! Using data visualization is a quick and dirty way to select a prior, but with a model as simple as ours and an observed distribution so close to normal (thanks, Central Limit Theorem!), I’m hoping that it will at least do better than throwing an un-informative prior at it and hoping for the best.\nSo we’re ready for our first of two models! Let’s run it with 5000 iterations with our normal prior with observed mean and standard deviation!\n\n\nClick here for code\nITERS &lt;- 5000\nmean_eggs &lt;- mean(repro$hatched_eggs, na.rm = TRUE)\nsd_eggs &lt;- sd(repro$hatched_eggs, na.rm = TRUE)\n\nmodel_eggs &lt;- stan_glm(\n  hatched_eggs ~ species,\n  data = repro_b,\n  family = poisson(link = \"log\"),\n  prior = normal(mean_eggs, sd_eggs),\n  chains = 1,\n  iter = ITERS,\n  seed = 440,\n  refresh = 0\n)\n\n\n\n\nClick here for code\neggs_tib &lt;- exp(as_tibble(model_eggs)) \ncolnames(eggs_tib) &lt;- c(\"a\", \"b\", \"c\", \"d\")\neggs_tib &lt;- eggs_tib |&gt;\n  mutate(\n    b = b + a,\n    c = c + a,\n    d = d + a\n  )\ncolnames(eggs_tib) &lt;- sort(unique(growth$species))\n\neggs_tib |&gt;\n  pivot_longer(cols = everything(), names_to = \"species\", values_to = \"eggs\") |&gt;\n  ggplot(aes(x = eggs, fill = species)) +\n  geom_histogram(bins = 50) +\n  scale_fill_wsj() +\n  facet_wrap(~species, nrow = 2) +\n  theme(legend.position = \"none\") +\n  labs(\n    x = \"Predicted number of eggs laid (per day)\",\n    y = paste0(\"Count (n = \", ITERS, \")\"),\n    title = \"Posterior distribution of eggs laid per day\",\n    subtitle = \"Only Gryllodes sigillatus shows lower egg production\"\n  )\n\n\n\n\n\n\n\n\n\nAnd here we have it! Not particularly sexy, but we have posterior samples. So far, it looks like most of the species are going to have about the same distribution of eggs hatched per day. We can verify that assumption with a 95% posterior confidence interval.\n\n\nClick here for code\nci_eggs &lt;- exp(as_tibble(posterior_interval(model_eggs)))\ncolnames(ci_eggs) &lt;- c(\"lower\", \"upper\")\nint_vals &lt;- unname(as_vector(slice_head(ci_eggs, n = 1)))\nci_eggs[2:4,1] &lt;- ci_eggs[2:4,1] + int_vals[1]\nci_eggs[2:4,2] &lt;- ci_eggs[2:4,2] + int_vals[2]\nci_eggs$species &lt;- sort(unique(growth$species))\n\nci_eggs &lt;- ci_eggs |&gt;\n  select(species, lower, upper) |&gt;\n  mutate(lower = round(lower, 2), upper = round(upper, 2))\n\ngt(ci_eggs) |&gt;\n  tab_header(md(\"**95% Posterior CI: eggs hatched per day**\")) |&gt;\n  cols_label(\n    species = \"Species\",\n    lower = \"Lower\",\n    upper = \"Upper\"\n  )\n\n\n\n\n\n\n\n\n95% Posterior CI: eggs hatched per day\n\n\nSpecies\nLower\nUpper\n\n\n\n\nGryllodes sigillatus\n67.79\n73.84\n\n\nGryllus madagascariensis\n68.82\n75.01\n\n\nTeleogryllus afer\n68.79\n74.97\n\n\nTeleogryllus lemur\n68.79\n74.97\n\n\n\n\n\n\n\nAs we assumed, our posterior confidence intervals are extremely close across all four species (even identical when rounded to 2 significant figures). There is a small dip for Gryllodes sigillatus, but not an appreciable one. Hopefully we get better results when we build our second model and combine them."
  },
  {
    "objectID": "projects/crickets/crickets.html#growth-rate-model",
    "href": "projects/crickets/crickets.html#growth-rate-model",
    "title": "Cricket Crackers: A Bayesian Approach",
    "section": "Growth rate model",
    "text": "Growth rate model\nNow that we have an idea of how many eggs each mating pair will produce for each species, it’s time to model how much those newly-hatched crickets will grow every day. Let’s sample our growth data to see what we’re working with.\n\n\nClick here for code\ngrowth |&gt;\n  select(-c(temperature_c, mortality_y_n)) |&gt;\n  slice_sample(n = 5) |&gt;\n  gt() |&gt;\n  tab_header(md(\"**Random sample: growth dataset**\"))\n\n\n\n\n\n\n\n\nRandom sample: growth dataset\n\n\nspecies\nindividuals\ndate_of_hatching\ndate_of_molting\nstage\ndate_of_measurement\nday_weekly\nbody_length_mm\nbody_weight_g\n\n\n\n\nGryllodes sigillatus\n77\n2023-11-08\n18/12/23\nL2\n2023-11-29\n21\n6.0\n0.016\n\n\nGryllodes sigillatus\n57\n2023-11-08\n23/11/23\nL1\n2023-11-15\n7\n2.5\n0.001\n\n\nGryllus madagascariensis\n131\n2023-11-30\n20/12/23\nL1\n2023-12-07\n7\n3.5\n0.001\n\n\nTeleogryllus afer\n25\n2023-11-17\n24/11/23\nL1\n2023-11-24\n7\n3.0\n0.001\n\n\nGryllus madagascariensis\n67\n2023-11-30\n18/01/24\nL4\n2023-12-28\n28\n8.5\n0.045\n\n\n\n\n\n\n\nAgain, we run into an issue with a lack of useful predictors. Let’s talk about why. Although we don’t see them in this sample, there are five life stages in the data, L1 through L5. Each of these life stages strictly increases cricket size. After all, if they weren’t growing, there’s no reason to molt. So if we want to maximize our cricket weight output, we will always be harvesting the crickets at stage L5. Thus, to build our second model we will only use the data that is from stage L5, when the crickets are the largest they will ever be.\nAdditionally, because we are only concerned with the amount of cricket flour produced, the body length of each cricket is of no help. If a cricket is larger in length but smaller in weight, it ultimately produces less flour. Finally, the date columns don’t help us either, since our model will be time-agnostic. Observe that the time it takes for each species’ eggs to hatch only effects the “start-up” time of a new cricket farm. Once the first batch of eggs hatches, a farmer will produce new crickets every day. So time-to-hatch isn’t relevant in real-world scenarios unless some species take so much longer to hatch that there may be a financial cost to a delayed startup. Let’s quickly see if that’s the case by visualizing the distribution of hatch times by species.\n\n\nClick here for code\nrepro |&gt;\n  mutate(hatchdays = difftime(date_of_hatching, date_of_laying, \n                              units = \"days\")) |&gt;\n  ggplot(aes(x = hatchdays, fill = species)) +\n  geom_histogram(stat = \"count\") +\n  scale_fill_wsj() +\n  facet_wrap(~ species, nrow = 2) +\n  labs(\n    x = \"Number of Days for an Egg to Hatch\",\n    y = \"Count\",\n    title = \"Cricket hatch times\",\n    subtitle = \"The number of days between laying and hatching cricket eggs\"\n  ) +\n  theme(legend.position = \"none\", axis.title = element_text())\n\n\n\n\n\n\n\n\n\nThere are some modest differences, but probably not enough to impact cricket harvesting at scale. Thus, unfortunately, we are again limited to only using species as a predictor. But notice that the variable we are interested in isn’t in the data! If we want to measure growth rate, our response variable is the amount of weight gained by each cricket from hatching to life stage L5. Thus, we can manipulate our data to create this variable.\nGrouping by individual cricket and species, we can measure growth_time as the number of days between hatching and stage L5. Dividing its final weight by this number of days, we arrive at our response variable: average growth per day for each cricket/species pair! Let’s see if that new variable approximates a normal distribution to help us choose a prior.\n\n\nClick here for code\ngrowth_b &lt;- growth |&gt;\n  filter(day_weekly == 42) |&gt;\n  mutate(\n    growth_time = as.numeric(\n      difftime(date_of_measurement, date_of_hatching, units = \"days\")\n    )\n  ) |&gt;\n  select(body_weight_g, growth_time, species, individuals)\n\ngrowth_b &lt;- growth_b |&gt;\n  mutate(growth_rate = body_weight_g / growth_time) |&gt;\n  filter(!is.na(growth_rate))\n\nMEAN &lt;- mean(growth_b$growth_rate, na.rm = TRUE)\nSD &lt;- sd(growth_b$growth_rate, na.rm = TRUE)\n\ngrowth_b |&gt;\n  ggplot(aes(x = growth_rate)) +\n  geom_density(\n    aes(color = \"Observed distribution\"),\n    linewidth = 2,\n    color = \"darkgray\"\n  ) +\n  stat_function(\n    aes(color = \"Normal distribution\"),\n    fun = dnorm, \n    args = list(mean = MEAN, sd = SD), \n    linewidth = 2,\n    color = LIGHT_BLUE\n  ) +\n  annotate(\n    geom = \"text\",\n    label = \"Normal distribution\",\n    x = 0.015,\n    y = 60,\n    hjust = \"right\",\n    vjust = \"bottom\",\n    fontface = \"bold\",\n    color = LIGHT_BLUE\n  ) +\n  annotate(\n    geom = \"text\",\n    label = \"Observed distribution\",\n    x = 0.01,\n    y = 150,\n    hjust = \"right\",\n    vjust = \"bottom\",\n    fontface = \"bold\",\n    color = \"darkgray\"\n  ) +\n  labs(\n    x = \"Growth rate (per day)\",\n    title = \"Observed growth rates vs normal distribution\",\n    subtitle = paste(\"The observed growth rate is not similar enough to use a\",\n                     \"normal prior.\")\n  ) +\n  theme(axis.title.x = element_text())\n\n\n\n\n\n\n\n\n\nThis time, our response variable doesn’t well-approximate a normal distribution. To make our life easier, let’s just an un-informative prior this time. It’s modeling time!\n\n\nClick here for code\nmodel_growth &lt;- stan_glm(\n  growth_rate ~ species,\n  data = growth_b,\n  family = gaussian,\n  chains = 1,\n  iter = ITERS,\n  seed = 440,\n  refresh = 0\n)\n\n\nBecause our response variables are so small, and because no one is harvesting a small number of crickets in practice, let’s visualize our posterior after multiplying the results by 100. That will show us a distribution of growth rates for 100 crickets at a time. This is only for visualization purposes though: we will use per-cricket growth when we build our final results.\n\n\nClick here for code\ngrowth_tib &lt;- as_tibble(model_growth)[,1:4]\ncolnames(growth_tib) &lt;- c(\"a\", \"b\", \"c\", \"d\")\ngrowth_tib &lt;- growth_tib |&gt;\n  mutate(\n    b = b + a,\n    c = c + a,\n    d = d + a\n  )\ncolnames(growth_tib) &lt;- sort(unique(growth$species))\n\ngrowth_tib |&gt;\n  pivot_longer(cols = everything(), names_to = \"species\", values_to = \"rate\") |&gt;\n  ggplot(aes(x = rate*100, fill = species)) +\n  geom_histogram(bins = 100) +\n  scale_fill_wsj() +\n  facet_wrap(~species) +\n  theme(legend.position = \"none\", axis.title.x = element_text()) +\n  labs(\n    x = \"Growth rate (per 100 crickets, per day)\",\n    y = paste0(\"Count (n = \", ITERS, \")\"),\n    title = \"Posterior daily growth rate\",\n    subtitle = paste(\n      \"Results are multiplied by 100 for ease of legibility. Model\",\n      \"coefficients \\nare per-cricket.\"\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\nClick here for code\nci_growth &lt;- as_tibble(posterior_interval(model_growth))[1:4,]\ncolnames(ci_growth) &lt;- c(\"lower\", \"upper\")\nint_vals &lt;- unname(as_vector(slice_head(ci_growth, n = 1)))\nci_growth[2:4,1] &lt;- ci_growth[2:4,1] + int_vals[1]\nci_growth[2:4,2] &lt;- ci_growth[2:4,2] + int_vals[2]\nci_growth$species &lt;- sort(unique(growth$species))\nci_growth &lt;- select(ci_growth, species, lower, upper) |&gt;\n  mutate(lower = round(lower, 4), upper = round(upper, 4))\n\ngt(ci_growth) |&gt;\n  tab_header(md(\"**Posterior 95% CI: per-cricket growth rate**\")) |&gt;\n  cols_label(\n    species = \"Species\",\n    lower = \"Lower bound\",\n    upper = \"Upper bound\"\n  )\n\n\n\n\n\n\n\n\nPosterior 95% CI: per-cricket growth rate\n\n\nSpecies\nLower bound\nUpper bound\n\n\n\n\nGryllodes sigillatus\n0.0009\n0.0016\n\n\nGryllus madagascariensis\n0.0126\n0.0144\n\n\nTeleogryllus afer\n0.0006\n0.0024\n\n\nTeleogryllus lemur\n0.0012\n0.0029\n\n\n\n\n\n\n\nWe seem to have a winner so far! The Gryllus madagascariensis are clearly the crickets with the highest per-day growth rate from stage L1 to L5. It is reasonable to expect that when we multiply our posterior sample from our two models together, Gryllus madagascariensis will be our winner. But there’s only one way to be sure, and that’s putting our final results together."
  },
  {
    "objectID": "projects/crickets/crickets.html#putting-the-models-together",
    "href": "projects/crickets/crickets.html#putting-the-models-together",
    "title": "Cricket Crackers: A Bayesian Approach",
    "section": "Putting the models together",
    "text": "Putting the models together\nSince we ran both models for exactly 5000 iterations, we can combine their results by simply multiplying them together row-wise. This has the effect of asking “for a given simulated number of eggs and a simulated growth rate, how much total weight will that many crickets generate that day?” That’s our research question, so let’s find out!\n\n\nClick here for code\nmelt_growth &lt;- growth_tib |&gt;\n  pivot_longer(cols = everything(), names_to = \"species\", values_to = \"growth\") \n\nmelt_eggs &lt;- eggs_tib |&gt;\n  pivot_longer(cols = everything(), names_to = \"species\", values_to = \"eggs\") \n\npost_meat_per_day &lt;- tibble(\n  species = melt_growth$species,\n  growth = melt_growth$growth,\n  eggs = melt_eggs$eggs,\n  meat_per_day = growth * eggs\n)\n\npost_meat_per_day |&gt;\n  ggplot(aes(x = meat_per_day, fill = species)) +\n  geom_histogram(bins = 50) +\n  scale_fill_wsj() +\n  facet_wrap(~species, nrow = 2) +\n  theme(legend.position = \"none\", axis.title.x = element_text()) +\n  labs(\n    x = \"Meat production per day\",\n    y = paste0(\"Count (n = \", ITERS, \")\"),\n    title = \"Posterior distribution of meat production per day\",\n    subtitle = \"Meat production values are shown per breeding pair.\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nClick here for code\nci_meat_per_day &lt;- post_meat_per_day %&gt;%\n  group_by(species) %&gt;%\n  summarise(\n    lower = round(quantile(meat_per_day, 0.025, na.rm = TRUE), 2),\n    upper = round(quantile(meat_per_day, 0.975, na.rm = TRUE), 2)\n  )\n\nci_table &lt;- left_join(ci_growth, ci_eggs, by = \"species\") |&gt;\n  mutate(\n    lower.z = ci_meat_per_day$lower,\n    upper.z = ci_meat_per_day$upper\n  ) |&gt;\n  gt(rowname_col = \"species\") |&gt;\n  tab_spanner(\n    label = \"Growth rate\\n(per-cricket)\", \n    columns = c(lower.x, upper.x)\n  ) |&gt;\n  tab_spanner(label = \"Eggs hatched\", columns = c(lower.y, upper.y)) |&gt;\n  tab_spanner(label = \"Meat per day\", columns = c(lower.z, upper.z)) |&gt;\n  cols_label(\n    lower.x = \"Lower\",\n    lower.y = \"Lower\",\n    lower.z = \"Lower\",\n    upper.x = \"Upper\",\n    upper.y = \"Upper\",\n    upper.z = \"Upper\"\n  ) |&gt;\n  tab_header(md(\"**Posterior 95% confidence intervals**\")) |&gt;\n  data_color(c(2,3,6,7), palette = \"#ededed\") |&gt;\n  tab_style(\n    locations = cells_body(columns = c(6,7)),\n    style = list(cell_text(weight = \"bold\"))\n  ) |&gt;\n  tab_options(\n    footnotes.padding = 3,\n    footnotes.font.size = 13\n  )\nci_table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosterior 95% confidence intervals\n\n\n\n\nGrowth rate (per-cricket)\n\n\nEggs hatched\n\n\nMeat per day\n\n\n\nLower\nUpper\nLower\nUpper\nLower\nUpper\n\n\n\n\nGryllodes sigillatus\n0.0009\n0.0016\n67.79\n73.84\n0.06\n0.12\n\n\nGryllus madagascariensis\n0.0126\n0.0144\n68.82\n75.01\n0.92\n1.03\n\n\nTeleogryllus afer\n0.0006\n0.0024\n68.79\n74.97\n0.08\n0.14\n\n\nTeleogryllus lemur\n0.0012\n0.0029\n68.79\n74.97\n0.12\n0.18"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I’m Mitch. 👋",
    "section": "",
    "text": "After leaving the Navy in 2022, I moved to North Carolina to study data science and political science at Duke University. Finishing my degree cost the military about $60,000 in aid and depleted my savings. Even my parents, who probably thought they could start spending their “put their kid through college” fund on fun things, had to chip in to help. My education was world-class but utterly unattainable for the majority of us. This website serves as my portfolio, but also my attempt to make my education more accessable.\nIf you want to see some of my analysis work, head to the Projects tab! If you are here to learn, the Tutorials tab is for you. Of course, if you are curious about the website’s structure, it is built with R and Quarto, and the code is available by clicking on the GitHub icon at the bottom-left of every page (or click here).\nAccess to information should always be free, so every article here is, and always will be, at no cost. If you want to show financial support, you can buy me a coffee! But I won’t ever make donor-exclusive educational content, so don’t feel like you’re missing out by not donating. It’s just one way to show thanks.\nI hope you enjoy the site, and feel free to reach out via GitHub issues to make suggestions for articles. Thanks for reading!"
  },
  {
    "objectID": "projects/border_crossings/crossings.html",
    "href": "projects/border_crossings/crossings.html",
    "title": "America’s Gates and the Fentanyl Crisis",
    "section": "",
    "text": "Click here for code\nlibrary(leaflet)\nlibrary(plotly)\nlibrary(sf)\nlibrary(tidyverse)\n\n# colors for data viz\nBLUE &lt;- \"#3c91c2\"\nYELLOW &lt;- \"#bbbf43\"\nRED &lt;- \"#d13f5a\"\n\n#border &lt;- read_csv(\"projects/border_crossings/data/crossings.csv\") |&gt;\nborder &lt;- read_csv(\"../../projects/border_crossings/data/crossings.csv\") |&gt;\n  janitor::clean_names()\n\n#od &lt;- read_csv(\"projects/border_crossings/data/overdoses.csv\")\nod &lt;- read_csv(\"../../projects/border_crossings/data/overdoses.csv\")\n\n# modest data cleaning\nborder &lt;- border |&gt;\n  separate(date, into = c(\"month\", \"year\"), sep = \"-\") |&gt;\n  mutate(\n    year = as.numeric(year), \n    year = if_else(year &gt; 25, year + 1900, year + 2000),\n    country = case_when(\n      border == \"US-Canada Border\" ~ \"Canada\",\n      border == \"US-Mexico Border\" ~ \"Mexico\",\n      T ~ NA\n    )\n  )\n\n# filter out 2024 data for in-line reporting below\nborder24 &lt;- border |&gt;\n  filter(year == 2024)\n\np_pov &lt;- border24 |&gt;\n  group_by(measure) |&gt;\n  summarise(total = sum(value)) |&gt;\n  filter(\n    !str_detect(measure, \"Passenger\"),\n    !str_detect(measure, \"Pedestrian\")\n  ) |&gt;\n  mutate(\n    measure = if_else(str_detect(measure, \"Personal\"), measure, \"Non-POV\")\n  ) |&gt;\n  group_by(measure) |&gt;\n  summarise(total = sum(total)) |&gt;\n  pivot_wider(names_from = measure, values_from = total) |&gt;\n  setNames(c(\"non_pov\", \"pov\")) |&gt;\n  mutate(p_pov = pov / (non_pov + pov)) |&gt;\n  pull(p_pov)\n\n# aggregate some 2024-specific data\nmost_common &lt;- border24 |&gt;\n  group_by(port_code, measure) |&gt;\n  summarize(total_value = sum(value, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(port_code, desc(total_value)) |&gt;\n  group_by(port_code) |&gt;\n  slice(1) |&gt;\n  select(port_code, most_common = measure)\n\nports &lt;- border24 |&gt;\n  select(port_code, port_name, state, latitude, longitude, country) |&gt;\n  distinct()\n\ncrossings24 &lt;- border24 |&gt;\n  group_by(port_code) |&gt;\n  summarise(crossings = sum(value)) |&gt;\n  left_join(ports) |&gt;\n  left_join(most_common)"
  },
  {
    "objectID": "projects/border_crossings/crossings.html#todays-crossings",
    "href": "projects/border_crossings/crossings.html#todays-crossings",
    "title": "America’s Gates and the Fentanyl Crisis",
    "section": "Today’s crossings",
    "text": "Today’s crossings\nBefore we explore our past, let’s get to know our present. In 2024, there were 366,523,458 border crossings across every American land port. Over 72% of the vehicles that crossed were personal vehicles, like your car or mine. Although there are 117 unique ports, not all entrances are created equal. Let’s look at a map to see where most of our inbound traffic comes from, and which types of traffic are most common there. Figure 1 is an interactive map showing the traffic coming through our land ports in 2024.\n\n\n\n\n\n\nTip\n\n\n\nFeel free to click on any circle for additional data.\n\n\n\n\nClick here for code\n# custom color palette  \ncolor_palette &lt;- colorFactor(\n  palette = c(BLUE, \"yellow\", RED, \"black\"),\n  domain = crossings24$most_common\n)\n\n# custom CSS for styling the map title\nmap_title &lt;- paste(\n  \"&lt;div class='leaflet-control'\",\n  \"style='padding-top: 0px;\",\n  \"padding-bottom: 8px;\",\n  \"padding-left: 11px;\",\n  \"font-size: 22px;'&gt;\",\n  \"&lt;b&gt;Inbound Border Crossings (2024)&lt;/b&gt;&lt;/div&gt;\"\n)\n\n# build the map\nleaflet(crossings24) |&gt;\n  addTiles() |&gt;\n  addCircleMarkers(\n    lng = ~longitude,\n    lat = ~latitude,\n    # set a minimum circle size of 3 (helps with legibility)\n    radius = ~pmax(sqrt(crossings) / 200, 3),\n    fill = TRUE,\n    stroke = FALSE,\n    fillOpacity = 0.7, \n    popup = ~paste0(\n      \"&lt;strong&gt;Port: &lt;/strong&gt;\", port_name,\n      \"&lt;br&gt;&lt;strong&gt;Most Common Method: &lt;/strong&gt;\", most_common,\n      \"&lt;br&gt;&lt;strong&gt;Total Crossings: &lt;/strong&gt;\", \n      # format X,XXX,XXX instead of XXXXXXX\n      scales::comma(crossings)\n    ),\n    color = ~color_palette(most_common)\n  ) |&gt;\n  addLegend(\n    \"bottomright\",\n    pal = color_palette,\n    values = ~most_common,\n    title = \"Most Common Method of Crossing\"\n  ) |&gt;\n  # add custom title box to the map\n  addControl(map_title, position = \"topright\")\n\n\n\n\n\n\n\n\nFigure 1: Crossing locations\n\n\n\n\nThat sea of red you’re seeing shows how dominant personally owned vehicles are at nearly every land port. Of our two borders, there was significantly more traffic from the south; the only high-traffic entrances from the north are Buffalo, Detroit, and Blaine (outside of Vancouver). However, the San Ysidro port of entry in southern California outnumbered all crossings from those three ports combined. The majority of our land traffic very clearly comes from Mexico and from privately owned vehicles."
  },
  {
    "objectID": "projects/border_crossings/crossings.html#total-crossings",
    "href": "projects/border_crossings/crossings.html#total-crossings",
    "title": "America’s Gates and the Fentanyl Crisis",
    "section": "Total crossings",
    "text": "Total crossings\nTraffic across our land borders has become a hot topic in our politics. President Trump has imposed tariffs on both Mexico and Canada in an attempt to reduce the flow of illicit drugs (specifically fentanyl) and trafficked people into the US. Because most fentanyl is smuggled into the US by American citizens, it is worth investigating total crossings instead of crossings made by immigrants. Figure 2 explores our total inbound crossings per year in all land ports in the US.\n\n\nClick here for code\n# build plot\nborder |&gt;\n  filter(year &lt; 2025) |&gt;\n  group_by(year) |&gt;\n  summarise(crossings = sum(value)) |&gt;\n  plot_ly() |&gt;\n  add_trace(\n    x = ~year, \n    y = ~crossings, \n    type = \"scatter\", \n    mode = \"lines+markers\",\n    line = list(width = 4, color = RED),\n    marker = list(size = 11, color = RED),\n    hovertemplate = \"&lt;b&gt;Crossings&lt;/b&gt;: %{y:,.0f}\",\n    name = \"\"\n  ) |&gt;\n  layout(\n    title = \"Border crossings into the US through land ports (1996-2024)\",\n    xaxis = list(title = \"\"),\n    yaxis = list(title = \"Total Crossings\"),\n    dragmode = FALSE,\n    hovermode = \"x unified\"\n  ) |&gt;\n  config(displayModeBar = FALSE)\n\n\n\n\n\n\n\n\nFigure 2: Total crossings\n\n\n\n\nThere are a few interesting notes from our border crossing history. First, we saw a rapid and steady decrease in border crossings after 2000. That could be related to decreased international travel by American citizens following the 9/11 attacks and increased security, but without data to back that hypothesis, we can only speculate.\nAfter 2011, we see a slow increase in crossings until the pronounced drop during the COVID-19 pandemic. It’s more difficult to point to direct policy changes that explain that increase during both the Obama and Trump administrations, so I will reserve my speculation. Interestingly, we have effectively returned to pre-COVID levels.\n\n\n\n\n\n\nNote\n\n\n\nObserve the y-axis scale in Figure 2. Even during the drop in crossings during the pandemic, we saw over 200 million unique crossings. Although that’s much lower than is typical, it is still significant in absolute number."
  },
  {
    "objectID": "projects/border_crossings/crossings.html#crossing-distribution",
    "href": "projects/border_crossings/crossings.html#crossing-distribution",
    "title": "America’s Gates and the Fentanyl Crisis",
    "section": "Crossing distribution",
    "text": "Crossing distribution\nSince effectively all fentanyl imports come through the southern border and via US citizens, it is worth exploring the distribution of crossings between Canada and Mexico. Figure 3 examines the evolution of that distribution since 1996.\n\n\nClick here for code\n# get proportion of overdoses done with opioids\nod_prop &lt;- od |&gt;\n  mutate(p_opioid = opioid_deaths / total_deaths)\n\n# note for Canadian border measures annotation\ncanada_note &lt;- list(\n  x = 2021,\n  y = 0.47,\n  text = \"&lt;b&gt;Canadian COVID measures&lt;/b&gt;\",\n  showarrow = FALSE,\n  textangle = 270,\n  font = list(\n    color = \"rgba(0, 0, 0, 0.2)\",\n    size = 14\n  )\n)\n\n# build the plot\nborder |&gt;\n  filter(year &lt; 2025) |&gt;\n  group_by(year, country) |&gt;\n  summarise(crossings = sum(value), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = country, values_from = crossings) |&gt;\n  mutate(\n    crossings = Canada + Mexico,\n    p_canada = Canada / crossings,\n    p_mex = Mexico / crossings\n  ) |&gt;\n  left_join(od_prop) |&gt;\n  \n  # Mexico line\n  plot_ly(\n    x = ~year, \n    y = ~p_mex,\n    type = \"scatter\",\n    mode = \"lines+markers\",\n    line = list(width = 4, color = RED),\n    marker = list(size = 11, color = RED),\n    hovertemplate = \"%{y:.3f}\",\n    name = \"&lt;b&gt;Mexico&lt;/b&gt;\"\n  ) |&gt;\n  \n  # Canada line\n  add_trace(\n    y = ~p_canada,\n    line = list(width = 4, color = BLUE),\n    marker = list(\n      size = 11, \n      color = BLUE,\n      symbol = \"square\"\n    ),\n    hovertemplate = \"%{y:.3f}\",\n    name = \"&lt;b&gt;Canada&lt;/b&gt;\"\n  ) |&gt;\n  \n  # opioid line\n  add_trace(\n    y = ~p_opioid,\n    line = list(width = 4, color = YELLOW),\n    marker = list(\n      size = 11, \n      color = YELLOW,\n      symbol = \"diamond\"\n    ),\n    hovertemplate = \"%{y:.3f}\",\n    name = \"&lt;b&gt;Opioid OD\\nProportion&lt;/b&gt;\"\n  ) |&gt;\n  \n  layout(\n    title = \"Border crossing proportions and opioid deaths\",\n    xaxis = list(title = \"\"),\n    yaxis = list(title = \"Proportion of Crossings\"),\n    dragmode = FALSE,\n    \n    # add mouse-over functionality across both lines at once\n    hovermode = \"x unified\",\n    \n    # Canadian border closure annotation bar\n    shapes = list(\n      list(type = \"rect\", fillcolor = \"gray\", line = list(color = \"gray\"),\n      opacity = 0.2,\n      y0 = 0.05,\n      y1 = 0.95,\n      x0 = 2020,\n      x1 = 2022,\n      layer = \"below\"\n    )),\n    annotations = canada_note\n  ) |&gt;\n  config(displayModeBar = FALSE)\n\n\n\n\n\n\n\n\nFigure 3: Crossing distribution\n\n\n\n\nEven as Figure 2 shows us total crossings have fluctuated by the hundreds of millions since 1997, the proportion of those crossings coming from Canada and Mexico have been remarkably consistent. Besides the Canadian travel measures causing dwindling crossings during the pandemic, about three of every four crossings have come from Mexico every year since 1997.\nWhile opioid deaths have been growing as a proportion of all overdose deaths since 1999, the proportion of crossings from each border remains consistent, and total land crossings have stagnated (save COVID-era declines). With total traffic not growing and the proportion of crossings coming from the south constant, factors outside of these datasets are likely responsible for the increase in opioid deaths. The per-dose lethality of fentanyl, industry-manufactured opioid addictions, and other socioeconomic issues are more likely culprits."
  },
  {
    "objectID": "projects/gun_violence/gun_violence.html",
    "href": "projects/gun_violence/gun_violence.html",
    "title": "The Lethality of the American Gun Lobby",
    "section": "",
    "text": "Click here for code\nlibrary(ggthemes)\nlibrary(gt)\nlibrary(tidymodels)\nlibrary(tidyverse)\n\nset.seed(440)\n\ndeath &lt;- read_csv(\"data/total_death_data.csv\")\ngun_group_count &lt;- read_csv(\"data/gun_interest.csv\")\n\nmean_death_year &lt;- death |&gt;\n  group_by(year, state) |&gt;\n  summarise(\n    population = sum(population),\n    gun_suicides = sum(gun_suicides, na.rm = TRUE),\n    gun_rate = mean(mean_fs_s, na.rm = TRUE),\n    poor_mental_rate = mean(p_poor_mental, na.rm = TRUE)\n  )\nClick here for code\nstates &lt;- death |&gt;\n  filter(year &gt;= min(gun_group_count$year)) |&gt;\n  left_join(gun_group_count) |&gt;\n  mutate(\n    lobbyist_positions = if_else(\n      is.na(lobbyist_positions), \n      0, \n      lobbyist_positions\n    ),\n    gun_suicides_per_100k = gun_suicides / (population / 100000),\n    log_positions = log(lobbyist_positions)\n  ) |&gt;\n  left_join(mean_death_year) |&gt;\n  select(-c(fem_fs_s, male_fs_s)) |&gt;\n  filter(lobbyist_positions &gt; 0)\n\n# choose state with the median gun_suicides_per_100k as baseline\nstates_grouped &lt;- states |&gt;\n  group_by(state) |&gt;\n  summarise(means = mean(gun_suicides_per_100k, na.rm = T))\nmedian_rate &lt;- median(states_grouped$means, na.rm = T)\nbaseline_state &lt;- states_grouped |&gt;\n  filter(means == median_rate) |&gt;\n  pull(state)\n\n# get mean number of special interest statements per year/state\nmean_log_positions &lt;- mean(states$log_positions)\n\n# create final dataset for modeling\nmodel_data &lt;- states |&gt;\n  mutate(\n    state = factor(\n      state, \n      levels = c(baseline_state, sort(unique(state[state != baseline_state])))\n    ),\n    active_lobby = if_else(lobbyist_positions &gt; 0, T, F),\n    region = case_when(\n      state %in% c(\"Texas\", \"Florida\") ~ \"South\",\n      state %in% c(\"South Dakota\", \"Nebraska\", \"Kansas\", \n                   \"Missouri\", \"Iowa\") ~ \"Midwest\",\n      state %in% c(\"Wisconsin\", \"Illinois\", \"Ohio\") ~ \"Great Lakes\",\n      state %in% c(\"Montana\", \"Colorado\", \"Arizona\") ~ \"West\",\n      state %in% c(\"Massachusetts\", \"New Jersey\", \"Rhode Island\", \n                   \"Maryland\") ~ \"Northeast\",\n      TRUE ~ \"Other\"\n    )\n  ) |&gt;\n  filter(state != \"District of Columbia\") |&gt;\n  na.omit()"
  },
  {
    "objectID": "projects/gun_violence/gun_violence.html#the-data",
    "href": "projects/gun_violence/gun_violence.html#the-data",
    "title": "The Lethality of the American Gun Lobby",
    "section": "The Data",
    "text": "The Data\nWe have aggregated and cleaned data from three distinct sources. First, mental health data is pulled from the CDC Behavioral Risk Factor Surveillance Survey (BRFSS). The survey began including mental health questions in 1994. The question of interest for this study is:\n“Now thinking about your mental health, which includes stress, depression, and problems with emotions, for how many days during the past 30 days was your mental health not good?”\nResponses are given an integer value between 1 and 30. Respondents with zero days of poor mental health are encoded with the value 88, which we have re-encoded as 0. Non-responses are not considered. Ultimately, 10,249,613 were considered, aggregated by state and year, and included in the model. We encode these responses as binary: “Poor” or “Good” mental health. The CDC defines 15 or more days of poor mental health in a 30-day period as concerning, so we encode responses of 15 or greater as “Poor” and others as “Good.”\nThe CDC also provides absolute number and per-capita suicide data by state through their CDC WONDER platform. Per-state proportions of suicides that were completed with firearms were compiled by the RAND Corporation, a non-profit, non-partisan think tank that tracks and reports on several high-profile political issues in the United States. We calculate total firearm suicide deaths by multiplying the total suicides of all types reported by the CDC and the proportion of those that were completed by firearm reported by RAND.\nSpecial interest group activity is provided by Hall et al. (2024). Their special interest dataset reports 13,619,409 individual state-level special interest group positions. We filter those positions using regular expression filters built by hand from group names listed on OpenSecrets, the website of the eponymous non-profit organization that tracks lobbying groups and campaign finance in Washington, DC. We aggregate firearm-related special interest group activity by year and state. Once gun-related groups were compiled, we manually reviewed the 1,168 groups to remove any anti-firearm special interest groups from the data. Note that manual review is prone to human error. Further details of these limitations are in the discussion section of this article.\nMost year/state combinations have no special interest group representation (i.e., the number of reported special interest statements is 0). Initially, we attempted to include these zero values but ran into issues with heteroscedasticity. Ultimately, we maintained homoscedasticity by removing rows with no special interest involvement. This reduction in data lessened the scope of our findings to states in which the pro-gun lobby has been active since 1997 but allowed us to construct a more trustworthy model. The resulting dataset is 186 observations in which a pro-gun group was active in a given state and year."
  },
  {
    "objectID": "projects/tidytuesday_01212025/mountains.html",
    "href": "projects/tidytuesday_01212025/mountains.html",
    "title": "Death on the Mountains",
    "section": "",
    "text": "Introduction\nHello all! Welcome to TidyTuesday! This week, journalist Elizabeth Hawley provides us with data that documents mountaineering expeditions in the Nepal Himalaya, the mountain range that includes Mount Everest. There were a few variables that piqued my interest, so I thought we could build a model to see if any of my those variables are related to fatalities during expeditions. Let’s read and clean the data!\n\n\nClick here for code\nlibrary(ggthemes)\nlibrary(gt)\nlibrary(tidyverse)\n\nexped &lt;- read_csv(\n  paste0(\n    \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/\",\n    \"main/data/2025/2025-01-21/exped_tidy.csv\"\n  )\n) |&gt;\n  janitor::clean_names()\n\npeaks &lt;- read_csv(\n  paste0(\n    \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/\",\n    \"main/data/2025/2025-01-21/peaks_tidy.csv\"\n  )\n) |&gt;\n  janitor::clean_names()\n\nclimbs &lt;- exped |&gt;\n  left_join(peaks) |&gt;\n  mutate(\n    deaths = mdeaths + hdeaths,\n    is_fatal = if_else(deaths &gt; 0, T, F),\n    season_factor = factor(\n      season_factor,\n      levels = c(\"Winter\", \"Spring\", \"Summer\", \"Autumn\")\n    ),\n    agency = factor(agency)\n  ) |&gt;\n  drop_na(is_fatal)\n\n\n\n\nExploratory Analysis\nOf course, it wouldn’t be TidyTuesday without a little bit of data viz. I’m thinking that oxygen use might predict deaths, although I have absolutely no domain knowledge in the field of mountaineering. My concern is that oxygen is only used after a certain altitude, so oxygen use and mountain height may be highly correlated. Let’s see if that’s the case, adding in the number of days that an expedition takes as a second axis.\n\n\nClick here for code\nclimbs |&gt;\n  mutate(o2used = if_else(o2used, \"O2 used\", \"No O2 used\")) |&gt;\n  ggplot(aes(x = highpoint, y = totdays, color = o2used, shape = o2used)) +\n  geom_jitter(size = 2.5) +\n  theme_fivethirtyeight() +\n  scale_color_fivethirtyeight() +\n  labs(\n    title = \"Higher altitude means oxygen use\",\n    subtitle = \"Summit height, length of journey, and oxygen\",\n    x = \"High point (m)\",\n    y = \"Total days\"\n  ) +\n  theme(\n    axis.title = element_text(),\n    legend.title = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nSo my assumption was correct: oxygen is mostly used at altitudes over 7,500 meters. I’ll add both to the model to see if one is more significant than the other. I’m also curious to see if winter expeditions are more lethal, so we’ll use winter as the baseline season against which we will compare all other seasons. I’ll also add the high point of the expedition, since we want to hold it constant when analyzing the effect of oxygen use.\nBefore we build our model, let’s see how many of our expeditions proved fatal.\n\n\nThe Model\n\n\nClick here for code\nclimbs |&gt;\n  mutate(is_fatal = if_else(is_fatal, \"Fatal\", \"Non-Fatal\")) |&gt;\n  group_by(is_fatal) |&gt;\n  summarise(prop = n() / nrow(climbs)) |&gt;\n  rename(Fatality = \"is_fatal\", Proportion = \"prop\") |&gt;\n  gt() |&gt;\n  fmt_number(columns = Proportion, decimals = 3) |&gt;\n  tab_header(md(\"**Expedition Fatalities**\"))\n\n\n\n\n\n\n\n\nExpedition Fatalities\n\n\nFatality\nProportion\n\n\n\n\nFatal\n0.043\n\n\nNon-Fatal\n0.957\n\n\n\n\n\n\n\nUh-oh, only 4% of the expeditions were fatal. With such an imbalance between the binary outcomes, ordinary logistic regression may struggle with class imbalance. Instead, we will use Firth’s logistic regression, which is designed to combat this exact problem. It will penalize the likelihood function using a penalty term related to the (Jefferys Prior)[https://en.wikipedia.org/wiki/Jeffreys_prior]. Long story short, it will help correct for the class imbalance issue. Let’s build the model and see what we get!\nNote: For ease of interpretation, I have exponentiated the coefficients.\n\n\nClick here for code\n# using Firth's logistic regression to account for few TRUE response values\nmodel &lt;- logistf::logistf(\n  is_fatal ~ highpoint + season_factor + o2used + totdays, \n  data = climbs\n)\n\nmodel_summary &lt;- summary(model)\nresults &lt;- tibble(\n  term = names(model_summary$coefficients),\n  coef = exp(model_summary$coefficients),\n  p_value = model_summary$prob\n)\n\nresults$term &lt;- c(\"Intercept\", \"High Point\", \"Season | Spring\",\n                  \"Season | Summer\", \"Season | Fall\", \"O2 Used\",\n                  \"Total Days\")\ncolnames(results) &lt;- c(\"Variable\", \"Coefficients\", \"P-value\")\n\n\n\n\nClick here for code\nresults |&gt;\n  filter(Variable != \"Intercept\") |&gt;\n  gt() |&gt;\n  fmt_number(columns = c(\"Coefficients\", \"P-value\"), decimals = 3) |&gt;\n  tab_header(md(\"**Model Results**\"))\n\n\n\n\n\n\n\n\nModel Results\n\n\nVariable\nCoefficients\nP-value\n\n\n\n\nHigh Point\n1.000\n0.603\n\n\nSeason | Spring\n1.156\n0.921\n\n\nSeason | Summer\n4.270\n0.495\n\n\nSeason | Fall\n0.711\n0.826\n\n\nO2 Used\n4.780\n0.005\n\n\nTotal Days\n0.990\n0.389\n\n\n\n\n\n\n\n\n\nResults\nInterpreting logistic regression can be awkward, so let’s start with the p-values. At the \\(\\alpha = 0.05\\) level, only the use of oxygen shows a significant association with the fatality of an expedition. Holding season, high point, and total days constant, the use of oxygen increases the probability of a fatality by 4.78-fold. Interestingly, season has no statistically significant association with fatality when holding other variables constant. I assumed that the baseline season (winter) would be significantly more fatal, but that’s not the case.\nHowever, it’s important to recognize the limitations of our model. Oxygen use can be caused by many factors. We have already shown that height predicts oxygen use, but so can medical emergencies or other variables that can appear during an expedition. It would be worth doing more analysis on oxygen use specifically to look for a causal relationship, rather than simple association.\n\n\nConclusion\nThanks for your attention! Firth’s logistic regression was new to me, so I got to learn from the modeling process and from the model itself. Lucky me! I hope you got something out of this little analysis project, and if you’d like to ask me any questions, feel free to send me a connect request on LinkedIn!\nThanks for your attention, and I’ll see you next time!"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "These are my independent projects! They can be quick visualizations, full analyses with write-ups, and even interactive data exploration dashboards. Feel free to use the tags on the right side of the page to navigate, and reach out to me on Discord with any questions. Thanks!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmerica’s Gates and the Fentanyl Crisis\n\n\n\nData Viz\n\n\nResearch\n\n\nGeospatial\n\n\n\n\n\n\n\nMitch Harrison\n\n\nMar 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeath on the Mountains\n\n\n\nData Viz\n\n\nTidyTuesday\n\n\n\n\n\n\n\nMitch Harrison\n\n\nJan 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosit::Conf 2023\n\n\n\nData Viz\n\n\nTidyTuesday\n\n\n\n\n\n\n\nMitch Harrison\n\n\nJan 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Lethality of the American Gun Lobby\n\n\n\nData Viz\n\n\nResearch\n\n\n\n\n\n\n\nMitch Harrison\n\n\nDec 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCricket Crackers: A Bayesian Approach\n\n\n\nBayesian Statistics\n\n\nData Viz\n\n\nResearch\n\n\n\n\n\n\n\nMitch Harrison\n\n\nOct 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCampus Pride Index\n\n\n\nData Viz\n\n\nTidyTuesday\n\n\n\n\n\n\n\nMitch Harrison\n\n\nJun 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Cheese Explorer\n\n\n\nData Viz\n\n\nTidyTuesday\n\n\nDashboard\n\n\n\n\n\n\n\nMitch Harrison\n\n\nJun 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur World in Emissions\n\n\n\nData Viz\n\n\nTidyTuesday\n\n\n\n\n\n\n\nMitch Harrison\n\n\nMay 21, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/math_stat_1.html",
    "href": "tutorials/math_stat_1.html",
    "title": "Welcome to Estimators! | Mathematical Statistics 1",
    "section": "",
    "text": "Say we have some data \\(\\mathbf{X}\\). It’s a vector, so just think of it like a list of \\(n\\) numbers. We want to learn something about how these data came to be. First, we will aggregate our data using a statistic.\n\n\n\n\n\n\nDefinition\n\n\n\nLet \\(X_1, \\cdots, X_n\\) be our data. A statistic is a function of that data. We will denote that statistic with \\(\\delta\\). Crucially, this function cannot contain anything that we don’t know. It is purely a function of known quantities.\n\n\n\n\nLet’s say that our data comes from a normal distribution (a “bell curve”). We denote this with \\(X \\sim N(\\mu, \\sigma^2)\\), where \\(\\mu\\) is the mean of the distribution and \\(\\sigma^2\\) is the variance. Also, to make our life easier, say we know the variance \\(\\sigma^2\\). In practice, this will basically never be the case, but it will simplify our math for now.\nWe have infinitely many options for statistics that we can choose. For example, we could use \\(X_1\\) (that is, the first data point in our vector). While we leave some data on the table in that case, it is certainly a statistic since \\(\\delta = X_1\\) is a function of our data, and there are no unknowns.\nAlternatively, we could use the observed mean of our data. We will call it \\(\\overline{X}\\) (pronounced “\\(X\\) bar”), and it is denoted with \\[\n\\delta(\\mathbf{X}) = \\overline{X} = \\frac{1}{n}\\sum_{i=1}^n X_i.\n\\]\nNotice that this is also a statistic! Although it looks much more complicated, we are still using our data and no unknowns. Here, \\(n\\) is the number of data points that we have, which we know. And we know every \\(X_i\\) because each is part of our data vector \\(\\mathbf{X}\\).\n\n\n\n\n\n\nNote\n\n\n\nConstants (e.g., 7) are also statistics, although no data are involved in the calculation. If it feels like you’re just guessing at random if you do this, you’re right.\n\n\nNow let’s look at an example of a function that is not a statistic: \\[\n\\delta(\\mathbf{X}) = T = \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}}.\n\\] This function will come back in future articles, but for now, recall that we said that we already know the variance \\(\\sigma^2\\). So that means we already know \\(\\sigma\\). We also know \\(n\\), as we mentioned earlier. But \\(\\mu\\) is unknown to us. Because we have an unknown value \\(\\mu\\) in the numerator, \\(T\\) is not a statistic."
  },
  {
    "objectID": "tutorials/math_stat_1.html#statistics",
    "href": "tutorials/math_stat_1.html#statistics",
    "title": "Welcome to Estimators! | Mathematical Statistics 1",
    "section": "",
    "text": "Let’s say that our data comes from a normal distribution (a “bell curve”). We denote this with \\(X \\sim N(\\mu, \\sigma^2)\\), where \\(\\mu\\) is the mean of the distribution and \\(\\sigma^2\\) is the variance. Also, to make our life easier, say we know the variance \\(\\sigma^2\\). In practice, this will basically never be the case, but it will simplify our math for now.\nWe have infinitely many options for statistics that we can choose. For example, we could use \\(X_1\\) (that is, the first data point in our vector). While we leave some data on the table in that case, it is certainly a statistic since \\(\\delta = X_1\\) is a function of our data, and there are no unknowns.\nAlternatively, we could use the observed mean of our data. We will call it \\(\\overline{X}\\) (pronounced “\\(X\\) bar”), and it is denoted with \\[\n\\delta(\\mathbf{X}) = \\overline{X} = \\frac{1}{n}\\sum_{i=1}^n X_i.\n\\]\nNotice that this is also a statistic! Although it looks much more complicated, we are still using our data and no unknowns. Here, \\(n\\) is the number of data points that we have, which we know. And we know every \\(X_i\\) because each is part of our data vector \\(\\mathbf{X}\\).\n\n\n\n\n\n\nNote\n\n\n\nConstants (e.g., 7) are also statistics, although no data are involved in the calculation. If it feels like you’re just guessing at random if you do this, you’re right.\n\n\nNow let’s look at an example of a function that is not a statistic: \\[\n\\delta(\\mathbf{X}) = T = \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}}.\n\\] This function will come back in future articles, but for now, recall that we said that we already know the variance \\(\\sigma^2\\). So that means we already know \\(\\sigma\\). We also know \\(n\\), as we mentioned earlier. But \\(\\mu\\) is unknown to us. Because we have an unknown value \\(\\mu\\) in the numerator, \\(T\\) is not a statistic."
  },
  {
    "objectID": "tutorials/math_stat_1.html#point-estimator-example",
    "href": "tutorials/math_stat_1.html#point-estimator-example",
    "title": "Welcome to Estimators! | Mathematical Statistics 1",
    "section": "Point estimator example",
    "text": "Point estimator example\nLet’s keep going with our data, which comes from a normal distribution. But, to get used to using \\(\\theta\\), say that \\(X \\sim N(\\theta, \\sigma^2)\\). One possible estimator is the example mean \\(\\overline{X}\\) from earlier (i.e., the mean of the observed data). Alternatively, we can use a constant: say 5. Intuitively, it feels like \\(\\hat{\\theta} = \\overline{X}\\) would be a better guess than a simple \\(\\hat{\\theta} = 5\\), because it is actually informed by the data. But how do we quantify that intuition? We will calculate and compare both bias and precision for each.\n\nBias\nBias tells us how often, on average, we get the correct value of our unknown parameter \\(\\theta\\). Mathematically, we hope that the following quantity is as small as possible: \\[\n\\mathbb{E}[\\delta(\\mathbf{X}) | \\theta] - \\theta.\n\\]\nThe confusing-looking term \\(\\mathbb{E}[\\cdot]\\) is the expected value of our estimator, given the value of the unknown parameter \\(\\theta\\). Basically, this is the expected value of \\(\\hat{\\theta}\\). If our estimator \\(\\hat{\\theta}\\) is expected to be exactly correct on average, then this whole term will be 0, which is the smallest possible bias.\n\n\nVariance\nVariance describes the variability of our estimator. Ideally, variance is also small. Intuitively we are less “sure” about our estimate if we have a wider variance. We denote variance with \\(Var(\\delta(\\mathbf{X})|\\theta)\\).\nHowever, notice that both bias and variance are conditional on the true value of our unknown parameter \\(\\theta\\). Thus, we cannot calculate these quantities directly. To deal with this, we will introduce the concept of loss in the next article here!"
  },
  {
    "objectID": "tutorials/math_stat_3.html",
    "href": "tutorials/math_stat_3.html",
    "title": "Jensen’s Inequality | Mathematical Statistics 3",
    "section": "",
    "text": "Hello again! Last time, we found the bias and variance of two different estimators and discussed the bias-variance tradeoff. Now that we have some necessary background knowledge, let’s discuss balancing bias and variance to find a better estimator. Recall that we want our estimator to be accurate (have a small bias) and precise (have a small variance).\nWe mentioned the mean squared error (MSE) as a common way of analyzing an estimator. In the below equality, we only discussed the left-hand side. But now that we know what bias and variance are, we can observe a new (but equivalent) definition of the MSE:\n\\[\n\\mathbb{E}[(\\delta(\\mathbf{X}) - \\theta)^2] =\nVar[\\delta(\\mathbf{X}) | \\theta] + bias^2[\\delta(\\mathbf{X}) | \\theta].\n\\]\nThere, \\(\\mathbf{X}\\) is our data, \\(\\theta\\) is our unknown parameter, \\(\\delta(\\mathbf{X})\\) is our estimator, and \\(bias^2[\\cdot]\\) denotes our estimator’s bias squared."
  },
  {
    "objectID": "tutorials/math_stat_3.html#bias-example",
    "href": "tutorials/math_stat_3.html#bias-example",
    "title": "Jensen’s Inequality | Mathematical Statistics 3",
    "section": "Bias example",
    "text": "Bias example\nWe are scientists. We are confident that the machine in our lab is working, and it is spitting out data that comes from the following distribution:\n\\[\nX_1, \\cdots, X_n \\overset{\\mathrm{iid}}{\\sim} Exp(\\theta).\n\\]\nThe expected value (mean) of an exponential distribution is \\(\\mathbb{E}[X] = 1/ \\theta\\). We also know (or find on Wikipedia) that the probability density function (PDF) of an exponential is given by:\n\\[\nf_ \\theta(x) = \\theta e^{- \\theta x}.\n\\]\nOur goal is to study \\(\\theta\\). Let’s re-arrange the expected value of the exponential distribution to isolate \\(\\theta\\):\n\\[\\begin{align*}\n\\mathbb{E}[X] &= \\frac{1}{ \\theta} \\\\\n\\theta\\mathbb{E}[X] &= 1 \\\\\n\\theta &= \\frac{1}{\\mathbb{E}[X]}.\n\\end{align*}\\]\nNow that we have isolated \\(\\theta\\), we can start guessing at some estimators. Let’s replace \\(\\mathbb{E}[X]\\) with our sample mean (the average of whatever data we end up with). Like we did before, let’s call it \\(\\overline{X}\\). Then our first estimator will be \\[\n\\hat{\\theta}_1 = \\frac{1}{\\overline{X}}\n\\]\nLet’s see if our new estimator is unbiased. That is, if it has zero bias.\n\n\n\n\n\n\nDefinition\n\n\n\nJensen’s inequality: Let \\(g(\\cdot)\\) be a convex function (i.e., it has a positive second derivative). In that case, \\[\n\\mathbb{E}[g(X)] \\ge g(\\mathbb{E}[X])\n\\] with equality if \\(g\\) is linear.\n\n\nIn our case, \\(g(X) = \\hat{\\theta}_1\\). Therefore, Jensen’s inequality tells us that\n\\[\n\\mathbb{E}\\left[\\frac{1}{\\overline{X}}\\right] &gt; \\frac{1}{\\mathbb{E}[X]}.\n\\]\nWe noted earlier that the expected value of the exponential distribution that we are working with is \\(1/\\theta\\). Then,\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\frac{1}{\\overline{X}}\\right]\n&&gt; \\frac{1}{\\mathbb{E}[X]} \\\\\n&&gt; \\frac{1}{\\frac{1}{\\theta}} \\\\\n&&gt; \\theta \\\\\n\\mathbb{E}[\\hat{\\theta}_1] &&gt; \\theta.\n\\end{align*}\\]\nSince the expected value of our estimator is not \\(\\theta\\), we have some non-zero bias. So, we know that our estimator is biased! Thanks for help Jensen.\nInstead of finding whether or not there is bias, we may want to calculate the exact bias of our estimator. For this example, doing so becomes quite the exercise in calculus and involves factoring constants out of integrals until those integrals evaluate to 1 (because we turn them into PDFs). Then we can safely get rid of them. But the math is such that it may be worth doing in a separate, smaller article. So for now, Jensen has at least showed us that our estimator is biased, and we will call that a win!"
  },
  {
    "objectID": "tutorials/tidytuesday_06112024.html",
    "href": "tutorials/tidytuesday_06112024.html",
    "title": "Campus Pride Index | TidyTutorial",
    "section": "",
    "text": "Introduction\nIn celebration of Pride Month, this week’s TidyTuesday provides data from the Campus Pride Index, which measures the safety and inclusivity of LGBTQ+ programs across universities in the United States.\nEach university is binned into one or more categories (e.g., military colleges, private/public, and others). What feels natural to me is to see how the Campus Pride Index compares across some of these categories. A proportionate stacked bar chart (where each bar has height equal to 1) is one option, but I would also like to see which types of universities are most common. If there are some categories with worse scores but with much smaller sample sizes, that would be helpful to know. So we’ll use a stacked bar, but not normalize the bar so we can also see how common each type is. Also bear in mind that a single university can (and often does) fall into multiple categories.\nLet’s set some global settings so I don’t have to worry about aspect ratio or other trivialities while we work.\n\n\nClick here for code\nknitr::opts_chunk$set(\n  fig.width = 10,        \n  fig.asp = 0.618,      # the golden ratio\n  fig.align = \"center\"  # center align figures\n)\n\n\n\n\nData Wrangling\nTime to load the data.\n\n\nClick here for code\nlibrary(tidyverse)\nlibrary(gglgbtq)\nlibrary(ggchicklet)\nlibrary(ggthemes)\nlibrary(DT)\n\n# load the data ----------------------------------------------------------------\n\npride_schools &lt;- read_csv(paste0(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/\",\n  \"2024/2024-06-11/pride_index.csv\"\n))\n\npride_tags &lt;- read_csv(paste0(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/\",\n  \"2024/2024-06-11/pride_index_tags.csv\"\n))\n\ndatatable(left_join(pride_schools, pride_tags))\n\n\n\n\n\n\nFirst, let’s format the data for ease of plotting. Right now, each category has its own column, with TRUE or NA values, where NA means “false” for our purposes. But we want the type of school to be represented in a single column so we can map that column to the color of the bars. To move multiple columns into a single one, we will pivot the data. Since we want to consolidate columns, we will need to make our data longer (i.e., add more rows), where each university now has multiple rows corresponding to TRUE or FALSE. Intuitively, to pivot the data longer, we use the pivot_longer function. Notice that once the pivot is complete, we only want to keep the rows where the value is TRUE, since the FALSE rows are just saying that “this university doesn’t fall into this type,” which is useless noise in our dataset.\n\n\nClick here for code\n# format data for plotting -----------------------------------------------------\n\nuni_types &lt;- pride_schools |&gt;\n  \n  # join both datasets into one\n  left_join(pride_tags) |&gt;\n  \n  # select which columns we want to analyze along with their ratings\n  select(rating, public, private, community, liberal_arts, technical,\n         religious, military, hbcu, hispanic_serving, aapi_serving,\n         other_minority_serving) |&gt; \n  \n  # replace NA with FALSE\n  mutate(across(everything(), ~ replace_na(., FALSE))) |&gt;\n  \n  # do the pivot\n  pivot_longer(cols = !rating, names_to = \"type\") |&gt;\n  \n  # drop the rows that don't apply\n  filter(value == TRUE) |&gt;\n  \n  # clean up some strings for prettier plotting\n  mutate(\n    type = str_replace_all(type, \"_\", \" \"),\n    type = str_to_title(type),\n    type = str_replace(type, \"Aapi\", \"AAPI\")\n  )\n\ndatatable(uni_types)\n\n\n\n\n\n\nThat looks just like we wanted it to. Now that our data is formatting, we can work on the plot. Per the data dictionary on the TidyVerse GitHub repository, we know that fractional scores are possible. A quick call to the unique function told me that the “fractional” scores are only half-stars, not any decimal in between two scores. So 1 and 1.5 are possible scores, but 1.7 is not. We should bin these scores by their leading digit so we have five possible fill values instead of ten. We’ll call these bins rating_levs, or “rating levels.”\nI would also like to order the bars in descending order of the total number of universities of that type. To do that, we’ll count how many of each category there are and save the order as a vector uni_levs, or “university levels.”\n\n\nClick here for code\nuni_levs &lt;- uni_types |&gt;\n  group_by(type) |&gt;\n  summarise(count = n()) |&gt;\n  arrange(desc(count)) |&gt;\n  pull(type)\n\nrating_levs &lt;- c(\"1 - 1.5\", \"2 - 2.5\", \"3 - 3.5\", \"4 - 4.5\", \"5\")\n\n\nFor our last data wrangling step, we can assign the rating bins to their respective ratings. I’ll create a new column for this and call it score. After that, we can group by type of university and score, and count the number of occurrences of each group. Then, we’ll be ready to plot.\n\n\nClick here for code\nuni_types &lt;- uni_types |&gt;\n  \n  # assign bins to the score variable\n  mutate(\n    score = case_when(\n      rating &lt; 2 ~ rating_levs[1],\n      rating &lt; 3 ~ rating_levs[2],\n      rating &lt; 4 ~ rating_levs[3],\n      rating &lt; 5 ~ rating_levs[4],\n      TRUE ~ rating_levs[5] \n    ),\n    \n    # order the score bins using the rating_levs we made earlier\n    score = factor(score, levels = rating_levs)\n  ) |&gt;\n  \n  # count the number of each type/score combination\n  group_by(type, score) |&gt;\n  summarise(count = n(), .groups = \"drop\") |&gt;\n  \n  # reorder the universities by descending order of number\n  mutate(type = factor(type, levels = rev(uni_levs)))\n\n\n\n\nThe Plot\nNow that we have our data, let’s get the skeleton of the plot going. I’m going to use a favorite “cheat code” of mine for making aesthetically pleasing bar graphs in R: the ggchicklet package. It lets us round the corners of each bar, which gives a much more aesthetic appearance (in my opinion). So, instead of using the geom_col function that is standard, we will use geom_chicklet instead.\nOne small note for geom_chicklet: it prefers to have bar graphs be vertical. But because my category names are long (and you never want to rotate text), I would like the plot to be horizontal. So I’ll map the type to the x axis and the counts to the y axis like geom_chicklet prefers, but I’ll use coord_flip afterwards to make it horizontal. This is the same technique that the author of the ggchicklet package uses in his demo on the ggchicklet GitHub repository.\n\n\nClick here for code\nuni_types |&gt;\n  ggplot(aes(x = type, y = count, fill = score)) +\n  geom_chicklet(position = position_stack(reverse = TRUE), width = 0.6) +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nThis is already a great start! We have some aesthetic changes to make, but our bins and bars are in the order that we were hoping. Let’s change some colors.\nFirst, I’ll use my favorite theme function, which comes from the ggthemes package. That theme is theme_fivethirtyeight, which takes its name from the legendary data visualizations of the FiveThirtyEight website.\nI also think it would be appropriate for us to use Pride colors, don’t you? Of course, there is an R package for that: the gglgbtq package, which I imported earlier. We will use the “rainbow” color palette provided by gglgbtq to color our bars.\n\n\nClick here for code\nuni_types |&gt;\n  ggplot(aes(x = type, y = count, fill = score)) +\n  geom_chicklet(position = position_stack(reverse = TRUE), width = 0.6) +\n  coord_flip() +\n  \n  # change theme and base font size\n  theme_fivethirtyeight() +\n  \n  # change bar colors and put the legend in the right order\n  scale_fill_manual(values = palette_lgbtq(\"rainbow\"))\n\n\n\n\n\n\n\n\n\nNow we’re cooking! I think we are safe to add the title and subtitle, and then we can make a few more aesthetic changes before wrapping up. I want the background to be black (personal preference), which means the text needs to be white. I also don’t think that horizontal grid lines are necessary when the y axis is discrete, so we will remove those. I love the legend, but I would like it to be stacked and placed vertically in the plot, rather than horizontal and below the plot.\n\n\nClick here for code\nuni_types |&gt;\n  ggplot(aes(x = type, y = count, fill = score)) +\n  geom_chicklet(position = position_stack(reverse = TRUE), width = 0.6) +\n  coord_flip() +\n  theme_fivethirtyeight() +\n  scale_fill_manual(values = palette_lgbtq(\"rainbow\")) +\n  \n  # add title and subtitle\n  labs(\n    title = \"Campus Pride Index Scores\",\n    subtitle = \"Higher scores mean increased LGBTQ-inclusive policies/programs\",\n  ) +\n  \n  theme(\n    # make all text white\n    text = element_text(color = \"white\", family = \"Lato\") ,\n    \n    # adjust title font size\n    plot.title = element_text(),\n    \n    # make background black\n    plot.background = element_rect(fill = \"black\"),\n    panel.background = element_rect(fill = \"black\"),\n    legend.background = element_rect(fill = \"black\"),\n    \n    # remove grid lines\n    panel.grid.major.y = element_blank(),\n    \n    # move legend\n    legend.direction = \"vertical\",\n    legend.position = c(0.9, 0.5),\n  )\n\n\n\n\n\n\n\n\n\nMuch better! Only a few small edits left. First, I don’t think the legend needs a title. I also want the higher scores to be higher on the legend, so we can reverse the order of the legend inside of the scale_fill_manual function. The y axis text is a little far from the axis for my liking, so we will shift that in, and we’ll be done, save for one more thing: fonts.\nI’m going to use custom fonts that aren’t shipped with R or ggplot. These fonts come from Google Fonts, and we will need to use two packages to get them to work: sysfonts to load fonts from Google and showtext to get them to work with our plots. Once we import them, we can use them like any other font in our ggplot graphs!\n\n\nClick here for code\n# load fonts from Google Fonts into our project\nsysfonts::font_add_google(name = \"Galada\")\nsysfonts::font_add_google(name = \"Lato\")\nshowtext::showtext_auto()\n\nuni_types |&gt;\n  ggplot(aes(x = type, y = count, fill = score)) +\n  geom_chicklet(position = position_stack(reverse = TRUE), width = 0.6) +\n  coord_flip() +\n  theme_fivethirtyeight() +\n  scale_fill_manual(\n    values = palette_lgbtq(\"rainbow\"),\n    guide = guide_legend(reverse = TRUE) # reverse the legend order\n  ) +\n  labs(\n    title = \"Campus Pride Index Scores\",\n    subtitle = \"Higher scores mean increased LGBTQ-inclusive policies/programs\",\n  ) +\n  theme(\n    # use Lato font from Google for all text\n    text = element_text(color = \"white\", family = \"Lato\") ,\n    \n    # use Galada font from Google just for the title\n    plot.title = element_text(family = \"Galada\"),\n    \n    plot.background = element_rect(fill = \"black\"),\n    panel.background = element_rect(fill = \"black\"),\n    panel.grid.major.y = element_blank(),\n    legend.background = element_rect(fill = \"black\"),\n    legend.title = element_blank(),\n    legend.direction = \"vertical\",\n    legend.position = c(0.9, 0.5),\n    \n    # shift y axis text closer to the margin\n    axis.text.y = element_text(margin = margin(r = -20))\n  )\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nDone! With some data wrangling and some nice themes, we have arrived of a graph that we can be proud of (get it?). I hope this helps you in your own data viz journey, but if you have further questions, feel free to join my Discord server and ask me personally! And if you are feeling grateful for my work (and are financially able to), you can give me a special thanks by buying me a coffee.\nAs always, thanks for reading, and see you next week!"
  }
]