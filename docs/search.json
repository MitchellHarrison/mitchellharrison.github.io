[
  {
    "objectID": "tutorials/math_stat_1.html",
    "href": "tutorials/math_stat_1.html",
    "title": "Welcome to Estimators | Mathematical Statistics 1",
    "section": "",
    "text": "Say we have some data \\(\\mathbf{X}\\). It’s a vector, so just think of it like a list of \\(n\\) numbers. We want to learn something about how these data came to be. First, we will aggregate our data using a statistic.\n\n\n\n\n\n\nDefinition\n\n\n\nLet \\(X_1, \\cdots, X_n\\) be our data. A statistic is a function of that data. We will denote that statistic with \\(\\delta\\). Crucially, this function cannot contain anything that we don’t know. It is purely a function of known quantities.\n\n\n\n\nLet’s say that our data comes from a normal distribution (a “bell curve”). We denote this with \\(X \\sim N(\\mu, \\sigma^2)\\), where \\(\\mu\\) is the mean of the distribution and \\(\\sigma^2\\) is the variance. Also, to make our life easier, say we know the variance \\(\\sigma^2\\). In practice, this will basically never be the case, but it will simplify our math for now.\nWe have infinitely many options for statistics that we can choose. For example, we could use \\(X_1\\) (that is, the first data point in our vector). While we leave some data on the table in that case, it is certainly a statistic since \\(\\delta = X_1\\) is a function of our data, and there are no unknowns.\nAlternatively, we could use the observed mean of our data. We will call it \\(\\overline{X}\\) (pronounced “\\(X\\) bar”), and it is denoted with \\[\n\\delta(\\mathbf{X}) = \\overline{X} = \\frac{1}{n}\\sum_{i=1}^n X_i.\n\\]\nNotice that this is also a statistic! Although it looks much more complicated, we are still using our data and no unknowns. Here, \\(n\\) is the number of data points that we have, which we know. And we know every \\(X_i\\) because each is part of our data vector \\(\\mathbf{X}\\).\n\n\n\n\n\n\nNote\n\n\n\nConstants (e.g., 7) are also statistics, although no data are involved in the calculation. If it feels like you’re just guessing at random if you do this, you’re right.\n\n\nNow let’s look at an example of a function that is not a statistic: \\[\n\\delta(\\mathbf{X}) = T = \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}}.\n\\] This function will come back in future articles, but for now, recall that we said that we already know the variance \\(sigma^2\\). So that means we already know \\(\\sigma\\). We also know \\(n\\), as we mentioned earlier. But \\(\\mu\\) is unknown to us. Because we have an unknown value \\(\\mu\\) in the numerator, \\(T\\) is not a statistic."
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Here is a place to browse my tutorials! Feel free to browse or sort by category on the right side of the page.\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to Estimators | Mathematical Statistics 1\n\n\n\nStatistics\n\n\n\n\n\n\n\nMitch Harrison\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "This is where my projects will be listed… soon!\n\n\n\n\n\n\n\n\n\n\n\n\nDummy project\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs/demo_blog/demo_blog.html",
    "href": "blogs/demo_blog/demo_blog.html",
    "title": "Demo blog post",
    "section": "",
    "text": "This is where I may do some writing."
  },
  {
    "objectID": "blogs/demo_blog/demo_blog.html#demo-blog",
    "href": "blogs/demo_blog/demo_blog.html#demo-blog",
    "title": "Demo blog post",
    "section": "",
    "text": "This is where I may do some writing."
  },
  {
    "objectID": "projects/dummy_project/dummy_project.html",
    "href": "projects/dummy_project/dummy_project.html",
    "title": "Dummy project",
    "section": "",
    "text": "Here is a sample code block!\n\n\nClick here for code\nlibrary(tidyverse)\ndata &lt;- read_csv(paste0(getwd(), \"/data/data.csv\"))\ndata |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_line(color = \"red\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About me",
    "section": "",
    "text": "This is where readers will see information about me, Mitch Harrison."
  },
  {
    "objectID": "index.html#the-about-me-page.",
    "href": "index.html#the-about-me-page.",
    "title": "About me",
    "section": "",
    "text": "This is where readers will see information about me, Mitch Harrison."
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blog",
    "section": "",
    "text": "We can browse blog posts here. For example, will take you do a dummy blog post full of nothing (yet).\n\n\n\n\n\n\n\n\n\n\n\n\nDemo blog post\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/math_stat_1.html#definition",
    "href": "tutorials/math_stat_1.html#definition",
    "title": "Welcome to Estimators | Mathematical Statistics 1",
    "section": "",
    "text": "This is a note block that you can use to highlight important information."
  },
  {
    "objectID": "tutorials/math_stat_1.html#statistics",
    "href": "tutorials/math_stat_1.html#statistics",
    "title": "Welcome to Estimators | Mathematical Statistics 1",
    "section": "",
    "text": "Let’s say that our data comes from a normal distribution (a “bell curve”). We denote this with \\(X \\sim N(\\mu, \\sigma^2)\\), where \\(\\mu\\) is the mean of the distribution and \\(\\sigma^2\\) is the variance. Also, to make our life easier, say we know the variance \\(\\sigma^2\\). In practice, this will basically never be the case, but it will simplify our math for now.\nWe have infinitely many options for statistics that we can choose. For example, we could use \\(X_1\\) (that is, the first data point in our vector). While we leave some data on the table in that case, it is certainly a statistic since \\(\\delta = X_1\\) is a function of our data, and there are no unknowns.\nAlternatively, we could use the observed mean of our data. We will call it \\(\\overline{X}\\) (pronounced “\\(X\\) bar”), and it is denoted with \\[\n\\delta(\\mathbf{X}) = \\overline{X} = \\frac{1}{n}\\sum_{i=1}^n X_i.\n\\]\nNotice that this is also a statistic! Although it looks much more complicated, we are still using our data and no unknowns. Here, \\(n\\) is the number of data points that we have, which we know. And we know every \\(X_i\\) because each is part of our data vector \\(\\mathbf{X}\\).\n\n\n\n\n\n\nNote\n\n\n\nConstants (e.g., 7) are also statistics, although no data are involved in the calculation. If it feels like you’re just guessing at random if you do this, you’re right.\n\n\nNow let’s look at an example of a function that is not a statistic: \\[\n\\delta(\\mathbf{X}) = T = \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}}.\n\\] This function will come back in future articles, but for now, recall that we said that we already know the variance \\(sigma^2\\). So that means we already know \\(\\sigma\\). We also know \\(n\\), as we mentioned earlier. But \\(\\mu\\) is unknown to us. Because we have an unknown value \\(\\mu\\) in the numerator, \\(T\\) is not a statistic."
  },
  {
    "objectID": "tutorials/math_stat_1.html#point-estimator",
    "href": "tutorials/math_stat_1.html#point-estimator",
    "title": "Welcome to Estimators | Mathematical Statistics 1",
    "section": "Point estimator",
    "text": "Point estimator"
  },
  {
    "objectID": "tutorials/math_stat_1.html#point-estimator-example",
    "href": "tutorials/math_stat_1.html#point-estimator-example",
    "title": "Welcome to Estimators | Mathematical Statistics 1",
    "section": "Point estimator example",
    "text": "Point estimator example\nLet’s keep going with our data, which comes from a normal distribution. But, to get used to using \\(\\theta\\), say that \\(X \\sim N(\\theta, \\sigma^2)\\). One possible estimator is the example mean \\(\\overline{X}\\) from earlier (i.e., the mean of the observed data). Alternatively, we can use a constant: say 5. Intuitively, it feels like \\(\\hat{\\theta} = \\overline{X}\\) would be a better guess than a simple \\(\\hat{\\theta} = 5\\), because it is actually informed by the data. But how do we quantify that intuition? We will calculate and compare both bias and precision for each.\n\nBias\nBias tells us how often, on average, we get the correct value of our unknown parameter \\(\\theta\\). Mathematically, we hope that the following quantity is as small as possible: \\[\n\\mathbb{E}[\\delta(\\mathbf{X}) | \\theta] - \\theta.\n\\]\nThe confusing-looking term \\(\\mathbb{E}[\\cdot]\\) is the expected value of our estimator, given the value of the unknown parameter \\(\\theta\\). Basically, this is the expected value of \\(\\hat{\\theta}\\). If our estimator \\(\\hat{\\theta}\\) is expected to be exactly correct on average, then this whole term will be 0, which is the smallest possible bias.\n\n\nVariance\nVariance describes the variability of our estimator. Ideally, variance is also small. Intuitively we are less “sure” about our estimate if we have a wider variance. We denote variance with \\(Var(\\delta(\\mathbf{X})|\\theta)\\).\nHowever, notice that both bias and variance are conditional on the true value of our unknown parameter \\(\\theta\\). Thus, we cannot calculate these quantities directly. To deal with this, we will introduce the concept of loss in the next article!"
  }
]