[
  {
    "objectID": "tutorials/math_stat_0.html",
    "href": "tutorials/math_stat_0.html",
    "title": "Hello, statistics. | Mathematical Statistics 0",
    "section": "",
    "text": "Background\nI recently finished STA 432 at Duke. For my fellow Duke students looking to take it: yes, it was that hard. But you’re smarter than me, so you will be fine! For the mathematically curious, the course is (somewhat longwindedly) called “Theory and Methods of Statistical Learning and Inference.” If, like me, you prefer helpful course names, call it “Mathematical Statistics.”\nWhile in STA 432, I kept finding myself in mathematics far beyond the “Google the topic and find it on 100 websites” stage. In fact, the topics covered in the course were typically covered by sources that assumed a level of mathematical maturity that I did not possess. I kept thinking that I needed to pre-understand the material to understand the explanation of that same material. I longed for a plain-language introduction to mathematically rigorous subjects.\nThis series is my humble attempt to communicate the beauty of mathematical statistics in comprehensible English to my fellow students (at Duke or at home). To facilitate this, we will take a “trust my word for it” approach, with proofs being in separate articles. However, I recommend at least reading through any proofs that I post. Struggling through those should prepare you for problems involving those same concepts. Even in the proofs articles, I’ll go step-by-step (very granularly) for easy comprehension. That is not a luxury that professors get in a short lecture period, but I’ll take my time to make sure every step is annotated, which I wish I had when I was in 432.\n\n\nCourse Outline\nThis course is broadly broken into three sections. First, we will study point estimators. If you’ve never heard of estimators, don’t worry. They’re just educated guesses at unknown distribution parameters (like the mean of a normal distribution). We will calculate estimators, study them, and choose between them.\nIn the middle portion of the course, we will look at confidence intervals. Again, no background knowledge is necessary here. In effect, confidence intervals tell us how confident we are that our chosen estimator falls into a particular range. Any statisticians reading just cringed at my description because it isn’t technically accurate, but we will discuss why that’s the case when we get to confidence intervals. Don’t worry; it’ll be pretty intuitive once we get there.\nLast but certainly not least, we will conduct hypothesis tests. There, we will choose two possible options for our estimator. For example, maybe we want to choose between X &gt; 5 or X &lt;= 5. We will use our data to decide which option is more likely to be correct. Of course, we will never be 100% of our guess, but we will get close.\nAnd that’s the course! It sounds nice and clean, and in many ways it is. However, like with much of statistics, the findings are the easy part. Finding the findings is the challenge. But I’ll try my best to get us through it so your GPA can be better than mine.\n\n\nCourse Materials\nMy course, which was run by Professor Alexander Volfovsky, had very few requirements. There was a recommended textbook (found here), which I found helpful to get a second perspective on topics. But if the price is off-putting, don’t worry. We only loosely followed it; apart from some homework questions, it was never explicitly required. So, for our purposes, I won’t reference the book directly. If you want a second perspective and don’t mind spending some coin, you have the Amazon link and can search the ISBN elsewhere.\nBesides that, you’ll only need a willingness to learn and a statistical curiosity!\n\n\nConclusion\nTo get started with the course, click here to go to the first article in the series, where we begin our discussion of estimators! Of course, if you want to ask me anything about this course, my time at Duke, or anything else, the best place to reach me is on Discord! And if you really want to support this project, you can buy me a coffee. Thank you for reading, and I can’t wait to get started!"
  },
  {
    "objectID": "tutorials/math_stat_3.html",
    "href": "tutorials/math_stat_3.html",
    "title": "Jensen’s Inequality | Mathematical Statistics 3",
    "section": "",
    "text": "Hello again! Last time, we found the bias and variance of two different estimators and discussed the bias-variance tradeoff. Now that we have some necessary background knowledge, let’s discuss balancing bias and variance to find a better estimator. Recall that we want our estimator to be accurate (have a small bias) and precise (have a small variance).\nWe mentioned the mean squared error (MSE) as a common way of analyzing an estimator. In the below equality, we only discussed the left-hand side. But now that we know what bias and variance are, we can observe a new (but equivalent) definition of the MSE:\n\\[\n\\mathbb{E}[(\\delta(\\mathbf{X}) - \\theta)^2] =\nVar[\\delta(\\mathbf{X}) | \\theta] + bias^2[\\delta(\\mathbf{X}) | \\theta].\n\\]\nThere, \\(\\mathbf{X}\\) is our data, \\(\\theta\\) is our unknown parameter, \\(\\delta(\\mathbf{X})\\) is our estimator, and \\(bias^2[\\cdot]\\) denotes our estimator’s bias squared."
  },
  {
    "objectID": "tutorials/math_stat_3.html#bias-example",
    "href": "tutorials/math_stat_3.html#bias-example",
    "title": "Jensen’s Inequality | Mathematical Statistics 3",
    "section": "Bias example",
    "text": "Bias example\nWe are scientists. We are confident that the machine in our lab is working, and it is spitting out data that comes from the following distribution:\n\\[\nX_1, \\cdots, X_n \\overset{\\mathrm{iid}}{\\sim} Exp(\\theta).\n\\]\nThe expected value (mean) of an exponential distribution is \\(\\mathbb{E}[X] = 1/ \\theta\\). We also know (or find on Wikipedia) that the probability density function (PDF) of an exponential is given by:\n\\[\nf_ \\theta(x) = \\theta e^{- \\theta x}.\n\\]\nOur goal is to study \\(\\theta\\). Let’s re-arrange the expected value of the exponential distribution to isolate \\(\\theta\\):\n\\[\\begin{align*}\n\\mathbb{E}[X] &= \\frac{1}{ \\theta} \\\\\n\\theta\\mathbb{E}[X] &= 1 \\\\\n\\theta &= \\frac{1}{\\mathbb{E}[X]}.\n\\end{align*}\\]\nNow that we have isolated \\(\\theta\\), we can start guessing at some estimators. Let’s replace \\(\\mathbb{E}[X]\\) with our sample mean (the average of whatever data we end up with). Like we did before, let’s call it \\(\\overline{X}\\). Then our first estimator will be \\[\n\\hat{\\theta}_1 = \\frac{1}{\\overline{X}}\n\\]\nLet’s see if our new estimator is unbiased. That is, if it has zero bias.\n\n\n\n\n\n\nDefinition\n\n\n\nJensen’s inequality: Let \\(g(\\cdot)\\) be a convex function (i.e., it has a positive second derivative). In that case, \\[\n\\mathbb{E}[g(X)] \\ge g(\\mathbb{E}[X])\n\\] with equality if \\(g\\) is linear.\n\n\nIn our case, \\(g(X) = \\hat{\\theta}_1\\). Therefore, Jensen’s inequality tells us that\n\\[\n\\mathbb{E}\\left[\\frac{1}{\\overline{X}}\\right] &gt; \\frac{1}{\\mathbb{E}[X]}.\n\\]\nWe noted earlier that the expected value of the exponential distribution that we are working with is \\(1/\\theta\\). Then,\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\frac{1}{\\overline{X}}\\right]\n&&gt; \\frac{1}{\\mathbb{E}[X]} \\\\\n&&gt; \\frac{1}{\\frac{1}{\\theta}} \\\\\n&&gt; \\theta \\\\\n\\mathbb{E}[\\hat{\\theta}_1] &&gt; \\theta.\n\\end{align*}\\]\nSince the expected value of our estimator is not \\(\\theta\\), we have some non-zero bias. So, we know that our estimator is biased! Thanks for help Jensen.\nInstead of finding whether or not there is bias, we may want to calculate the exact bias of our estimator. For this example, doing so becomes quite the exercise in calculus and involves factoring constants out of integrals until those integrals evaluate to 1 (because we turn them into PDFs). Then we can safely get rid of them. But the math is such that it may be worth doing in a separate, smaller article. So for now, Jensen has at least showed us that our estimator is biased, and we will call that a win!"
  },
  {
    "objectID": "tutorials/tidytuesday_06112024.html",
    "href": "tutorials/tidytuesday_06112024.html",
    "title": "Campus Pride Index | TidyTutorial",
    "section": "",
    "text": "Introduction\nIn celebration of Pride Month, this week’s TidyTuesday provides data from the Campus Pride Index, which measures the safety and inclusivity of LGBTQ+ programs across universities in the United States.\nEach university is binned into one or more categories (e.g., military colleges, private/public, and others). What feels natural to me is to see how the Campus Pride Index compares across some of these categories. A proportionate stacked bar chart (where each bar has height equal to 1) is one option, but I would also like to see which types of universities are most common. If there are some categories with worse scores but with much smaller sample sizes, that would be helpful to know. So we’ll use a stacked bar, but not normalize the bar so we can also see how common each type is. Also bear in mind that a single university can (and often does) fall into multiple categories.\nLet’s set some global settings so I don’t have to worry about aspect ratio or other trivialities while we work.\n\n\nClick here for code\nknitr::opts_chunk$set(\n  fig.width = 10,        \n  fig.asp = 0.618,      # the golden ratio\n  fig.align = \"center\"  # center align figures\n)\n\n\n\n\nData Wrangling\nTime to load the data.\n\n\nClick here for code\nlibrary(tidyverse)\nlibrary(gglgbtq)\nlibrary(ggchicklet)\nlibrary(ggthemes)\nlibrary(DT)\n\n# load the data ----------------------------------------------------------------\n\npride_schools &lt;- read_csv(paste0(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/\",\n  \"2024/2024-06-11/pride_index.csv\"\n))\n\npride_tags &lt;- read_csv(paste0(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/\",\n  \"2024/2024-06-11/pride_index_tags.csv\"\n))\n\ndatatable(left_join(pride_schools, pride_tags))\n\n\n\n\n\n\nFirst, let’s format the data for ease of plotting. Right now, each category has its own column, with TRUE or NA values, where NA means “false” for our purposes. But we want the type of school to be represented in a single column so we can map that column to the color of the bars. To move multiple columns into a single one, we will pivot the data. Since we want to consolidate columns, we will need to make our data longer (i.e., add more rows), where each university now has multiple rows corresponding to TRUE or FALSE. Intuitively, to pivot the data longer, we use the pivot_longer function. Notice that once the pivot is complete, we only want to keep the rows where the value is TRUE, since the FALSE rows are just saying that “this university doesn’t fall into this type,” which is useless noise in our dataset.\n\n\nClick here for code\n# format data for plotting -----------------------------------------------------\n\nuni_types &lt;- pride_schools |&gt;\n  \n  # join both datasets into one\n  left_join(pride_tags) |&gt;\n  \n  # select which columns we want to analyze along with their ratings\n  select(rating, public, private, community, liberal_arts, technical,\n         religious, military, hbcu, hispanic_serving, aapi_serving,\n         other_minority_serving) |&gt; \n  \n  # replace NA with FALSE\n  mutate(across(everything(), ~ replace_na(., FALSE))) |&gt;\n  \n  # do the pivot\n  pivot_longer(cols = !rating, names_to = \"type\") |&gt;\n  \n  # drop the rows that don't apply\n  filter(value == TRUE) |&gt;\n  \n  # clean up some strings for prettier plotting\n  mutate(\n    type = str_replace_all(type, \"_\", \" \"),\n    type = str_to_title(type),\n    type = str_replace(type, \"Aapi\", \"AAPI\")\n  )\n\ndatatable(uni_types)\n\n\n\n\n\n\nThat looks just like we wanted it to. Now that our data is formatting, we can work on the plot. Per the data dictionary on the TidyVerse GitHub repository, we know that fractional scores are possible. A quick call to the unique function told me that the “fractional” scores are only half-stars, not any decimal in between two scores. So 1 and 1.5 are possible scores, but 1.7 is not. We should bin these scores by their leading digit so we have five possible fill values instead of ten. We’ll call these bins rating_levs, or “rating levels.”\nI would also like to order the bars in descending order of the total number of universities of that type. To do that, we’ll count how many of each category there are and save the order as a vector uni_levs, or “university levels.”\n\n\nClick here for code\nuni_levs &lt;- uni_types |&gt;\n  group_by(type) |&gt;\n  summarise(count = n()) |&gt;\n  arrange(desc(count)) |&gt;\n  pull(type)\n\nrating_levs &lt;- c(\"1 - 1.5\", \"2 - 2.5\", \"3 - 3.5\", \"4 - 4.5\", \"5\")\n\n\nFor our last data wrangling step, we can assign the rating bins to their respective ratings. I’ll create a new column for this and call it score. After that, we can group by type of university and score, and count the number of occurrences of each group. Then, we’ll be ready to plot.\n\n\nClick here for code\nuni_types &lt;- uni_types |&gt;\n  \n  # assign bins to the score variable\n  mutate(\n    score = case_when(\n      rating &lt; 2 ~ rating_levs[1],\n      rating &lt; 3 ~ rating_levs[2],\n      rating &lt; 4 ~ rating_levs[3],\n      rating &lt; 5 ~ rating_levs[4],\n      TRUE ~ rating_levs[5] \n    ),\n    \n    # order the score bins using the rating_levs we made earlier\n    score = factor(score, levels = rating_levs)\n  ) |&gt;\n  \n  # count the number of each type/score combination\n  group_by(type, score) |&gt;\n  summarise(count = n(), .groups = \"drop\") |&gt;\n  \n  # reorder the universities by descending order of number\n  mutate(type = factor(type, levels = rev(uni_levs)))\n\n\n\n\nThe Plot\nNow that we have our data, let’s get the skeleton of the plot going. I’m going to use a favorite “cheat code” of mine for making aesthetically pleasing bar graphs in R: the ggchicklet package. It lets us round the corners of each bar, which gives a much more aesthetic appearance (in my opinion). So, instead of using the geom_col function that is standard, we will use geom_chicklet instead.\nOne small note for geom_chicklet: it prefers to have bar graphs be vertical. But because my category names are long (and you never want to rotate text), I would like the plot to be horizontal. So I’ll map the type to the x axis and the counts to the y axis like geom_chicklet prefers, but I’ll use coord_flip afterwards to make it horizontal. This is the same technique that the author of the ggchicklet package uses in his demo on the ggchicklet GitHub repository.\n\n\nClick here for code\nuni_types |&gt;\n  ggplot(aes(x = type, y = count, fill = score)) +\n  geom_chicklet(position = position_stack(reverse = TRUE), width = 0.6) +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nThis is already a great start! We have some aesthetic changes to make, but our bins and bars are in the order that we were hoping. Let’s change some colors.\nFirst, I’ll use my favorite theme function, which comes from the ggthemes package. That theme is theme_fivethirtyeight, which takes its name from the legendary data visualizations of the FiveThirtyEight website.\nI also think it would be appropriate for us to use Pride colors, don’t you? Of course, there is an R package for that: the gglgbtq package, which I imported earlier. We will use the “rainbow” color palette provided by gglgbtq to color our bars.\n\n\nClick here for code\nuni_types |&gt;\n  ggplot(aes(x = type, y = count, fill = score)) +\n  geom_chicklet(position = position_stack(reverse = TRUE), width = 0.6) +\n  coord_flip() +\n  \n  # change theme and base font size\n  theme_fivethirtyeight() +\n  \n  # change bar colors and put the legend in the right order\n  scale_fill_manual(values = palette_lgbtq(\"rainbow\"))\n\n\n\n\n\n\n\n\n\nNow we’re cooking! I think we are safe to add the title and subtitle, and then we can make a few more aesthetic changes before wrapping up. I want the background to be black (personal preference), which means the text needs to be white. I also don’t think that horizontal grid lines are necessary when the y axis is discrete, so we will remove those. I love the legend, but I would like it to be stacked and placed vertically in the plot, rather than horizontal and below the plot.\n\n\nClick here for code\nuni_types |&gt;\n  ggplot(aes(x = type, y = count, fill = score)) +\n  geom_chicklet(position = position_stack(reverse = TRUE), width = 0.6) +\n  coord_flip() +\n  theme_fivethirtyeight() +\n  scale_fill_manual(values = palette_lgbtq(\"rainbow\")) +\n  \n  # add title and subtitle\n  labs(\n    title = \"Campus Pride Index Scores\",\n    subtitle = \"Higher scores mean increased LGBTQ-inclusive policies/programs\",\n  ) +\n  \n  theme(\n    # make all text white\n    text = element_text(color = \"white\", family = \"Lato\") ,\n    \n    # adjust title font size\n    plot.title = element_text(),\n    \n    # make background black\n    plot.background = element_rect(fill = \"black\"),\n    panel.background = element_rect(fill = \"black\"),\n    legend.background = element_rect(fill = \"black\"),\n    \n    # remove grid lines\n    panel.grid.major.y = element_blank(),\n    \n    # move legend\n    legend.direction = \"vertical\",\n    legend.position = c(0.9, 0.5),\n  )\n\n\n\n\n\n\n\n\n\nMuch better! Only a few small edits left. First, I don’t think the legend needs a title. I also want the higher scores to be higher on the legend, so we can reverse the order of the legend inside of the scale_fill_manual function. The y axis text is a little far from the axis for my liking, so we will shift that in, and we’ll be done, save for one more thing: fonts.\nI’m going to use custom fonts that aren’t shipped with R or ggplot. These fonts come from Google Fonts, and we will need to use two packages to get them to work: sysfonts to load fonts from Google and showtext to get them to work with our plots. Once we import them, we can use them like any other font in our ggplot graphs!\n\n\nClick here for code\n# load fonts from Google Fonts into our project\nsysfonts::font_add_google(name = \"Galada\")\nsysfonts::font_add_google(name = \"Lato\")\nshowtext::showtext_auto()\n\nuni_types |&gt;\n  ggplot(aes(x = type, y = count, fill = score)) +\n  geom_chicklet(position = position_stack(reverse = TRUE), width = 0.6) +\n  coord_flip() +\n  theme_fivethirtyeight() +\n  scale_fill_manual(\n    values = palette_lgbtq(\"rainbow\"),\n    guide = guide_legend(reverse = TRUE) # reverse the legend order\n  ) +\n  labs(\n    title = \"Campus Pride Index Scores\",\n    subtitle = \"Higher scores mean increased LGBTQ-inclusive policies/programs\",\n  ) +\n  theme(\n    # use Lato font from Google for all text\n    text = element_text(color = \"white\", family = \"Lato\") ,\n    \n    # use Galada font from Google just for the title\n    plot.title = element_text(family = \"Galada\"),\n    \n    plot.background = element_rect(fill = \"black\"),\n    panel.background = element_rect(fill = \"black\"),\n    panel.grid.major.y = element_blank(),\n    legend.background = element_rect(fill = \"black\"),\n    legend.title = element_blank(),\n    legend.direction = \"vertical\",\n    legend.position = c(0.9, 0.5),\n    \n    # shift y axis text closer to the margin\n    axis.text.y = element_text(margin = margin(r = -20))\n  )\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nDone! With some data wrangling and some nice themes, we have arrived of a graph that we can be proud of (get it?). I hope this helps you in your own data viz journey, but if you have further questions, feel free to join my Discord server and ask me personally! And if you are feeling grateful for my work (and are financially able to), you can give me a special thanks by buying me a coffee.\nAs always, thanks for reading, and see you next week!"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Here is a place to browse my tutorials! Feel free to browse or sort by category on the right side of the page. If you have any questions or ideas for new topics, let me know on Discord!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur World in Emissions | TidyTutorial\n\n\n\nData Viz\n\n\n\n\n\n\n\nMitch Harrison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCampus Pride Index | TidyTutorial\n\n\n\nData Viz\n\n\n\n\n\n\n\nMitch Harrison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Bias-Variance Tradeoff | Mathematical Statistics 2\n\n\n\nStatistics\n\n\nMathematical Statistics\n\n\n\n\n\n\n\nMitch Harrison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJensen’s Inequality | Mathematical Statistics 3\n\n\n\nStatistics\n\n\nMathematical Statistics\n\n\n\n\n\n\n\nMitch Harrison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to Estimators! | Mathematical Statistics 1\n\n\n\nStatistics\n\n\nMathematical Statistics\n\n\n\n\n\n\n\nMitch Harrison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHello, statistics. | Mathematical Statistics 0\n\n\n\nStatistics\n\n\nMathematical Statistics\n\n\n\n\n\n\n\nMitch Harrison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSufficiency | Mathematical Statistics 4\n\n\n\nStatistics\n\n\nMathematical Statistics\n\n\n\n\n\n\n\nMitch Harrison\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "These are my independent projects! They can be quick visualizations, full analyses with write-ups, and even interactive data exploration dashboards. Feel free to use the tags on the right side of the page to navigate, and reach out to me on Discord with any questions. Thanks!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Cheese Explorer | TidyTuesday\n\n\n\nData Viz\n\n\nTidyTuesday\n\n\nDashboard\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCampus Pride Index | TidyTuesday\n\n\n\nData Viz\n\n\nTidyTuesday\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur World in Emissions | TidyTuesday\n\n\n\nData Viz\n\n\nTidyTuesday\n\n\n\n\n\n\n\nMitch Harrison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCricket Crackers: A Bayesian Approach\n\n\n\nBayesian Statistics\n\n\nData Viz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/tidytuesday_06112024/pride.html",
    "href": "projects/tidytuesday_06112024/pride.html",
    "title": "Campus Pride Index | TidyTuesday",
    "section": "",
    "text": "Welcome!\nHappy pride month! On this fine TidyTuesday afternoon, we will see how different types of colleges and universities handle LGBTQ+ inclusion! The Campus Pride Index tracks safety, inclusivity, and LGBTQ+ policies/programs at universities across the United States. Results are on a 1-5 scale (with higher numbers being most inclusive), and colleges are grouped by various discrete categories. Today, we’ll build a stacked horizontal bar chart to see the distribution of scores for some of those categories. I’ll use the ggchicklet package and some custom fonts for easy aesthetic changes, and we’ll be done! If you want to see a step-by-step tutorial explaining the code, click here.\n\n\nClick here for code\nlibrary(tidyverse)\nlibrary(gglgbtq)\nlibrary(ggchicklet)\nlibrary(ggthemes)\n\n# load custom fonts for the plot -----------------------------------------------\n\nsysfonts::font_add_google(name = \"Galada\")\nsysfonts::font_add_google(name = \"Lato\")\nshowtext::showtext_auto()\n\n# load the data ----------------------------------------------------------------\n\npride_schools &lt;- read_csv(paste0(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/\",\n  \"2024/2024-06-11/pride_index.csv\"\n))\n\npride_tags &lt;- read_csv(paste0(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/\",\n  \"2024/2024-06-11/pride_index_tags.csv\"\n))\n\n# format data for plotting -----------------------------------------------------\n\nuni_types &lt;- pride_schools |&gt;\n  left_join(pride_tags) |&gt;\n  select(rating, public, private, community, liberal_arts, technical,\n         religious, military, hbcu, hispanic_serving, aapi_serving,\n         other_minority_serving) |&gt; \n  mutate(across(everything(), ~ replace_na(., FALSE))) |&gt;\n  pivot_longer(cols = !rating, names_to = \"type\") |&gt;\n  filter(value == TRUE) |&gt;\n  mutate(\n    type = str_replace_all(type, \"_\", \" \"),\n    type = str_to_title(type),\n    type = str_replace(type, \"Aapi\", \"AAPI\")\n  )\n\n# factor levels for ordering bars/fills ----------------------------------------\n\nuni_levs &lt;- uni_types |&gt;\n  group_by(type) |&gt;\n  summarise(count = n()) |&gt;\n  arrange(desc(count)) |&gt;\n  pull(type)\n\nrating_levs &lt;- c(\"1 - 1.5\", \"2 - 2.5\", \"3 - 3.5\", \"4 - 4.5\", \"5\")\n\n# build the plot! --------------------------------------------------------------\n\nuni_types |&gt;\n  mutate(\n    score = case_when(\n      rating &lt; 2 ~ rating_levs[1],\n      rating &lt; 3 ~ rating_levs[2],\n      rating &lt; 4 ~ rating_levs[3],\n      rating &lt; 5 ~ rating_levs[4],\n      TRUE ~ rating_levs[5] \n    ),\n    score = factor(score, levels = rating_levs)\n  ) |&gt;\n  group_by(type, score) |&gt;\n  summarise(count = n(), .groups = \"drop\") |&gt;\n  mutate(type = factor(type, levels = rev(uni_levs))) |&gt;\n  ggplot(aes(x = type, y = count, fill = score)) +\n  geom_chicklet(position = position_stack(reverse = TRUE), width = 0.6) +\n  coord_flip() +\n  theme_fivethirtyeight() +\n  scale_fill_manual(\n    values = palette_lgbtq(\"rainbow\"),\n    guide = guide_legend(reverse = TRUE)\n  ) +\n  labs(\n    title = \"Campus Pride Index Scores\",\n    subtitle = \"Higher scores mean increased LGBTQ-inclusive policies/programs\",\n  ) +\n  theme(\n    text = element_text(color = \"white\", family = \"Lato\") ,\n    plot.title = element_text(family = \"Galada\"),\n    plot.background = element_rect(fill = \"black\"),\n    panel.background = element_rect(fill = \"black\"),\n    panel.grid.major.y = element_blank(),\n    legend.background = element_rect(fill = \"black\"),\n    legend.title = element_blank(),\n    legend.direction = \"vertical\",\n    legend.position = c(0.9, 0.5),\n    axis.text.y = element_text(margin = margin(r = -20))\n  )\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nThere we have it! Seeing very few 1s is a good sign, but there is always room for progress. The top three types of colleges by number (public, private, and liberal arts) are highly inclusive, while community colleges (where I started my educational journey) have a long way to go.\nIf you have any questions or suggestions for improvements, the best way to reach me is on Discord! And, of course, if you want to support this work financially, you can buy me a coffee!\nThanks for reading, and I’ll see you next week!"
  },
  {
    "objectID": "projects/tidytuesday_05212024/emissions.html",
    "href": "projects/tidytuesday_05212024/emissions.html",
    "title": "Our World in Emissions | TidyTuesday",
    "section": "",
    "text": "Hello, all! Welcome to TidyTuesday. This week, as climate analysts often do, we are going to get mildly depressing in pursuit of a pretty graph. This time, we will look at emissions from various actors’ coal, natural gas, and cement production. Spoiler: it’s not good.\nThe data for this week are brought to us by Carbon Majors, who have compiled a database going all the way back to the 1850’s! The dataset contains emission data for 75 state and non-state actors, but we will aggregate into total emissions by type for the plot. If you want to get more granular in your own plot, check out the data on the TidyTuesday GitHub repository here!\n\n\nClick here for code\nlibrary(tidyverse)\n\n# read data and rename an ugly column ------------------------------------------\nemis&lt;- read_csv(paste0(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/\",\n  \"data/2024/2024-05-21/emissions.csv\"\n  )\n)\n\nemis &lt;- emis |&gt;\n  rename(emissions = \"total_emissions_MtCO2e\")\n\n# constants for ease of code legibility ----------------------------------------\nLEVS &lt;- c(\"Coal\", \"Oil & NGL\", \"Natural Gas\", \"Cement\")\nBG_COLOR &lt;- \"#F0F0F0\"\nGRAY &lt;- \"gray35\"\nUN_TEXT &lt;- paste(\n  \"In 1995, the United Nations\\nConference of the Parties met for\\nthe first\", \n  \"time to discuss the looming\\nthreat of climate change. The COP\\nhas\",\n  \"met twenty-eight times since.\"\n)\n\n# data cleanup -----------------------------------------------------------------\nemis |&gt;\n  filter(year &gt;= 1900) |&gt; # lots of near-zero space without this filter\n  mutate(\n    commodity = if_else(str_detect(commodity, \"Coal\"), \"Coal\", commodity),\n    commodity = factor(commodity, levels = LEVS) # re-order areas\n  ) |&gt;\n  group_by(year, commodity) |&gt;\n  summarise(emissions = sum(emissions), .groups = \"drop\") |&gt;\n  \n  # start of plot --------------------------------------------------------------\n  ggplot(aes(x = year, y = emissions, fill = commodity)) +\n  geom_area(alpha = 0.9) +\n  \n  # UN COP annotation text box -------------------------------------------------\n  annotate(\n    geom = \"segment\",\n    x = 1995,\n    xend = 1995,\n    y = 35500,\n    yend = 20500,\n    linetype = \"solid\",\n    linejoin = \"round\",\n    linewidth = 1,\n    color = \"grey35\",\n    arrow = arrow(type = \"closed\", length = unit(0.2, \"cm\"))\n  ) +\n  annotate(\n    geom = \"rect\",\n    xmin = 1950.5,\n    xmax = 1993.5,\n    ymin = 23500,\n    ymax = 35800,\n    fill = BG_COLOR\n  ) +\n  annotate(\n    geom = \"text\",\n    x = 1992,\n    y = 30000,\n    label = UN_TEXT,\n    color = GRAY,\n    fontface = \"italic\",\n    hjust = \"right\"\n  ) +\n  \n  # replace legend with annotation text ----------------------------------------\n  annotate(\n    geom = \"text\",\n    color = \"white\",\n    x = 2020,\n    y = c(1000, 4700, 13000, 26000),\n    label = c(\"Cement\", \"Natural Gas\", \"Oil & NGL\", \"Coal\"),\n    hjust = \"right\",\n    fontface = \"bold\"\n  ) +\n  \n  # visual style elements (love you, ggthemes) ---------------------------------\n  ggthemes::scale_fill_colorblind() +\n  ggthemes::theme_fivethirtyeight() +\n  \n  # customize axis breaks and labels -------------------------------------------\n  scale_x_continuous(breaks = seq(1900, 2020, 20)) +\n  scale_y_continuous(\n    breaks = seq(0, 40000, 5000), \n    label = scales::label_number(scale = 1e-3, suffix = \"k\")\n  ) +\n  labs(\n    x = element_blank(),\n    y = latex2exp::TeX(\"Emissions ($MtCO_2e$)\"),\n    title = \"Our World in Emissions\",\n    subtitle = latex2exp::TeX(\n      paste(\n        \"Emissions are measured in Millions of Tons of $CO_2$ equivalent\",\n        \"($MtCO_2e$)\"\n      )\n    ),\n    caption = paste(\n      \"Made with love by Mitch Harrison\",\n      \"                                                                       \",\n      \"Source: Carbon Majors database and TidyTuesday\"\n    )\n  ) + \n  \n  # theme cleanup --------------------------------------------------------------\n  geom_hline(yintercept = 0, linewidth = 0.7, color = GRAY) + # bold axis\n  theme(\n    legend.position = \"none\", # hide legend\n    axis.title.y = element_text(size = 10),\n    plot.background = element_rect(fill = BG_COLOR)\n  ) \n\n\n\n\n\n\n\n\n\nSo there she is! As we can see, the UN COP seems to be fighting an uphill battle. Emissions are rising, but a good analyst must note the limitations of the data. What jumps out to me is that renewables aren’t listed here because it’s only a graph of emissions. For all we know (from this graph), these emissions only produce a small portion of the world’s energy, and we are arguing about a couple of percentage points. Maybe we have defeated climate change after all!\nOf course, that’s not the case, but proving that point will require outside data. So, I welcome everyone reading to write a fuller report using more evidence. If nothing else, it would make for some fun data viz practice!\nIf you want a step-by-step guide to how I made this plot, there is a tutorial page here, or even stop by my Discord server and ask me! And, of course, if you appreciate my work enough to buy me a coffee, you can do so here. Thank you for reading, and see you next week!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I’m Mitch. 👋",
    "section": "",
    "text": "After leaving the Navy in 2022, I moved to North Carolina to study data science and political science at Duke University. Finishing my degree cost the military about $60,000 in aid and depleted my savings. Even my parents, who probably thought they could start spending their “put their kid through college” fund on fun things, had to chip in to help. My education was world-class but utterly unattainable for the majority of us. This website is my attempt to turn my course notes, homework, and projects into comprehensible articles for free. If nothing else, future Duke students struggling through their data science degree can look here for a different perspective.\nIf you want to see some of my analysis work, head to the Projects tab! If you are here to learn, the Tutorials tab is for you. Of course, if you are curious about the website’s structure, it is built with R and Quarto, and the code is available by clicking on the GitHub icon at the bottom-left of every page (or click here).\nAccess to information should always be free, so every article here is, and always will be, at no cost. If you want to show financial support, you can buy me a coffee! But I won’t ever make donor-exclusive educational content, so don’t feel like you’re missing out by not donating. It’s just one way to show thanks.\nI hope you enjoy the site, and feel free to reach out via GitHub issues to make suggestions for articles. Thanks for reading!"
  },
  {
    "objectID": "tutorials/tidytuesday_05212024.html",
    "href": "tutorials/tidytuesday_05212024.html",
    "title": "Our World in Emissions | TidyTutorial",
    "section": "",
    "text": "Welcome! If you saw my post for this week’s TidyTuesday, I’m glad you liked it enough to learn from it! If not, you can either scroll to the bottom to see the final product or click here. For this plot, we will use an area plot to visualize the global emissions by type going back to 1900. To start, we will use a bare-bones ggplot2 area chart with no bells or whistles to see what we are working with.\n\n\nClick here for code\nlibrary(tidyverse)\n\n# read data and rename an ugly column ------------------------------------------\nemis&lt;- read_csv(paste0(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/\",\n  \"data/2024/2024-05-21/emissions.csv\"\n  )\n)\n\nemis &lt;- emis |&gt;\n  rename(emissions = \"total_emissions_MtCO2e\")\n\nemis |&gt;\n  group_by(year, commodity) |&gt;\n  summarise(emissions = sum(emissions), .groups = \"drop\") |&gt;\n  \n  # start of plot --------------------------------------------------------------\n  ggplot(aes(x = year, y = emissions, fill = commodity)) +\n  geom_area(alpha = 0.9)\n\n\n\n\n\n\n\n\n\nOkay, we’ve learned a lot. First, there are a lot of categories. A good rule of thumb is that once you get to about seven colors, even non-colorblind humans struggle to differentiate. But there is hope! Notice that there are several types of coal production. Let’s aggregate them. Second, there is a long tail on the left because of near-zero data. Let’s bring our limit to the right to get a better look.\n\n\nClick here for code\nemis |&gt;\n  filter(year &gt;= 1900) |&gt; # get rid of that tail\n  mutate(\n    # aggregate coal\n    commodity = if_else(str_detect(commodity, \"Coal\"), \"Coal\", commodity),\n  ) |&gt;\n  group_by(year, commodity) |&gt;\n  summarise(emissions = sum(emissions), .groups = \"drop\") |&gt;\n  \n  # start of plot --------------------------------------------------------------\n  ggplot(aes(x = year, y = emissions, fill = commodity)) +\n  geom_area(alpha = 0.9)\n\n\n\n\n\n\n\n\n\nMuch better! But to me, having the smallest category (cement) on top feels awkward. Let’s reorder the categories! I’ll do so in descending order of emissions in the last year.\n\n\nClick here for code\nLEVS &lt;- c(\"Coal\", \"Oil & NGL\", \"Natural Gas\", \"Cement\") # our desired order\n\nemis |&gt;\n  filter(year &gt;= 1900) |&gt;\n  mutate(\n    commodity = if_else(str_detect(commodity, \"Coal\"), \"Coal\", commodity),\n    commodity = factor(commodity, levels = LEVS) # re-order \n  ) |&gt;\n  group_by(year, commodity) |&gt;\n  summarise(emissions = sum(emissions), .groups = \"drop\") |&gt;\n  \n  # start of plot --------------------------------------------------------------\n  ggplot(aes(x = year, y = emissions, fill = commodity)) +\n  geom_area(alpha = 0.9)\n\n\n\n\n\n\n\n\n\nNow we’re cooking! It’s time for some style points. I’ll use my favorite aesthetic cheat code: ggthemes. Let’s add a theme and color scheme. I’m going with the FiveThirtyEight theme and a colorblind-friendly palette. I’ll also take this opportunity to adjust the opacity down just a touch. This is a personal choice, but I find it nice to be able to see the grid behind such ink-heavy plots as area plots.\n\n\n\n\n\n\nImportant\n\n\n\nRemember: unless you are making plots for a very small number of people and you know for certain that none are colorblind, making inaccessible plots is inexcusable. Of course, we all make mistakes, so if you ever notice an accessibility issue on my site, reach out and let me know on Discord or via a GitHub issue so I can improve for next time!\n\n\n\n\nClick here for code\nLEVS &lt;- c(\"Coal\", \"Oil & NGL\", \"Natural Gas\", \"Cement\") # our desired order\n\nemis |&gt;\n  filter(year &gt;= 1900) |&gt;\n  mutate(\n    commodity = if_else(str_detect(commodity, \"Coal\"), \"Coal\", commodity),\n    commodity = factor(commodity, levels = LEVS) # re-order \n  ) |&gt;\n  group_by(year, commodity) |&gt;\n  summarise(emissions = sum(emissions), .groups = \"drop\") |&gt;\n  \n  # start of plot --------------------------------------------------------------\n  ggplot(aes(x = year, y = emissions, fill = commodity)) +\n  geom_area(alpha = 0.9) + # drop the opacity just a touch\n\n  # add theme and colors (love you, ggthemes) \n  ggthemes::scale_fill_colorblind() +\n  ggthemes::theme_fivethirtyeight()\n\n\n\n\n\n\n\n\n\nAnd just like that, it feels like we are almost there! Let’s change a few things at once. We will change the background color, add the title/subtitle/axis labels/caption, and format the \\(y\\)-axis to read 30k instead of 30000. That will give us a feel for the final color scheme and how the fonts feel on the page. Because of the subscript “2” in \\(CO_2\\), I will use the latex2exp package use \\(\\LaTeX\\) typesetting in the plot.\n\n\n\n\n\n\nNote\n\n\n\nOne note that is unique to this plot. When we use theme_fivethirtyeight, it removes the \\(y\\)-axis title. So, although we normally wouldn’t have to explicitly set the axis title to element_text in the theme function, we will here.\n\n\n\n\nClick here for code\nLEVS &lt;- c(\"Coal\", \"Oil & NGL\", \"Natural Gas\", \"Cement\")\nBG_COLOR &lt;- \"#F0F0F0\" # this will be our background color\n\nemis |&gt;\n  filter(year &gt;= 1900) |&gt;\n  mutate(\n    commodity = if_else(str_detect(commodity, \"Coal\"), \"Coal\", commodity),\n    commodity = factor(commodity, levels = LEVS) \n  ) |&gt;\n  group_by(year, commodity) |&gt;\n  summarise(emissions = sum(emissions), .groups = \"drop\") |&gt;\n  \n  # start of plot --------------------------------------------------------------\n  ggplot(aes(x = year, y = emissions, fill = commodity)) +\n  geom_area(alpha = 0.9) +\n\n  ggthemes::scale_fill_colorblind() +\n  ggthemes::theme_fivethirtyeight() +\n\n  # abbreviate the y axis labels using the scales package\n  scale_y_continuous(label = scales::label_number(scale = 1e-3, suffix = \"k\")) +\n\n  # add labels to the plot -----------------------------------------------------\n  labs(\n    x = element_blank(),\n    y = latex2exp::TeX(\"Emissions ($MtCO_2e$)\"), # LaTeX typesetting with TeX()\n    title = \"Our World in Emissions\",\n    subtitle = latex2exp::TeX(\n      paste(\n        \"Emissions are measured in Millions of Tons of $CO_2$ equivalent\",\n        \"($MtCO_2e$)\"\n      )\n    ),\n    caption = paste(\n      \"Made with love by Mitch Harrison\",\n      # long blank line to \"hack\" a an annotation in the bottom-left corner\n      \"                                                                       \",\n      \"Source: Carbon Majors database and TidyTuesday\"\n    )\n  ) +\n  theme(\n    axis.title.y = element_text(size = 10),\n    plot.background = element_rect(fill = BG_COLOR) # change background color\n  ) \n\n\n\n\n\n\n\n\n\nYou could submit this plot for public consumption without shame, but we can do better! For example, I think we could safely remove the legend by annotating the colors directly on the plot. Let’s use a geom_text to do just that. While this entire process has been creative, we are getting into highly subjective territory here. So if you don’t like these changes, do something else! I would love to see your ideas.\nTo make the annotations, I want the text to be right-justified and directly atop one another. To accomplish that, I will give geom_text a single \\(x\\) value but several \\(y\\) values (one for each category).\n\n\nClick here for code\nLEVS &lt;- c(\"Coal\", \"Oil & NGL\", \"Natural Gas\", \"Cement\")\nBG_COLOR &lt;- \"#F0F0F0\"\n\nemis |&gt;\n  filter(year &gt;= 1900) |&gt;\n  mutate(\n    commodity = if_else(str_detect(commodity, \"Coal\"), \"Coal\", commodity),\n    commodity = factor(commodity, levels = LEVS) \n  ) |&gt;\n  group_by(year, commodity) |&gt;\n  summarise(emissions = sum(emissions), .groups = \"drop\") |&gt;\n  \n  # start of plot --------------------------------------------------------------\n  ggplot(aes(x = year, y = emissions, fill = commodity)) +\n  geom_area(alpha = 0.9) +\n\n  ggthemes::scale_fill_colorblind() +\n  ggthemes::theme_fivethirtyeight() +\n  scale_y_continuous(label = scales::label_number(scale = 1e-3, suffix = \"k\")) +\n\n  # add annotation text to replace the legend ----------------------------------\n  annotate(\n    geom = \"text\",\n    color = \"white\",\n    x = 2020,\n    y = c(1000, 4700, 13000, 26000),\n    label = c(\"Cement\", \"Natural Gas\", \"Oil & NGL\", \"Coal\"),\n    hjust = \"right\",\n    fontface = \"bold\"\n  ) +\n\n  labs(\n    x = element_blank(),\n    y = latex2exp::TeX(\"Emissions ($MtCO_2e$)\"),\n    title = \"Our World in Emissions\",\n    subtitle = latex2exp::TeX(\n      paste(\n        \"Emissions are measured in Millions of Tons of $CO_2$ equivalent\",\n        \"($MtCO_2e$)\"\n      )\n    ),\n    caption = paste(\n      \"Made with love by Mitch Harrison\",\n      \"                                                                       \",\n      \"Source: Carbon Majors database and TidyTuesday\"\n    )\n  ) +\n\n  theme(\n    legend.position = \"none\", # hide the legend\n    axis.title.y = element_text(size = 10),\n    plot.background = element_rect(fill = BG_COLOR)\n  ) \n\n\n\n\n\n\n\n\n\nNailed it. Now, I will happily take criticism here. I don’t love that the “Cement” label isn’t entirely encompassed by its data. But I think it’s much cleaner than having a legend drawing our eye away from the plot, so I’ll keep it.\nThe last thing we have to do before we can worry about the big annotation in the middle of the plot is change where the axes break. That is, set the years and emission amount displayed on the x and y axes, respectively. And while I’m at it, I will use a geom_hline to make the \\(x\\)-axis a bit bolder since it melts into the background a little bit too much for my liking.\n\n\nClick here for code\nLEVS &lt;- c(\"Coal\", \"Oil & NGL\", \"Natural Gas\", \"Cement\")\nBG_COLOR &lt;- \"#F0F0F0\"\nGRAY &lt;- \"gray35\"\n\nemis |&gt;\n  filter(year &gt;= 1900) |&gt;\n  mutate(\n    commodity = if_else(str_detect(commodity, \"Coal\"), \"Coal\", commodity),\n    commodity = factor(commodity, levels = LEVS) \n  ) |&gt;\n  group_by(year, commodity) |&gt;\n  summarise(emissions = sum(emissions), .groups = \"drop\") |&gt;\n  \n  # start of plot --------------------------------------------------------------\n  ggplot(aes(x = year, y = emissions, fill = commodity)) +\n  geom_area(alpha = 0.9) +\n\n  ggthemes::scale_fill_colorblind() +\n  ggthemes::theme_fivethirtyeight() +\n\n  # change where the axis breaks occur -----------------------------------------\n  scale_x_continuous(breaks = seq(1900, 2020, 20)) +\n  scale_y_continuous(\n    breaks = seq(0, 40000, 5000), \n    label = scales::label_number(scale = 1e-3, suffix = \"k\")\n  ) +\n\n  annotate(\n    geom = \"text\",\n    color = \"white\",\n    x = 2020,\n    y = c(1000, 4700, 13000, 26000),\n    label = c(\"Cement\", \"Natural Gas\", \"Oil & NGL\", \"Coal\"),\n    hjust = \"right\",\n    fontface = \"bold\"\n  ) +\n\n  labs(\n    x = element_blank(),\n    y = latex2exp::TeX(\"Emissions ($MtCO_2e$)\"),\n    title = \"Our World in Emissions\",\n    subtitle = latex2exp::TeX(\n      paste(\n        \"Emissions are measured in Millions of Tons of $CO_2$ equivalent\",\n        \"($MtCO_2e$)\"\n      )\n    ),\n    caption = paste(\n      \"Made with love by Mitch Harrison\",\n      \"                                                                       \",\n      \"Source: Carbon Majors database and TidyTuesday\"\n    )\n  ) +\n\n  geom_hline(yintercept = 0, linewidth = 0.7, color = GRAY) + # bold axis\n  theme(\n    legend.position = \"none\", # hide the legend\n    axis.title.y = element_text(size = 10),\n    plot.background = element_rect(fill = BG_COLOR)\n  ) \n\n\n\n\n\n\n\n\n\nOnce I write-in the line breaks, I’ll use the annotate function as before. But that’s not all. By default, there is no background with text annotations, so the grid overlaps the text and decreases legibility. To fix this, I’ll use annotate to put a rectangle the same color as the plot background behind the text, which “removes” the grid lines behind the text.\nFinally, to accomplish the arrow, we will use our final annotate to draw a line segment and put an arrowhead at the end.\n\n\n\n\n\n\nNote\n\n\n\nNormally, the order that we put things in a ggplot2 pipeline doesn’t matter. But here, if you put the background rectangle after the text annotation, it will cover the text, rendering it invisible.\n\n\nBecause this is our last edit, I will take this opportunity to make one very oft-forgotten change: write my alt text. Since you’re here, I know you respect the power of data communication. Alt text lets us communicate with those who sometimes miss out on learning from plots online. As our color palette did for colorblind viewers, we owe it to our non-sighted friends to let them participate.\nAnd finally, I’ll change the aspect ratio of the plot. You may have heard of the golden ratio, which is a ratio that many humans find inherently satisfying to look at. That ratio is approximately 1.618:1. The inverse of that number is 0.618, which will be our horizontal aspect ratio (1.618 is vertical). Because the quarto headers won’t render with the document, my final header is below:\n#| label: plt-final\n#| fig-width: 8\n#| fig-align: \"center\"\n#| fig-asp: 0.618\n#| fig-alt: |\n#|   This plot is titled Our World in Emissions. It is an area plot that shows\n#|   global emissions over time by type. The types are coal, natural gas,\n#|   cement, and oil and NGL. The plot notes that in 1995, the UN first met to\n#|   discuss the climate threat. The plot shows near-zero emissions from 1900 to\n#|   1920, when a slow increase begins. From there, emission growth seems to be\n#|   exponentially increasing, with no decline since the UN first met. Coal is\n#|   the largest emitter, then oil and NGL, then natural gas, and finally,\n#|   cement.\nNow, let’s see the plot!\n\n\nClick here for code\n# constants for ease of code legibility ----------------------------------------\nLEVS &lt;- c(\"Coal\", \"Oil & NGL\", \"Natural Gas\", \"Cement\")\nBG_COLOR &lt;- \"#F0F0F0\"\nUN_TEXT &lt;- paste(\n  \"In 1995, the United Nations\\nConference of the Parties met for\\nthe first\", \n  \"time to discuss the looming\\nthreat of climate change. The COP\\nhas\",\n  \"met twenty-eight times since.\"\n)\n\n# data cleanup -----------------------------------------------------------------\nemis |&gt;\n  filter(year &gt;= 1900) |&gt; # lots of near-zero space without this filter\n  mutate(\n    commodity = if_else(str_detect(commodity, \"Coal\"), \"Coal\", commodity),\n    commodity = factor(commodity, levels = LEVS) # re-order areas\n  ) |&gt;\n  group_by(year, commodity) |&gt;\n  summarise(emissions = sum(emissions), .groups = \"drop\") |&gt;\n  \n  # start of plot --------------------------------------------------------------\n  ggplot(aes(x = year, y = emissions, fill = commodity)) +\n  geom_area(alpha = 0.9) +\n  \n  # UN COP annotation text box -------------------------------------------------\n\n  # the arrow\n  annotate(\n    geom = \"segment\",\n    x = 1995,\n    xend = 1995,\n    y = 35500,\n    yend = 20500,\n    linetype = \"solid\",\n    linejoin = \"round\",\n    linewidth = 1,\n    color = \"grey35\",\n    arrow = arrow(type = \"closed\", length = unit(0.2, \"cm\"))\n  ) +\n\n  # the background rectangle (must be before the text)\n  annotate(\n    geom = \"rect\",\n    xmin = 1945.5,\n    xmax = 1993.5,\n    ymin = 23500,\n    ymax = 35800,\n    fill = BG_COLOR\n  ) +\n\n  # annotation text\n  annotate(\n    geom = \"text\",\n    x = 1992,\n    y = 30000,\n    label = UN_TEXT,\n    color = GRAY,\n    fontface = \"italic\",\n    hjust = \"right\"\n  ) +\n  \n  # replace legend with annotation text ----------------------------------------\n  annotate(\n    geom = \"text\",\n    color = \"white\",\n    x = 2020,\n    y = c(1000, 4700, 13000, 26000),\n    label = c(\"Cement\", \"Natural Gas\", \"Oil & NGL\", \"Coal\"),\n    hjust = \"right\",\n    fontface = \"bold\"\n  ) +\n  \n  # visual style elements (love you, ggthemes) ---------------------------------\n  ggthemes::scale_fill_colorblind() +\n  ggthemes::theme_fivethirtyeight() +\n  \n  # customize axis breaks and labels -------------------------------------------\n  scale_x_continuous(breaks = seq(1900, 2020, 20)) +\n  scale_y_continuous(\n    breaks = seq(0, 40000, 5000), \n    label = scales::label_number(scale = 1e-3, suffix = \"k\")\n  ) +\n  labs(\n    x = element_blank(),\n    y = latex2exp::TeX(\"Emissions ($MtCO_2e$)\"),\n    title = \"Our World in Emissions\",\n    subtitle = latex2exp::TeX(\n      paste(\n        \"Emissions are measured in Millions of Tons of $CO_2$ equivalent\",\n        \"($MtCO_2e$)\"\n      )\n    ),\n    caption = paste(\n      \"Made with love by Mitch Harrison\",\n      \"                                                                       \",\n      \"Source: Carbon Majors database and TidyTuesday\"\n    )\n  ) + \n  \n  # theme cleanup --------------------------------------------------------------\n  geom_hline(yintercept = 0, linewidth = 0.7, color = GRAY) + # bold axis\n  theme(\n    legend.position = \"none\", \n    axis.title.y = element_text(size = 10),\n    plot.background = element_rect(fill = BG_COLOR)\n  ) \n\n\n\n\n\n\n\n\n\nNo plot is perfect, but I am happy with what we have accomplished, and I hope you are too! If you have any questions or corrections, feel free to reach out on Discord, and I’ll be happy to help. And, of course, if you want to contribute to this effort financially, you are more than welcome to buy me a coffee.\nThanks for sticking around, and good luck with your TidyTuesday adventures!"
  },
  {
    "objectID": "tutorials/math_stat_2.html",
    "href": "tutorials/math_stat_2.html",
    "title": "The Bias-Variance Tradeoff | Mathematical Statistics 2",
    "section": "",
    "text": "Last time, we noted that we cannot precisely calculate bias or variance since they are conditional on the parameter we are estimating in the first place. If we knew that value, we wouldn’t need an estimator. Ideally, our estimators would be as close as possible to the true value of our unknown parameter \\(\\theta\\), but there may be infinitely many possible values for \\(\\theta\\). Instead, we will seek to be close to a range of possible values of \\(\\theta\\).\n\n\n\n\n\n\nDefinition\n\n\n\nCloseness describes how much we are willing to “pay” to be some “distance” away from the true value of \\(\\theta\\). We will measure this distance with a function (called a loss function), which is denoted as:\n\\[\n\\ell(\\theta, a).\n\\] This notation is somewhat confusing, because we are now using \\(a\\) to denote our estimate instead of \\(\\hat{\\theta}\\). So you can think of it as \\(\\ell(\\theta, \\hat{\\theta})\\) if that is more helpful (as it is for me).\n\n\nAs an example, if we choose a loss function,\n\\[\n\\ell(\\theta, \\hat{\\theta}) = (\\theta - \\hat{\\theta})^2,\n\\] then we have arrived at the squared-error loss function. Observe that the difference between our guess and the true value is the error, and we are squaring that value. If we take the mean of that function, we arrive at a critical value in mathematical statistics that you may have heard of: the mean-squared error (MSE).\n\n\n\n\n\n\nDefinition\n\n\n\nRisk is the expected loss for a given loss function \\(\\ell\\). Mathematically, it is denoted as \\[\nR_{\\delta}(\\theta) = \\mathbb{E}_{X|\\theta}[\\ell(\\theta, \\delta(\\mathbf{X}))],\n\\] where \\(\\delta(\\mathbf{X})\\) is the function that gives us our estimator. In effect, the risk is the expected loss given some loss function \\(\\ell(\\theta,\\hat{\\theta})\\).\n\n\nWe will get back to loss next time."
  },
  {
    "objectID": "tutorials/math_stat_2.html#worked-example-bias",
    "href": "tutorials/math_stat_2.html#worked-example-bias",
    "title": "The Bias-Variance Tradeoff | Mathematical Statistics 2",
    "section": "Worked example: bias",
    "text": "Worked example: bias\nLet’s work on our first example problem to nail some things down. Let our data \\(X_1, \\cdots, X_n \\sim N(\\mu, \\sigma^2)\\), and let each data point be independent and identically distributed (iid). Like last time, \\(\\sigma^2\\) is fixed and known. We are interested in estimating \\(\\mu\\). Recall our two estimators from the previous article:\n\\[\n\\begin{align*}\n\\delta_1(\\mathbf{X}) &= \\overline{X} \\\\\n\\delta_2(\\mathbf{X}) &= 5.\n\\end{align*}\n\\]\nThe expected value of \\(\\delta_2\\) is 5. Let’s find the expected value of \\(\\delta_1\\). Recall our generic formula for the expectation of an estimator is \\(\\mathbb{E}(\\delta(\\mathbf{X}) | \\theta)\\) for some estimator \\(\\delta\\) and true value \\(\\theta\\). In this case, our estimator is \\(\\overline{X}\\) and our parameter is \\(\\mu\\). Thus: \\[\n\\begin{align*}\n\\mathbb{E}(\\delta_1(\\mathbf{X})|\\theta) &= \\mathbb{E}(\\overline{X} | \\mu) \\\\\n&= \\mathbb{E}\\left[\\frac{\\sum_{i=1}^nX_i}{n}\\right]\n& \\text{expanding out }\\overline{X} \\\\\n&= \\frac{1}{n}\\sum_{i=1}^n\\mathbb{E}(X_i|\\mu)\n& \\text{expectation can be distributed into sums} \\\\\n&= \\frac{1}{n}\\sum_{i=1}^n\\mu\n& \\text{because } \\mathbb{E}(X_i) = \\mu \\text{ for any }i\\\\\n&= \\frac{n\\mu}{n} \\\\\n&= \\mu\n\\end{align*}\n\\]\nWe have a bias of zero! That means that the expected value of our estimator is precisely equivalent to our unknown parameter. It feels like we have solved statistics, right? Well… no. But we will get to that in a moment. For now, let’s visualize bias.\nLet’s say that the true (but unknown to us) value of \\(\\mu\\) is 1. One of our estimators was the constant 5. If we let 5 be our guess for the mean, we are saying, “I think the distribution is centered on 5,” when in reality, it is centered on 1. That difference is shown below.\n\n\nClick here for code\nlibrary(tidyverse)\n\nSIGMA &lt;- 1 # this is our fixed and known standard deviation\nMU &lt;- 1 # this is our true but unknown value\nMU_HAT &lt;- 5 # this is our estimator\n\nggplot() + \n  \n  # plot the true distribution\n  stat_function(\n    fun = dnorm,\n    args = list(mean = MU, sd = SIGMA),\n    color = \"coral3\",\n    linewidth = 2\n  ) +\n  geom_vline(xintercept = MU, color = \"coral3\", size = 1) +\n  annotate(\n    geom = \"text\",\n    hjust = \"right\",\n    x = -.1,\n    y = .3,\n    label = latex2exp::TeX(\"$N(1, \\\\sigma^2)$\"),\n    color = \"coral3\"\n  ) +\n  \n  # plot the estimated distribution\n  stat_function(\n    fun = dnorm,\n    args = list(mean = MU_HAT, sd = SIGMA),\n    color = \"cyan4\",\n    linewidth = 2\n  ) +\n  geom_vline(xintercept = MU_HAT, color = \"cyan4\", size = 1) +\n  annotate(\n    geom = \"text\",\n    hjust = \"right\",\n    x = 6.8,\n    y = .3,\n    label = latex2exp::TeX(\"$N(5, \\\\sigma^2)$\"),\n    color = \"cyan4\"\n  ) +\n  \n  # annotation arrow\n  annotate(\n    geom = \"segment\",\n    x = MU,\n    xend = MU_HAT,\n    y = .19,\n    yend = .19,\n    arrow = arrow(length = unit(0.3, \"cm\"), ends = \"both\", type = \"closed\")\n  ) +\n  annotate(\n    geom = \"text\",\n    x = 3,\n    y = 0.2,\n    label = \"bias\",\n    fontface = \"bold\"\n  ) +\n  \n  # aesthetic fixes\n  theme_minimal() +\n  labs(x = latex2exp::TeX(\"\\\\mu\")) +\n  scale_x_continuous(limits = c(-2, 8), breaks = -3:8) +\n  theme(\n    axis.title.y = element_blank(),\n    axis.line.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank(),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    axis.line.x = element_line()\n  )\n\n\n\n\n\n\n\n\n\nAs we can see, the smaller our bias, the closer our guess becomes to the true value of our parameter. We can also see that a bias of zero is ideal. But there is more to an estimator than bias. Let’s move on to variance."
  },
  {
    "objectID": "tutorials/math_stat_2.html#worked-example-variance",
    "href": "tutorials/math_stat_2.html#worked-example-variance",
    "title": "The Bias-Variance Tradeoff | Mathematical Statistics 2",
    "section": "Worked example: variance",
    "text": "Worked example: variance\nFormally, the variance of an estimator is the expected value of the squared sampling deviations. But that explanation might not be conceptually helpful. So, let’s see an example.\nSay a city has a population of 1,000,000. We want to get the average height of everyone in town. So we sample 1,000 people, measure their height, and use some function to calculate an estimator. We have used the same two estimators so far, \\(\\delta_1 = \\overline{X}\\) and \\(\\delta_2 = 5\\), so we will continue with those.\nDepending on the height of everyone in our 1,000-person sample, we will get a different \\(\\overline{X}\\) each time we take a sample. If we take 100 samples of 1,000 people per sample, we will (probably) arrive at 100 distinct values of \\(\\overline{X}\\). But if we use 5 as our estimator (perhaps to mean 5 feet tall), there will be no such variation in our estimator.\nThe measure of how much our estimator fluctuates as we take more and more samples is a helpful, intuitive understanding of variance. Ideally, we want to have as little variance as possible. We know our estimator has no variance if we use a constant (a number is always just that number), but what about \\(\\overline{X}\\)? Let’s calculate.\n\\[\n\\begin{align*}\nVar[\\delta_1(\\mathbf{X})|\\mu] &= Var(\\overline{X}) \\\\\n&= Var\\left[\\frac{1}{n}\\sum_{i=1}^nX_i\\right]\n& \\text{expanding out } \\overline{X} \\\\\n&= \\frac{1}{n^2}\\sum_{i=1}^nVar(X_i)\n& \\text{must square constants when factoring out of variance} \\\\\n&= \\frac{1}{n^2}\\sum_{i=1}^n\\sigma^2\n& Var(X_i) = \\sigma^2 \\text{ is fixed and known} \\\\\n&= \\frac{n\\sigma^2}{n^2} \\\\\n&= \\frac{\\sigma^2}{n}\n\\end{align*}\n\\]\nNotice that this value is a positive number, which means the variance of \\(\\delta_1 = \\overline{X}\\) is higher than the variance of \\(\\delta_2 = 5\\), which is zero."
  },
  {
    "objectID": "tutorials/math_stat_2.html#the-tradeoff",
    "href": "tutorials/math_stat_2.html#the-tradeoff",
    "title": "The Bias-Variance Tradeoff | Mathematical Statistics 2",
    "section": "The Tradeoff",
    "text": "The Tradeoff\nThis phenomenon, where one estimator has better bias and the other has better variance, is an example of a central balancing act that statisticians have to perform: the bias-variance tradeoff. We have seen an estimator with no bias but positive variance (\\(\\overline{X}\\)) and another with no variance but non-zero bias (5). How do we know which is better? We will explore that question in the following article when we explore this tradeoff more deeply."
  },
  {
    "objectID": "tutorials/math_stat_1.html",
    "href": "tutorials/math_stat_1.html",
    "title": "Welcome to Estimators! | Mathematical Statistics 1",
    "section": "",
    "text": "Say we have some data \\(\\mathbf{X}\\). It’s a vector, so just think of it like a list of \\(n\\) numbers. We want to learn something about how these data came to be. First, we will aggregate our data using a statistic.\n\n\n\n\n\n\nDefinition\n\n\n\nLet \\(X_1, \\cdots, X_n\\) be our data. A statistic is a function of that data. We will denote that statistic with \\(\\delta\\). Crucially, this function cannot contain anything that we don’t know. It is purely a function of known quantities.\n\n\n\n\nLet’s say that our data comes from a normal distribution (a “bell curve”). We denote this with \\(X \\sim N(\\mu, \\sigma^2)\\), where \\(\\mu\\) is the mean of the distribution and \\(\\sigma^2\\) is the variance. Also, to make our life easier, say we know the variance \\(\\sigma^2\\). In practice, this will basically never be the case, but it will simplify our math for now.\nWe have infinitely many options for statistics that we can choose. For example, we could use \\(X_1\\) (that is, the first data point in our vector). While we leave some data on the table in that case, it is certainly a statistic since \\(\\delta = X_1\\) is a function of our data, and there are no unknowns.\nAlternatively, we could use the observed mean of our data. We will call it \\(\\overline{X}\\) (pronounced “\\(X\\) bar”), and it is denoted with \\[\n\\delta(\\mathbf{X}) = \\overline{X} = \\frac{1}{n}\\sum_{i=1}^n X_i.\n\\]\nNotice that this is also a statistic! Although it looks much more complicated, we are still using our data and no unknowns. Here, \\(n\\) is the number of data points that we have, which we know. And we know every \\(X_i\\) because each is part of our data vector \\(\\mathbf{X}\\).\n\n\n\n\n\n\nNote\n\n\n\nConstants (e.g., 7) are also statistics, although no data are involved in the calculation. If it feels like you’re just guessing at random if you do this, you’re right.\n\n\nNow let’s look at an example of a function that is not a statistic: \\[\n\\delta(\\mathbf{X}) = T = \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}}.\n\\] This function will come back in future articles, but for now, recall that we said that we already know the variance \\(\\sigma^2\\). So that means we already know \\(\\sigma\\). We also know \\(n\\), as we mentioned earlier. But \\(\\mu\\) is unknown to us. Because we have an unknown value \\(\\mu\\) in the numerator, \\(T\\) is not a statistic."
  },
  {
    "objectID": "tutorials/math_stat_1.html#statistics",
    "href": "tutorials/math_stat_1.html#statistics",
    "title": "Welcome to Estimators! | Mathematical Statistics 1",
    "section": "",
    "text": "Let’s say that our data comes from a normal distribution (a “bell curve”). We denote this with \\(X \\sim N(\\mu, \\sigma^2)\\), where \\(\\mu\\) is the mean of the distribution and \\(\\sigma^2\\) is the variance. Also, to make our life easier, say we know the variance \\(\\sigma^2\\). In practice, this will basically never be the case, but it will simplify our math for now.\nWe have infinitely many options for statistics that we can choose. For example, we could use \\(X_1\\) (that is, the first data point in our vector). While we leave some data on the table in that case, it is certainly a statistic since \\(\\delta = X_1\\) is a function of our data, and there are no unknowns.\nAlternatively, we could use the observed mean of our data. We will call it \\(\\overline{X}\\) (pronounced “\\(X\\) bar”), and it is denoted with \\[\n\\delta(\\mathbf{X}) = \\overline{X} = \\frac{1}{n}\\sum_{i=1}^n X_i.\n\\]\nNotice that this is also a statistic! Although it looks much more complicated, we are still using our data and no unknowns. Here, \\(n\\) is the number of data points that we have, which we know. And we know every \\(X_i\\) because each is part of our data vector \\(\\mathbf{X}\\).\n\n\n\n\n\n\nNote\n\n\n\nConstants (e.g., 7) are also statistics, although no data are involved in the calculation. If it feels like you’re just guessing at random if you do this, you’re right.\n\n\nNow let’s look at an example of a function that is not a statistic: \\[\n\\delta(\\mathbf{X}) = T = \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}}.\n\\] This function will come back in future articles, but for now, recall that we said that we already know the variance \\(\\sigma^2\\). So that means we already know \\(\\sigma\\). We also know \\(n\\), as we mentioned earlier. But \\(\\mu\\) is unknown to us. Because we have an unknown value \\(\\mu\\) in the numerator, \\(T\\) is not a statistic."
  },
  {
    "objectID": "tutorials/math_stat_1.html#point-estimator-example",
    "href": "tutorials/math_stat_1.html#point-estimator-example",
    "title": "Welcome to Estimators! | Mathematical Statistics 1",
    "section": "Point estimator example",
    "text": "Point estimator example\nLet’s keep going with our data, which comes from a normal distribution. But, to get used to using \\(\\theta\\), say that \\(X \\sim N(\\theta, \\sigma^2)\\). One possible estimator is the example mean \\(\\overline{X}\\) from earlier (i.e., the mean of the observed data). Alternatively, we can use a constant: say 5. Intuitively, it feels like \\(\\hat{\\theta} = \\overline{X}\\) would be a better guess than a simple \\(\\hat{\\theta} = 5\\), because it is actually informed by the data. But how do we quantify that intuition? We will calculate and compare both bias and precision for each.\n\nBias\nBias tells us how often, on average, we get the correct value of our unknown parameter \\(\\theta\\). Mathematically, we hope that the following quantity is as small as possible: \\[\n\\mathbb{E}[\\delta(\\mathbf{X}) | \\theta] - \\theta.\n\\]\nThe confusing-looking term \\(\\mathbb{E}[\\cdot]\\) is the expected value of our estimator, given the value of the unknown parameter \\(\\theta\\). Basically, this is the expected value of \\(\\hat{\\theta}\\). If our estimator \\(\\hat{\\theta}\\) is expected to be exactly correct on average, then this whole term will be 0, which is the smallest possible bias.\n\n\nVariance\nVariance describes the variability of our estimator. Ideally, variance is also small. Intuitively we are less “sure” about our estimate if we have a wider variance. We denote variance with \\(Var(\\delta(\\mathbf{X})|\\theta)\\).\nHowever, notice that both bias and variance are conditional on the true value of our unknown parameter \\(\\theta\\). Thus, we cannot calculate these quantities directly. To deal with this, we will introduce the concept of loss in the next article here!"
  },
  {
    "objectID": "tutorials/math_stat_4.html",
    "href": "tutorials/math_stat_4.html",
    "title": "Sufficiency | Mathematical Statistics 4",
    "section": "",
    "text": "Now that we are getting comfortable with estimators and bias, we can develop some intuition. Say we are estimating the mean of a normal distribution like we did a few articles ago. Like last time, we will let the variance \\(\\sigma^2\\) be known. We have seen two different unbiased estimators before:\n\\[\\begin{align*}\n\\hat{\\theta}_1 &= \\overline{X} \\\\\n\\hat{\\theta}_2 &= X_1\n\\end{align*}\\]\nIntuitively, it feels like \\(\\hat{\\theta}_1\\) would be a better estimator because it uses more of the data. We have previously shown that both of these estimators are unbiased, but \\(Var(\\hat{\\theta}_1) &lt; Var(\\hat{\\theta}_2)\\). Because the biases are equivalent (zero), we say that the estimator with a smaller variance is more efficient."
  },
  {
    "objectID": "projects/tidytuesday_06042024/cheese.html",
    "href": "projects/tidytuesday_06042024/cheese.html",
    "title": "The Cheese Explorer | TidyTuesday",
    "section": "",
    "text": "Click here for code\nlibrary(tidyverse)\nlibrary(DT)\n\ntheme_set(theme_minimal(base_size = 10, base_family = \"Atkinson Hyperlegible\"))\n\ncheese &lt;- read_csv(paste0(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/\",\n  \"data/2024/2024-06-04/cheeses.csv\"\n))\n\nCHEDDAR &lt;- \"#f9b02e\"\n\nMILKS &lt;- cheese |&gt;\n  pull(milk) |&gt;\n  strsplit(\",\\\\s*\") |&gt;\n  unlist() |&gt;\n  unique() |&gt;\n  na.omit() |&gt;\n  sort()"
  },
  {
    "objectID": "projects/tidytuesday_06042024/cheese.html#row",
    "href": "projects/tidytuesday_06042024/cheese.html#row",
    "title": "The Cheese Explorer | TidyTuesday",
    "section": "Row",
    "text": "Row\n\n\nClick here for code\nmilk_count &lt;- cheese |&gt;\n  separate_rows(milk, sep = \", \") |&gt;\n  count(milk, name = \"count\") |&gt;\n  arrange(desc(count)) |&gt;\n  filter(!is.na(milk)) |&gt;\n  mutate(milk = str_to_title(milk))\n\nmilk_count &lt;- milk_count |&gt;\n  mutate(milk = factor(milk, levels = rev(milk_count$milk)))\n\ntop_milk &lt;- milk_count |&gt;\n  slice_head(n = 1) |&gt;\n  pull(milk)\n\nlist(color = \"white\", value = str_to_title(top_milk), icon = \"droplet\")\n\n\n$color\n[1] \"white\"\n\n$value\n[1] \"Cow\"\n\n$icon\n[1] \"droplet\"\n\n\n\n\nClick here for code\nflavor_count &lt;- cheese |&gt;\n  separate_rows(flavor, sep = \", \") |&gt;\n  count(flavor, name = \"count\") |&gt;\n  arrange(desc(count)) |&gt;\n  filter(!is.na(flavor))\n\nflavor_count &lt;- flavor_count |&gt;\n  mutate(flavor = factor(flavor, levels = rev(flavor_count$flavor))) |&gt;\n  slice_head(n = 10)\n\ntop_flavor &lt;- flavor_count |&gt;\n  slice_head(n = 1) |&gt;\n  pull(flavor)\n\nlist(color = CHEDDAR, value = str_to_title(top_flavor), icon = \"eyedropper\")\n\n\n$color\n[1] \"#f9b02e\"\n\n$value\n[1] \"Sweet\"\n\n$icon\n[1] \"eyedropper\"\n\n\n\n\nClick here for code\ncountry_count &lt;- cheese |&gt;\n  separate_rows(country, sep = \", \") |&gt;\n  count(country, name = \"count\") |&gt;\n  arrange(desc(count)) |&gt;\n  filter(!is.na(country))\n\ncountry_count &lt;- country_count |&gt;\n  mutate(country = factor(country, levels = rev(country_count$country))) |&gt;\n  slice_head(n = 10)\n\ntop_country &lt;- country_count |&gt;\n  slice_head(n = 1) |&gt;\n  pull(country)\n\nlist(color = \"white\", value = top_country, icon = \"globe-americas\")\n\n\n$color\n[1] \"white\"\n\n$value\n[1] United States\n61 Levels: Slovakia Norway Mongolia Mauritania Macedonia Lithuania ... United States\n\n$icon\n[1] \"globe-americas\""
  },
  {
    "objectID": "projects/tidytuesday_06042024/cheese.html#row-1",
    "href": "projects/tidytuesday_06042024/cheese.html#row-1",
    "title": "The Cheese Explorer | TidyTuesday",
    "section": "Row",
    "text": "Row\n\nColumn\n\n\nClick here for code\nmilk_count |&gt;\n  ggplot(aes(y = milk, x = count)) +\n  geom_col(fill = CHEDDAR) +\n  geom_text(\n    aes(label = count), \n    hjust = -0.1, \n    color = CHEDDAR, \n    fontface = \"bold\",\n    vjust = \"center\",\n    size = 5\n  ) +\n  coord_cartesian(clip = \"off\") +\n  labs(\n    title = \"The hardest workers in the dairy business\",\n    subtitle = \"Number of unique cheeses produced by each source animal/plant\"\n  ) +\n  theme(\n    axis.title = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\n\nClick here for code\ncountry_count |&gt;\n  ggplot(aes(y = country, x = count)) +\n  geom_col(fill = CHEDDAR) +\n  geom_text(\n    aes(label = count), \n    hjust = -0.1, \n    color = CHEDDAR, \n    fontface = \"bold\",\n    vjust = \"center\",\n    size = 5\n  ) +\n  coord_cartesian(clip = \"off\") +\n  labs(\n    title = \"The lands of the cheese\",\n    subtitle = \"Number of unique cheeses by source country\"\n  ) +\n  theme(\n    axis.title = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\n\nClick here for code\ntexture_count &lt;- cheese |&gt;\n  separate_rows(texture, sep = \", \") |&gt;\n  count(texture, name = \"count\") |&gt;\n  arrange(desc(count)) |&gt;\n  filter(!is.na(texture)) |&gt;\n  mutate(texture = str_to_title(texture))\n\ntexture_count &lt;- texture_count |&gt;\n  mutate(texture = factor(texture, levels = rev(texture_count$texture))) |&gt;\n  slice_head(n = 10)\n\ntexture_count |&gt;\n  ggplot(aes(y = texture, x = count)) +\n  geom_col(fill = CHEDDAR) +\n  geom_text(\n    aes(label = count), \n    hjust = -0.1, \n    color = CHEDDAR, \n    fontface = \"bold\",\n    vjust = \"center\",\n    size = 5\n  ) +\n  coord_cartesian(clip = \"off\") +\n  labs(\n    title = \"It's a texture thing\",\n    subtitle = \"Top ten cheese textures worldwide\"\n  ) +\n  theme(\n    axis.title = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\n\nColumn\nWelcome to TidyTuesday! This is a quick demonstration of the new Quarto dashboard functionality, which allows for quick-and-easy construction of data visualization dashboards with a markdown-like syntax. If you want the full source code for this dashboard (only about 200 lines!!), click here.\nOf course, if you want to discuss the project, reach out on Discord or say thanks by buying me a coffee. Enjoy!\n\n\nClick here for code\nDT::datatable(cheese)"
  },
  {
    "objectID": "projects/crickets/crickets.html",
    "href": "projects/crickets/crickets.html",
    "title": "Cricket Crackers: A Bayesian Approach",
    "section": "",
    "text": "Click here for code\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ggthemes)\nlibrary(gt)\nlibrary(bayesplot)\nlibrary(rstanarm)\n\ngrowth &lt;- read_csv(\"data/growth.csv\") |&gt; \n#growth &lt;- read_csv(\"projects/crickets/data/growth.csv\") |&gt;\n  janitor::clean_names() |&gt;\n  mutate(\n    date_of_hatching = dmy(date_of_hatching),\n    date_of_measurement = dmy(date_of_measurement)\n  )\n\nrepro &lt;- read_csv(\"data/reproduction.csv\") |&gt; \n#repro &lt;- read_csv(\"projects/crickets/data/reproduction.csv\") |&gt; \n  janitor::clean_names() |&gt;\n  mutate(\n    date_of_hatching = dmy(date_of_hatching),\n    date_of_laying = dmy(date_of_laying)\n  )\n\ntemp &lt;- read_csv(\"data/temp_humid.csv\") |&gt;\n#temp &lt;- read_csv(\"projects/crickets/data/temp_humid.csv\") |&gt;\n  janitor::clean_names()\nClick here for code\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618,\n  fit.retina = 3,\n  fig.align = \"center\",\n  dpi = 300\n)\n\nset.seed(1337)\ntheme_set(theme_fivethirtyeight())\n\nLIGHT_BLUE &lt;- \"#32a4a8\""
  },
  {
    "objectID": "projects/crickets/crickets.html#introduction",
    "href": "projects/crickets/crickets.html#introduction",
    "title": "Cricket Crackers: A Bayesian Approach",
    "section": "",
    "text": "The data used in this case study was collected as part of research conducted by Valéry Riantsoa, who is a PhD student at the University of Antananarivo. Riantsoa’s research is part of Project LIFEPLAN, which has the objective of inventorying biological species and developing statistical techniques for studying novel species. Riantsoa focused on four species of Madagascarian crickets: Gryllus madagascariensis, Gryllodes sigillatus, Teleogryllus lemur, and Teleogryllus afer, which have been determined to have the potential to be farmed as highly-nutritious protein sources.\nTo study the four species of crickets, 150 crickets of each species (600 crickets total) each were placed into their own box, which contained a 5 \\(\\times\\) 5 \\(\\times\\) 5 cm egg carton, 1 gram of feed, and 1 gram cotton ball. The boxes were placed into incubators with a constant temperature of 28 degrees C. Another objective was to keep humidity constant, but there were variations in humidity and thus those were recorded as well, since crickets are known to be sensitive to changes in humidity. The parameters that were measured for each cricket were growth (body weight and body weight), mortality, molting times, oviposition, and fecundity. These parameters were measured every 7 days, and it was also noted what stage in the life cycle the cricket was at (L1 through L5).\nThe data used for the case study were split into three different datasets. The growth dataset contained the data for the various aforementioned growth parameters. The reproduction dataset contained data on the reproductive activity of the randomly selected 20 mating pairs per species. The temp_humid dataset recorded information on the humidity of the incubators throughout the experiment.\n\n\n\nThe observational unit for this research was a randomly selected group of 150 crickets of each species from the total population of each cricket species being farmed. Also, to analyze reproductive activity for each species, 20 randomly selected breeding pairs of each species of crickets were chosen from the group of crickets initially chosen for this research.\n\n\n\n\nbody_weight_g: the weight of a cricket (in grams)\ngrowth_time: the number of days hatched passed between a given day a cricket was measured and when the cricket hatched\ngrowth_rate: daily body mass growth rate (grams/day for a particular cricket, calculated as body_weight_g / growth_time)\nspecies: the species of the particular cricket (Gryllus madagascariensis, Gryllodes sigillatus, Teleogryllus lemur, or Teleogryllus afer)\nhatched_eggs: number of eggs hatched by a given breeding pair\n\n\n\n\nThe overall objective of the research project is to determine which species of crickets is best suited for cultivation. Compared to traditional protein sources, cultivating crickets is much more cost effective in terms of land and feed needed. Therefore, crickets can be farmed as a food source in regions where the environment is otherwise unfriendly to livestock cultivation. Cricket meat has already been successfully made into cricket flour, which can be used to make nutritious snacks such as cricket crackers.\nBased on the data, we determined that it would be best to look at which species of crickets has the highest average of meat produced per day. Meat produced per day is the change in body mass per cricket per day (grams/day for a particular cricket) multiplied by the total number of crickets in a population; in order to determine the total number of crickets in a population, we will model the number of eggs hatched by a given cricket species breeding pair. Thus, the main research question that this case study aims to address is what species of crickets, on average, have the largest amount of meat produced per day.\n\n\n\n\n\n\nClick here for code\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ggthemes)\nlibrary(gt)\nlibrary(knitr)\nlibrary(bayesplot)\nlibrary(rstanarm)\n\ngrowth &lt;- read_csv(\"data/growth.csv\", show_col_types = FALSE) |&gt; \n  janitor::clean_names() |&gt;\n  mutate(\n    date_of_hatching = dmy(date_of_hatching),\n    date_of_measurement = dmy(date_of_measurement)\n  )\n\nrepro &lt;- read_csv(\"data/reproduction.csv\", show_col_types = FALSE) |&gt; \n  janitor::clean_names() |&gt;\n  mutate(\n    date_of_hatching = dmy(date_of_hatching),\n    date_of_laying = dmy(date_of_laying)\n  )\n\ntemp &lt;- read_csv(\"data/temp_humid.csv\", show_col_types = FALSE) |&gt; \n  janitor::clean_names()\n\n\n\n\nClick here for code\np_success &lt;- repro |&gt;\n  group_by(paired_adults, species) |&gt;\n  summarise(\n    eggs_laid = sum(eggs_number, na.rm = T),\n    hatched = sum(hatched_eggs, na.rm = T)  \n  ) |&gt;\n  mutate(succ_prop = hatched / eggs_laid)\n\np_success$id &lt;- 1:nrow(p_success)\n\np_success |&gt;\n  ggplot(aes(x = succ_prop, color = species)) +\n  geom_density(linewidth = 1) +\n  theme_calc() +\n  scale_color_wsj() +\n  labs(\n    x = \"Proportion of eggs hatched\",\n    y = \"Density\",\n    color = \"Species\",\n    title = \"Successful eggs distribution\",\n    subtitle = \"The proportion of eggs successfully hatched by species\"\n  )\n\n\n\n\n\n\n\n\n\nBased on this plot, it can be observed that the distributions of the proportion of successful eggs hatched for each of the breeding pairs grouped by species appear to be unimodal and right skewed. The medians of the distributions for Gryllus madagascariensis + Teleogryllus afer and Gryllodes sigillatus + Teleogryllus lemur respectively are very similar, with the second pair having slightly higher medians. In addition, the difference between the medians appears to be less than 0.01.\n\n\nClick here for code\ngrowth |&gt;\n  group_by(stage, species) |&gt;\n  summarise(weight = mean(body_weight_g, na.rm = T)) |&gt;\n  ungroup() |&gt;\n  mutate(stage = as.factor(stage)) |&gt;\n  ggplot(aes(x = stage, y = weight, color = species, group = species)) +\n  geom_line(linewidth = 1) +\n  theme_calc() +\n  scale_color_wsj() +\n  labs(\n    x = \"Growth stage\",\n    y = \"Weight (g)\",\n    color = \"Species\",\n    title = \"Cricket growth rates\",\n    subtitle = \"Mean weight of all crickets by life stage and species\"\n  )\n\n\n\n\n\n\n\n\n\nThe growth rates for all four species appear to be very similar between the L1 and L4 life stages, but Gryllus madagascariensis appears to have a significantly larger increase in growth rate compared to the other species. Additionally, Gryllus madagascariensis and Teleogryllus afer matured to the L5 stages about a week before the other two species. In the L5 life stage, Gryllus madagascariensis grew as large as 5 times the size of the largest individuals of the other species.\n\n\nClick here for code\ngrowth |&gt;\n  filter(day_weekly == 42) |&gt;\n  mutate(dead = ifelse(mortality_y_n=='Y', 1, 0)) |&gt;\n  group_by(species, dead) |&gt;\n  summarise(count = n()) |&gt;\n  mutate(dead = case_when(dead == 0 ~ \"Not Dead\",\n                          dead == 1 ~ \"Dead\")) |&gt;\n  pivot_wider(names_from = dead, values_from = count) |&gt;\n  rename(Species = species) |&gt;\n  kable()\n\n\n\n\n\nSpecies\nNot Dead\nDead\n\n\n\n\nGryllodes sigillatus\n127\n23\n\n\nGryllus madagascariensis\n140\n10\n\n\nTeleogryllus afer\n117\n33\n\n\nTeleogryllus lemur\n134\n16\n\n\n\n\n\nThis table summarizes the proportion of crickets that were dead and alive by species at the end of the research experiment. The table shows that Gryllus madagascariensis had the lowest proportion of dead crickets and the highest proportion of crickets that were alive.\n\n\nClick here for code\nrepro |&gt;\n  mutate(hatchdays = difftime(date_of_hatching, date_of_laying, \n                              units = \"days\")) |&gt;\n  ggplot(aes(x = hatchdays, fill = species)) +\n  geom_histogram(stat = \"count\") +\n  theme_calc() +\n  scale_fill_wsj() +\n  facet_wrap(~ species, nrow = 2) +\n  labs(x = \"Number of Days for an Egg to Hatch\",\n       y = \"Count\",\n       title = \"Cricket Hatch Times\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nThis plot visualizes the distribution of the time each egg cluster took to hatch after laying by species. It appears that the majority of Gryllus madagascariensis eggs took 15 days or more to hatch. On the other hand, for the other three species, all eggs hatched within 12 days. In addition, all of Teleogryllus afer eggs hatched in 10 days or less.\nBased on the EDA, it appears that the average proportion of eggs hatched in relation to the total number of eggs laid is very similar regardless of species or genus. In addition, it is significant to note that in comparison to the other three species of crickets, Gryllus madagascariensis crickets were measured to have a significantly higher growth rate between the L4 and L5 stages, longer hatch times, and both the highest proportion of living crickets and lowest proportion of dead crickets at the end of the experiment.\nThere were also some additional factors of consideration that were noted during the EDA. For example, there could be a possibility of crickets eating their own molts, which could have affected the researchers’ counts. In addition, crickets could have also eaten unhatched eggs, thus potentially affecting the counts of hatched eggs per breeding pair."
  },
  {
    "objectID": "projects/crickets/crickets.html#methodology",
    "href": "projects/crickets/crickets.html#methodology",
    "title": "Cricket Crackers: A Bayesian Approach",
    "section": "Methodology",
    "text": "Methodology\n\nData Cleaning\nIn order to ensure a data set that satisfies the assumptions of independence, we decided to only include the observations in which the individual cricket had reached the 42nd day to be measured for our model which left us with the 600 individual crickets (150 per species) which is the exact number of crickets per sepcies used in the experiment.\nWe also added a new variable for measuring growth (growth_time), which we defined as the number of days hatched passed between a given day a cricket was measured and when the cricket hatched.\nFor the reproduction dataset, in order to satisfy the assumptions of independence, we took the mean number of eggs hatched by a breeding pair and rounded down so that the data could be used for our Poisson model that requires an integer response variable.\n\n\nModel Overview and Justification\nWe initially implemented two Bayesian generalized linear models using stan_glm() from the rstanarm package.\nThe first model is a model from the Gaussian family that had the daily growth rate as the response variable with species as the predictor. This model used a normal prior using the sample mean regardless of species and sample standard deviation regardless of species. The second model is a model from the Poisson family with log-link that had the number of eggs hatched for a given breeding pair as the response variable with species as the predictor. This model also used a normal prior with sample mean and sample standard deviation of the number of eggs hatched per breeding pair regardless of species.\nBoth models contain a single intercept (for Grillodes sigillatus), and posterior samples reported for the other three species are deviations from that intercept. As such, posterior samples for non-intercept species are added to the intercept samples and are shown in the following histograms. For example, a posterior sample of 50 eggs for Grillodes sigillatus and a coefficient estimate of -10 for Teleogryllus afer would result in a reported sample of 40 eggs for Teleogryllus afer.\nWe decided to use a Gaussian family model for predicting daily growth rate because it is a continuous variable, which matches the support of the Gaussian distribution. Similarly, we decided to use a Poisson family model for predicting the number of eggs hatched because the support is the same as that of the Poisson distribution, which is only natural numbers starting from 0.\nWe use the sample mean and sample standard without regard for species to avoid skewing our results too heavily based off of information from our observational data because of the comparatively small number of cricket breeding pairs compared to the number of breeding pairs one would be cultivating in the real world.\nAfter running sampling from these two models, two separate posterior distributions were generated. The values from these posterior distributions were then multiplied together to get a final distribution of meat produced per breeding pair per species per day.\n\n\nModel Assumptions\nFor this model, two assumptions were made. Because dead crickets were not measured for their body mass, we dropped these dead crickets from our dataset under the assumption that they would not contribute to our final meat production. Based on the EDA, it was observed that the four species of crickets, especially Gryllus madagascariensis, experienced significant growth in body weight between the L4 and L5. For the model used in this case study, the growth rate between L4 and L5 has been collapsed such that the growth rate from day 1 to day 42 is assumed to be linear."
  },
  {
    "objectID": "projects/crickets/crickets.html#results",
    "href": "projects/crickets/crickets.html#results",
    "title": "Cricket Crackers: A Bayesian Approach",
    "section": "Results",
    "text": "Results\n\nBayesian Daily Growth Rate Model\n\n\nClick here for code\nITERS &lt;- 5000\n\ngrowth_b &lt;- growth |&gt;\n  filter(day_weekly == 42) |&gt;\n  mutate(\n    growth_time = as.numeric(\n      difftime(date_of_measurement, date_of_hatching, units = \"days\")\n    )\n  ) |&gt;\n  select(body_weight_g, growth_time, species, individuals)\n\ngrowth_b &lt;- growth_b |&gt;\n  mutate(growth_rate = body_weight_g / growth_time) |&gt;\n  filter(!is.na(growth_rate))\n\nMEAN &lt;- mean(growth_b$growth_rate, na.rm = TRUE)\nSD &lt;- sd(growth_b$growth_rate, na.rm = TRUE)\nmodel_growth &lt;- stan_glm(\n  growth_rate ~ species,\n  data = growth_b,\n  family = gaussian,\n  prior = normal(MEAN, SD),\n  chains = 1,\n  iter = ITERS,\n  seed = 440\n)\n\n\n\n\nClick here for code\ngrowth_tib &lt;- as_tibble(model_growth)[,1:4]\ncolnames(growth_tib) &lt;- c(\"a\", \"b\", \"c\", \"d\")\ngrowth_tib &lt;- growth_tib |&gt;\n  mutate(\n    b = b + a,\n    c = c + a,\n    d = d + a\n  )\ncolnames(growth_tib) &lt;- sort(unique(growth$species))\n\ngrowth_tib |&gt;\n  pivot_longer(cols = everything(), names_to = \"species\", values_to = \"rate\") |&gt;\n  ggplot(aes(x = rate*100, fill = species)) +\n  geom_histogram(bins = 100) +\n  theme_calc() +\n  scale_fill_wsj() +\n  facet_wrap(~species) +\n  theme(legend.position = \"none\") +\n  labs(\n    x = \"Growth rate (per 100 crickets, per day)\",\n    y = paste0(\"Count (n = \", ITERS, \")\"),\n    title = \"Posterior daily growth rate\",\n    subtitle = paste(\n      \"Results are multiplied by 100 for ease of legibility. Model\",\n      \"coefficients \\n are per-cricket.\"\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\nClick here for code\nci_growth &lt;- as_tibble(posterior_interval(model_growth))[1:4,]\ncolnames(ci_growth) &lt;- c(\"lower\", \"upper\")\nint_vals &lt;- unname(as_vector(slice_head(ci_growth, n = 1)))\nci_growth[2:4,1] &lt;- ci_growth[2:4,1] + int_vals[1]\nci_growth[2:4,2] &lt;- ci_growth[2:4,2] + int_vals[2]\nci_growth$species &lt;- sort(unique(growth$species))\nci_growth &lt;- select(ci_growth, species, lower, upper) |&gt;\n  mutate(lower = round(lower, 4), upper = round(upper, 4))\n\n\n\n\nBayesian Hatched Eggs Model\n\n\nClick here for code\nmean_eggs &lt;- mean(repro$hatched_eggs, na.rm = TRUE)\nsd_eggs &lt;- sd(repro$hatched_eggs, na.rm = TRUE)\n\nrepro_b &lt;- repro |&gt;\n  group_by(species, paired_adults) |&gt;\n  summarise(hatched_eggs = floor(mean(hatched_eggs, na.rm = TRUE)))\n\nmodel_eggs &lt;- stan_glm(\n  hatched_eggs ~ species,\n  data = repro_b,\n  family = poisson(link = \"log\"),\n  prior = normal(mean_eggs, sd_eggs),\n  chains = 1,\n  iter = ITERS,\n  seed = 440\n)\n\n\n\n\nClick here for code\neggs_tib &lt;- exp(as_tibble(model_eggs)) \ncolnames(eggs_tib) &lt;- c(\"a\", \"b\", \"c\", \"d\")\neggs_tib &lt;- eggs_tib |&gt;\n  mutate(\n    b = b + a,\n    c = c + a,\n    d = d + a\n  )\ncolnames(eggs_tib) &lt;- sort(unique(growth$species))\n\neggs_tib |&gt;\n  pivot_longer(cols = everything(), names_to = \"species\", values_to = \"eggs\") |&gt;\n  ggplot(aes(x = eggs, fill = species)) +\n  geom_histogram(bins = 50) +\n  theme_calc() +\n  scale_fill_wsj() +\n  facet_wrap(~species, nrow = 2) +\n  theme(legend.position = \"none\") +\n  labs(\n    x = \"Predicted number of eggs laid (per day)\",\n    y = paste0(\"Count (n = \", ITERS, \")\"),\n    title = \"Posterior distribution of eggs laid per day\",\n    subtitle = \"Only Gryllodes sigillatus shows lower egg production\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nClick here for code\nci_eggs &lt;- exp(as_tibble(posterior_interval(model_eggs)))\ncolnames(ci_eggs) &lt;- c(\"lower\", \"upper\")\nint_vals &lt;- unname(as_vector(slice_head(ci_eggs, n = 1)))\nci_eggs[2:4,1] &lt;- ci_eggs[2:4,1] + int_vals[1]\nci_eggs[2:4,2] &lt;- ci_eggs[2:4,2] + int_vals[2]\nci_eggs$species &lt;- sort(unique(growth$species))\n\nci_eggs &lt;- ci_eggs |&gt;\n  select(species, lower, upper) |&gt;\n  mutate(lower = round(lower, 2), upper = round(upper, 2))\n\n\n\n\nFinal Meat-per-day Distribution\n\n\nClick here for code\nmelt_growth &lt;- growth_tib |&gt;\n  pivot_longer(cols = everything(), names_to = \"species\", values_to = \"growth\") \n\nmelt_eggs &lt;- eggs_tib |&gt;\n  pivot_longer(cols = everything(), names_to = \"species\", values_to = \"eggs\") \n\npost_meat_per_day &lt;- tibble(\n  species = melt_growth$species,\n  growth = melt_growth$growth,\n  eggs = melt_eggs$eggs,\n  meat_per_day = growth * eggs\n)\n\nCI_meat_per_day &lt;- post_meat_per_day |&gt;\n  group_by(species) |&gt;\n  summarise(\n    lower = round(quantile(meat_per_day, 0.025), 2),\n    upper = round(quantile(meat_per_day, 0.975), 2)\n  )\n\npost_meat_per_day |&gt;\n  ggplot(aes(x = meat_per_day, fill = species)) +\n  geom_histogram(bins = 50) +\n  theme_calc() +\n  scale_fill_wsj() +\n  facet_wrap(~species, nrow = 2) +\n  theme(legend.position = \"none\") +\n  labs(\n    x = \"Posterior meat production per day\",\n    y = paste0(\"Count (n = \", ITERS, \")\"),\n    title = \"Simulated distribution of meat production per day\",\n    subtitle = \"Meat production values are shown per breeding pair.\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nClick here for code\nci_table &lt;- left_join(ci_growth, ci_eggs, by = \"species\") |&gt;\n  mutate(\n    lower.z = CI_meat_per_day$lower,\n    upper.z = CI_meat_per_day$upper\n  ) |&gt;\n  gt(rowname_col = \"species\") |&gt;\n  tab_spanner(\n    label = \"Growth rate\\n(per-cricket)\", \n    columns = c(lower.x, upper.x)\n  ) |&gt;\n  tab_spanner(label = \"Eggs hatched\", columns = c(lower.y, upper.y)) |&gt;\n  tab_spanner(label = \"Meat per day*\", columns = c(lower.z, upper.z)) |&gt;\n  cols_label(\n    lower.x = \"5%\",\n    lower.y = \"5%\",\n    lower.z = \"5%\",\n    upper.x = \"95%\",\n    upper.y = \"95%\",\n    upper.z = \"95%\"\n  ) |&gt;\n  tab_header(md(\"**Posterior 95% confidence intervals**\")) |&gt;\n  data_color(c(2,3,6,7), palette = \"#ededed\") |&gt;\n  tab_style(\n    locations = cells_body(columns = c(6,7)),\n    style = list(cell_text(weight = \"bold\"))\n  ) |&gt;\n  tab_options(\n    footnotes.padding = 3,\n    footnotes.font.size = 13\n  )\nci_table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosterior 95% confidence intervals\n\n\n\n\nGrowth rate (per-cricket)\n\n\nEggs hatched\n\n\nMeat per day*\n\n\n\n5%\n95%\n5%\n95%\n5%\n95%\n\n\n\n\nGryllodes sigillatus\n0.0008\n0.0016\n67.79\n73.84\n0.05\n0.12\n\n\nGryllus madagascariensis\n0.0126\n0.0144\n68.82\n75.01\n0.91\n1.03\n\n\nTeleogryllus afer\n0.0007\n0.0024\n68.79\n74.97\n0.08\n0.14\n\n\nTeleogryllus lemur\n0.0012\n0.0029\n68.79\n74.97\n0.12\n0.18\n\n\n\n\n\n\n\nWe are 95% confident that for a given breeding pair of Gryllodes sigillatus crickets, we expect that the average meat per day outputted would be between 0.06 g and 0.12 g. We are 95% confident that for a given breeding pair of Gryllus madagascariensis crickets, we expect that the average meat per day outputted would be between 0.92 g and 1.03 g. We are 95% confident that for a given breeding pair of Teleogryllus afer crickets, we expect that the average meat per day outputted would be between 0.08 g and 0.14 g. We are 95% confident that for a given breeding pair of Teleogryllus lemur crickets, we expect that the average meat per day outputted would be between 0.12 g and 0.18 g."
  },
  {
    "objectID": "projects/crickets/crickets.html#discussion",
    "href": "projects/crickets/crickets.html#discussion",
    "title": "Cricket Crackers: A Bayesian Approach",
    "section": "Discussion",
    "text": "Discussion\nBased on our combined model output, the Gryllus madagascariensis species is best fit for cultivation based on the average meat outputted per breeding pair for a given species with the caveat that the Gryllus madagascariensis species has a longer mean incubation time in days (Appendix fig. 1) and negligibly lower lifetime hatch rate (Appendix fig. 2) when compared to its counterpart species which might make lengthen the time it takes to start up cricket meat production.\nWe do feel that the model development process using the data for this case study was limited in terms of finding variables in the dataset to use as predictors. The limitations can be attributed to how the only main independent variable we found was the species of crickets, since this was experimental data collected, it is reasonable that there was only one independent variable. One of the few metrics in which Gryllus madagascariensis underperformed relative to the other species was time to hatch after the eggs had been laid. However, this issue only affects the initial startup time and increases the need for egg storage.\nBecause the datasets had limited number of parameters that we could use as predictors, we would recommend in the future that Riantsoa and the researchers run a different experiment with just the Gryllus madagascariensis and adjust for other factors that were controlled in this experiment, such as humidity and temperature. We also recommend using more breeding pairs in future experiments, as there were only 20 breeding pairs per species of cricket. With further experimentation on the optimal conditions for Gryllus madagascariensis cultivation, access to better alternative protein sources in areas that lack the necessary infrastructure and environment for traditional livestock cultivation can continue to improve. Success in this area of research could lead to more efficient and equitable access to protein sources."
  },
  {
    "objectID": "projects/crickets/crickets.html#appendix",
    "href": "projects/crickets/crickets.html#appendix",
    "title": "Cricket Crackers: A Bayesian Approach",
    "section": "Appendix",
    "text": "Appendix\n\nfig. 1\n\n\nClick here for code\nrepro |&gt;\n  mutate(incubation_time = as.numeric(\n      difftime(date_of_hatching, date_of_laying, units = \"days\")\n  )) |&gt;\n  ggplot(aes(x = incubation_time, fill = species)) +\n  geom_density() +\n  theme_calc() +\n  scale_fill_wsj() +\n  facet_wrap(~species) +\n  theme(legend.position = \"none\") +\n  labs(\n    x = \"Incubation time (days)\",\n    y = element_blank(),\n    title = \"Cricket egg incubation density\",\n    subtitle = paste(\n      \"Incubation time is the time difference (in days) between\",\n      \"the laying and hatching \\n of an egg.\"\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\nfig. 2\n\n\nClick here for code\nrepro |&gt;\n  group_by(paired_adults, species) |&gt;\n  summarise(\n    eggs_laid = sum(eggs_number, na.rm = T),\n    hatched = sum(hatched_eggs, na.rm = T)  \n  ) |&gt;\n  mutate(p_success = hatched / eggs_laid) |&gt;\n  ggplot(aes(x = p_success, fill = species)) +\n  geom_density() +\n  scale_fill_wsj() +\n  theme_calc() +\n  facet_wrap(~species) +\n  theme(legend.position = \"none\") +\n  labs(\n    x = \"Hatch rate\",\n    y = element_blank(),\n    title = \"Lifetime hatch rate by species\",\n    subtitle = \"Each mating pair is averaged over their entire mating lives.\"\n  )"
  },
  {
    "objectID": "projects/crickets/crickets.html#changes-from-original-submission",
    "href": "projects/crickets/crickets.html#changes-from-original-submission",
    "title": "Cricket Crackers: A Bayesian Approach",
    "section": "Changes From Original Submission",
    "text": "Changes From Original Submission\nThe feedback provided had a suggestion for improvement regarding the introduction, specifically adding in more context about the research study and why studying the cultivation of crickets is significant. We added the following to our discussion of the research question in the introduction:\n\nCompared to traditional protein sources, cultivating crickets is much more cost effective in terms of land and feed needed. Therefore, crickets can be farmed as a food source in regions where the environment is otherwise unfriendly to livestock cultivation. Cricket meat has already been successfully made into cricket flour, which can be used to make nutritious snacks such as cricket crackers.\n\nOn our initial submission feedback, a concern was noted that we did not sufficiently explain that there were not per-species intercepts in either of the models. We added the following text to note the single intercept and explain the displayed results:\n\nBoth models contain a single intercept (for Grillodes sigillatus), and posterior samples reported for the other three species are deviations from that intercept. As such, posterior samples for non-intercept species are added to the intercept samples and are shown in the following histograms. For example, a posterior sample of 50 eggs for Grillodes sigillatus and a coefficient estimate of -10 for Teleogryllus afer would result in a reported sample of 40 eggs for Teleogryllus afer.\n\nWe also had too much of a negative tone to our conclusion, so we changed some of the language to end on a “high note.” That language is here:\n\nWith further experimentation on the optimal conditions for Gryllus madagascariensis cultivation, access to better alternative protein sources in areas that lack the necessary infrastructure and environment for traditional livestock cultivation can continue to improve. Success in this area of research could lead to more efficient and equitable access to protein sources."
  },
  {
    "objectID": "tutorials/math_stat_4.html#example",
    "href": "tutorials/math_stat_4.html#example",
    "title": "Sufficiency | Mathematical Statistics 4",
    "section": "Example",
    "text": "Example\nLet \\(\\mathbf{X} \\overset{\\mathrm{iid}}{\\sim} Poisson(\\theta)\\). Because the sum of Poisson random variables is also Poisson (if you didn’t know this, it’s a helpful trick to memorize), we can let\n\\[\nT = \\sum_{i=1}^n X_i \\sim Poisson(n\\theta).\n\\]\nbe our statistic. For an individual \\(X_i\\), we know (from memory or from Wikipedia) that the PDF is\n\\[\n\\mathbb{P}(X_i= x|\\theta) = \\prod_{i=1}^n \\frac{exp(-\\theta)\\theta^{x_i}}{x_i!}.\n\\]\nWe can then find \\(\\mathbb{P}_T(\\cdot)\\), replacing \\(\\theta\\) with \\(n\\theta\\) and \\(x_i\\) with \\(t\\), because \\(T\\) follows a \\(Poisson(n\\theta)\\) instead of a \\(Poisson(\\theta)\\) distribution. Then, we arrive at our PDF:\n\\[\n\\mathbb{P}_T(T=t|\\theta) = \\frac{exp\\{-n\\theta\\}(n\\theta)^t}{t!}.\n\\]\nWhen we solve for"
  },
  {
    "objectID": "projects/crickets/crickets.html#the-experiment",
    "href": "projects/crickets/crickets.html#the-experiment",
    "title": "Cricket Crackers: A Bayesian Approach",
    "section": "The Experiment",
    "text": "The Experiment\nResearchers are comparing four species of crickets: Teleogryllus lemus, Gryllodes sigillatus, Teleogryllus afer, and Gryllus madagascariensis. In this experiment, 150 crickets from each species are incubated in a lab-controlled setting. Temperature and humidity were controlled, and each species had 12 hours of light and dark per day. Unlimited food, in the form of chicken feed, and unlimited water were provided to each cricket. Researchers allowed each cricket to grow individually inside a ventilated plastic box. They measured each cricket’s body length and weight measurements weekly and conducted checks every three days to see if a cricket had molted (that is, shed its exoskeleton and entered a new growth stage).\nUpon reaching adulthood, 20 females and 20 males from each species were selected at random and paired together to breed eggs. Every 24 hours, researchers check for eggs, count them, and separate them to allow them to hatch. They then record the number of eggs laid and the number successfully hatched. We are left with a pair of datasets: one for reproduction data about the eggs and the other about the growth rates of the crickets when they hatch!"
  },
  {
    "objectID": "projects/crickets/crickets.html#our-plan",
    "href": "projects/crickets/crickets.html#our-plan",
    "title": "Cricket Crackers: A Bayesian Approach",
    "section": "Our Plan",
    "text": "Our Plan\nGiven these data, our job as an analyst is to help decide which species is best for future cultivation. Ideally, we want to maximize the amount of cricket flour generated per unit of time (in our case, per day). Since the entire cricket is ground into flour, this is equivalent to trying to find the maximum amount of body weight added per cricket per day. With that number, any future cricket farmer could simply multiply by the number of crickets they expect to have to arrive at total flour production estimates per day.\nLet’s take a Bayesian approach. We will build two models: the first will give a posterior distribution of the number of eggs laid per species, and the second will be a posterior distribution of growth per day by species. Finally, we can multiply the posterior samples of each model together to arrive at a final distribution that is a combination of the number of eggs laid and the amount of growth each cricket will experience per day. In other words, total meat per day! Once we find the best species under these ideal conditions, future research can vary the breeding conditions to find optimal conditions for that species."
  },
  {
    "objectID": "projects/crickets/crickets.html#the-egg-production-model",
    "href": "projects/crickets/crickets.html#the-egg-production-model",
    "title": "Cricket Crackers: A Bayesian Approach",
    "section": "The Egg Production Model",
    "text": "The Egg Production Model\nLet’s take a sample of our reproduction dataset to see what we are working with.\n\n\nClick here for code\nrepro |&gt;\n  select(-c(temperature_c, mortality_of_adults_y_n)) |&gt;\n  slice_sample(n = 5) |&gt;\n  gt() |&gt;\n  tab_header(md(\"**Random sample: reproduction dataset**\"))\n\n\n\n\n\n\n\n\nRandom sample: reproduction dataset\n\n\nspecies\npaired_adults\ndate_of_laying\neggs_number\ndate_of_hatching\nhatched_eggs\n\n\n\n\nGryllodes sigillatus\n19\n2023-12-23\n102\n2024-01-03\n66\n\n\nTeleogryllus lemur\n6\n2023-11-07\nNA\nNA\nNA\n\n\nTeleogryllus afer\n15\n2024-01-13\n103\n2024-01-20\n72\n\n\nTeleogryllus afer\n10\n2024-01-13\n113\n2024-01-20\n79\n\n\nTeleogryllus lemur\n7\n2023-11-01\n116\n2023-11-08\n81\n\n\n\n\n\n\n\nWe can start to see the structure of the data. Each row represents a single mating pair (paired_adults) on a specific date (date_of_laying). The eggs laid that day hatch at a later date (date_of_hatching), and the number of those eggs that successfully hatch are provided as well (hatched_eggs). The first question that comes to my mind is whether any of the four species has a significantly higher hatch rate than the others. Let’s have a look.\n\n\nClick here for code\np_success &lt;- repro |&gt;\n  group_by(paired_adults, species) |&gt;\n  summarise(\n    eggs_laid = sum(eggs_number, na.rm = T),\n    hatched = sum(hatched_eggs, na.rm = T)  \n  ) |&gt;\n  mutate(succ_prop = hatched / eggs_laid)\n\np_success$id &lt;- 1:nrow(p_success)\n\np_success |&gt;\n  ggplot(aes(x = succ_prop, color = species)) +\n  geom_density(linewidth = 2) +\n  scale_color_wsj() +\n  labs(\n    x = \"Proportion of eggs hatched\",\n    y = \"Density\",\n    color = \"Species\",\n    title = \"Successful eggs distribution\",\n    subtitle = \"The proportion of laid eggs successfully hatched by species\"\n  )\n\n\n\n\n\n\n\n\n\nThe four species seem to neatly stick to one of two modes. But again, we have to observe the \\(x\\) axis: there is next to no variance among any of the four species! The two modes are just about \\(0.0672\\) and \\(0.0676\\). Either we have a set of four strangely uniform cricket species, or there is a date collection error somewhere. Either way, hatch rate probably isn’t going to help us determine the best species. Since all four species have an approximately equal hatch rate, all that matters to us is the absolute number of eggs hatched. Let’s model it!\n\nBuilding the model\nThe only variable left to use as a predictor is the species of each mating pair. We have already eliminated hatch rate and both environmental factors that we were given by researchers, so we won’t be able to correct for other predictors. That’s a shame, but a univariate model is better than none. To maintain the independence of our observations, we don’t want to include every pair multiple times. If we did, then we would have reason to believe that the eggs hatched by one row is impacted by the eggs of another row. After all, they are the same breeding pair!\nTo maintain independence, let’s take the mean of each mating pair’s reproductive life. In doing so, we lose some data but gain statistical independence, without which we can’t do a model at all. Once we take that mean, we can run a Bayesian regression model with our species predictor and arrive at a per-species posterior distribution of the number of eggs successfully hatched per day. To make life easier, let’s see if we can use a common prior. The easiest one to check is the normal prior. Let’s plot our observed density against a normal distribution with the sample mean and standard deviation.\n\n\nClick here for code\nrepro_b &lt;- repro |&gt;\n  group_by(species, paired_adults) |&gt;\n  summarise(hatched_eggs = floor(mean(hatched_eggs, na.rm = TRUE)))\n\nrepro_b |&gt;\n  ggplot(aes(x = hatched_eggs)) +\n  geom_density(\n    aes(color = \"Observed distribution\"),\n    linewidth = 2,\n    color = \"darkgray\"\n  ) +\n  stat_function(\n    aes(color = \"Normal distribution\"),\n    fun = dnorm, \n    args = list(mean = mean(repro_b$hatched_eggs), \n                sd = sd(repro_b$hatched_eggs)), \n    linewidth = 2,\n    color = LIGHT_BLUE\n  ) +\n  annotate(\n    geom = \"text\",\n    label = \"Normal distribution\",\n    x = 67,\n    y = 0.035,\n    hjust = \"right\",\n    vjust = \"bottom\",\n    fontface = \"bold\",\n    color = LIGHT_BLUE\n  ) +\n  annotate(\n    geom = \"text\",\n    label = \"Observed distribution\",\n    x = 73,\n    y = 0.077,\n    hjust = \"right\",\n    vjust = \"bottom\",\n    fontface = \"bold\",\n    color = \"darkgray\"\n  ) +\n  labs(\n    x = \"Eggs hatched\",\n    title = \"Observed eggs hatched vs normal distribution\",\n    subtitle = paste(\"The observed egg hatch rate is similar enough to use a\",\n                     \"normal prior.\")\n  ) +\n  theme(axis.title.x = element_text())\n\n\n\n\n\n\n\n\n\nLooks good enough to me! Using data visualization is a quick and dirty way to select a prior, but with a model as simple as ours and an observed distribution so close to normal (thanks, Central Limit Theorem!), I’m hoping that it will at least do better than throwing an un-informative prior at it and hoping for the best.\nSo we’re ready for our first of two models! Let’s run it with 5000 iterations with our normal prior with observed mean and standard deviation!\n\n\nClick here for code\nITERS &lt;- 5000\nmean_eggs &lt;- mean(repro$hatched_eggs, na.rm = TRUE)\nsd_eggs &lt;- sd(repro$hatched_eggs, na.rm = TRUE)\n\nmodel_eggs &lt;- stan_glm(\n  hatched_eggs ~ species,\n  data = repro_b,\n  family = poisson(link = \"log\"),\n  prior = normal(mean_eggs, sd_eggs),\n  chains = 1,\n  iter = ITERS,\n  seed = 440,\n  refresh = 0\n)\n\n\n\n\nClick here for code\neggs_tib &lt;- exp(as_tibble(model_eggs)) \ncolnames(eggs_tib) &lt;- c(\"a\", \"b\", \"c\", \"d\")\neggs_tib &lt;- eggs_tib |&gt;\n  mutate(\n    b = b + a,\n    c = c + a,\n    d = d + a\n  )\ncolnames(eggs_tib) &lt;- sort(unique(growth$species))\n\neggs_tib |&gt;\n  pivot_longer(cols = everything(), names_to = \"species\", values_to = \"eggs\") |&gt;\n  ggplot(aes(x = eggs, fill = species)) +\n  geom_histogram(bins = 50) +\n  scale_fill_wsj() +\n  facet_wrap(~species, nrow = 2) +\n  theme(legend.position = \"none\") +\n  labs(\n    x = \"Predicted number of eggs laid (per day)\",\n    y = paste0(\"Count (n = \", ITERS, \")\"),\n    title = \"Posterior distribution of eggs laid per day\",\n    subtitle = \"Only Gryllodes sigillatus shows lower egg production\"\n  )\n\n\n\n\n\n\n\n\n\nAnd here we have it! Not particularly sexy, but we have posterior samples. So far, it looks like most of the species are going to have about the same distribution of eggs hatched per day. We can verify that assumption with a 95% posterior confidence interval.\n\n\nClick here for code\nci_eggs &lt;- exp(as_tibble(posterior_interval(model_eggs)))\ncolnames(ci_eggs) &lt;- c(\"lower\", \"upper\")\nint_vals &lt;- unname(as_vector(slice_head(ci_eggs, n = 1)))\nci_eggs[2:4,1] &lt;- ci_eggs[2:4,1] + int_vals[1]\nci_eggs[2:4,2] &lt;- ci_eggs[2:4,2] + int_vals[2]\nci_eggs$species &lt;- sort(unique(growth$species))\n\nci_eggs &lt;- ci_eggs |&gt;\n  select(species, lower, upper) |&gt;\n  mutate(lower = round(lower, 2), upper = round(upper, 2))\n\ngt(ci_eggs) |&gt;\n  tab_header(md(\"**95% Posterior CI: eggs hatched per day**\")) |&gt;\n  cols_label(\n    species = \"Species\",\n    lower = \"Lower\",\n    upper = \"Upper\"\n  )\n\n\n\n\n\n\n\n\n95% Posterior CI: eggs hatched per day\n\n\nSpecies\nLower\nUpper\n\n\n\n\nGryllodes sigillatus\n67.79\n73.84\n\n\nGryllus madagascariensis\n68.82\n75.01\n\n\nTeleogryllus afer\n68.79\n74.97\n\n\nTeleogryllus lemur\n68.79\n74.97\n\n\n\n\n\n\n\nAs we assumed, our posterior confidence intervals are extremely close across all four species (even identical when rounded to 2 significant figures). There is a small dip for Gryllodes sigillatus, but not an appreciable one. Hopefully we get better results when we build our second model and combine them."
  },
  {
    "objectID": "projects/crickets/crickets.html#growth-rate-model",
    "href": "projects/crickets/crickets.html#growth-rate-model",
    "title": "Cricket Crackers: A Bayesian Approach",
    "section": "Growth rate model",
    "text": "Growth rate model\nNow that we have an idea of how many eggs each mating pair will produce for each species, it’s time to model how much those newly-hatched crickets will grow every day. Let’s sample our growth data to see what we’re working with.\n\n\nClick here for code\ngrowth |&gt;\n  select(-c(temperature_c, mortality_y_n)) |&gt;\n  slice_sample(n = 5) |&gt;\n  gt() |&gt;\n  tab_header(md(\"**Random sample: growth dataset**\"))\n\n\n\n\n\n\n\n\nRandom sample: growth dataset\n\n\nspecies\nindividuals\ndate_of_hatching\ndate_of_molting\nstage\ndate_of_measurement\nday_weekly\nbody_length_mm\nbody_weight_g\n\n\n\n\nGryllodes sigillatus\n77\n2023-11-08\n18/12/23\nL2\n2023-11-29\n21\n6.0\n0.016\n\n\nGryllodes sigillatus\n57\n2023-11-08\n23/11/23\nL1\n2023-11-15\n7\n2.5\n0.001\n\n\nGryllus madagascariensis\n131\n2023-11-30\n20/12/23\nL1\n2023-12-07\n7\n3.5\n0.001\n\n\nTeleogryllus afer\n25\n2023-11-17\n24/11/23\nL1\n2023-11-24\n7\n3.0\n0.001\n\n\nGryllus madagascariensis\n67\n2023-11-30\n18/01/24\nL4\n2023-12-28\n28\n8.5\n0.045\n\n\n\n\n\n\n\nAgain, we run into an issue with a lack of useful predictors. Let’s talk about why. Although we don’t see them in this sample, there are five life stages in the data, L1 through L5. Each of these life stages strictly increases cricket size. After all, if they weren’t growing, there’s no reason to molt. So if we want to maximize our cricket weight output, we will always be harvesting the crickets at stage L5. Thus, to build our second model we will only use the data that is from stage L5, when the crickets are the largest they will ever be.\nAdditionally, because we are only concerned with the amount of cricket flour produced, the body length of each cricket is of no help. If a cricket is larger in length but smaller in weight, it ultimately produces less flour. Finally, the date columns don’t help us either, since our model will be time-agnostic. Observe that the time it takes for each species’ eggs to hatch only effects the “start-up” time of a new cricket farm. Once the first batch of eggs hatches, a farmer will produce new crickets every day. So time-to-hatch isn’t relevant in real-world scenarios unless some species take so much longer to hatch that there may be a financial cost to a delayed startup. Let’s quickly see if that’s the case by visualizing the distribution of hatch times by species.\n\n\nClick here for code\nrepro |&gt;\n  mutate(hatchdays = difftime(date_of_hatching, date_of_laying, \n                              units = \"days\")) |&gt;\n  ggplot(aes(x = hatchdays, fill = species)) +\n  geom_histogram(stat = \"count\") +\n  scale_fill_wsj() +\n  facet_wrap(~ species, nrow = 2) +\n  labs(\n    x = \"Number of Days for an Egg to Hatch\",\n    y = \"Count\",\n    title = \"Cricket hatch times\",\n    subtitle = \"The number of days between laying and hatching cricket eggs\"\n  ) +\n  theme(legend.position = \"none\", axis.title = element_text())\n\n\n\n\n\n\n\n\n\nThere are some modest differences, but probably not enough to impact cricket harvesting at scale. Thus, unfortunately, we are again limited to only using species as a predictor. But notice that the variable we are interested in isn’t in the data! If we want to measure growth rate, our response variable is the amount of weight gained by each cricket from hatching to life stage L5. Thus, we can manipulate our data to create this variable.\nGrouping by individual cricket and species, we can measure growth_time as the number of days between hatching and stage L5. Dividing its final weight by this number of days, we arrive at our response variable: average growth per day for each cricket/species pair! Let’s see if that new variable approximates a normal distribution to help us choose a prior.\n\n\nClick here for code\ngrowth_b &lt;- growth |&gt;\n  filter(day_weekly == 42) |&gt;\n  mutate(\n    growth_time = as.numeric(\n      difftime(date_of_measurement, date_of_hatching, units = \"days\")\n    )\n  ) |&gt;\n  select(body_weight_g, growth_time, species, individuals)\n\ngrowth_b &lt;- growth_b |&gt;\n  mutate(growth_rate = body_weight_g / growth_time) |&gt;\n  filter(!is.na(growth_rate))\n\nMEAN &lt;- mean(growth_b$growth_rate, na.rm = TRUE)\nSD &lt;- sd(growth_b$growth_rate, na.rm = TRUE)\n\ngrowth_b |&gt;\n  ggplot(aes(x = growth_rate)) +\n  geom_density(\n    aes(color = \"Observed distribution\"),\n    linewidth = 2,\n    color = \"darkgray\"\n  ) +\n  stat_function(\n    aes(color = \"Normal distribution\"),\n    fun = dnorm, \n    args = list(mean = MEAN, sd = SD), \n    linewidth = 2,\n    color = LIGHT_BLUE\n  ) +\n  annotate(\n    geom = \"text\",\n    label = \"Normal distribution\",\n    x = 0.015,\n    y = 60,\n    hjust = \"right\",\n    vjust = \"bottom\",\n    fontface = \"bold\",\n    color = LIGHT_BLUE\n  ) +\n  annotate(\n    geom = \"text\",\n    label = \"Observed distribution\",\n    x = 0.01,\n    y = 150,\n    hjust = \"right\",\n    vjust = \"bottom\",\n    fontface = \"bold\",\n    color = \"darkgray\"\n  ) +\n  labs(\n    x = \"Growth rate (per day)\",\n    title = \"Observed growth rates vs normal distribution\",\n    subtitle = paste(\"The observed growth rate is not similar enough to use a\",\n                     \"normal prior.\")\n  ) +\n  theme(axis.title.x = element_text())\n\n\n\n\n\n\n\n\n\nThis time, our response variable doesn’t well-approximate a normal distribution. To make our life easier, let’s just an un-informative prior this time. It’s modeling time!\n\n\nClick here for code\nmodel_growth &lt;- stan_glm(\n  growth_rate ~ species,\n  data = growth_b,\n  family = gaussian,\n  chains = 1,\n  iter = ITERS,\n  seed = 440,\n  refresh = 0\n)\n\n\nBecause our response variables are so small, and because no one is harvesting a small number of crickets in practice, let’s visualize our posterior after multiplying the results by 100. That will show us a distribution of growth rates for 100 crickets at a time. This is only for visualization purposes though: we will use per-cricket growth when we build our final results.\n\n\nClick here for code\ngrowth_tib &lt;- as_tibble(model_growth)[,1:4]\ncolnames(growth_tib) &lt;- c(\"a\", \"b\", \"c\", \"d\")\ngrowth_tib &lt;- growth_tib |&gt;\n  mutate(\n    b = b + a,\n    c = c + a,\n    d = d + a\n  )\ncolnames(growth_tib) &lt;- sort(unique(growth$species))\n\ngrowth_tib |&gt;\n  pivot_longer(cols = everything(), names_to = \"species\", values_to = \"rate\") |&gt;\n  ggplot(aes(x = rate*100, fill = species)) +\n  geom_histogram(bins = 100) +\n  scale_fill_wsj() +\n  facet_wrap(~species) +\n  theme(legend.position = \"none\", axis.title.x = element_text()) +\n  labs(\n    x = \"Growth rate (per 100 crickets, per day)\",\n    y = paste0(\"Count (n = \", ITERS, \")\"),\n    title = \"Posterior daily growth rate\",\n    subtitle = paste(\n      \"Results are multiplied by 100 for ease of legibility. Model\",\n      \"coefficients \\nare per-cricket.\"\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\nClick here for code\nci_growth &lt;- as_tibble(posterior_interval(model_growth))[1:4,]\ncolnames(ci_growth) &lt;- c(\"lower\", \"upper\")\nint_vals &lt;- unname(as_vector(slice_head(ci_growth, n = 1)))\nci_growth[2:4,1] &lt;- ci_growth[2:4,1] + int_vals[1]\nci_growth[2:4,2] &lt;- ci_growth[2:4,2] + int_vals[2]\nci_growth$species &lt;- sort(unique(growth$species))\nci_growth &lt;- select(ci_growth, species, lower, upper) |&gt;\n  mutate(lower = round(lower, 4), upper = round(upper, 4))\n\ngt(ci_growth) |&gt;\n  tab_header(md(\"**Posterior 95% CI: per-cricket growth rate**\")) |&gt;\n  cols_label(\n    species = \"Species\",\n    lower = \"Lower bound\",\n    upper = \"Upper bound\"\n  )\n\n\n\n\n\n\n\n\nPosterior 95% CI: per-cricket growth rate\n\n\nSpecies\nLower bound\nUpper bound\n\n\n\n\nGryllodes sigillatus\n0.0009\n0.0016\n\n\nGryllus madagascariensis\n0.0126\n0.0144\n\n\nTeleogryllus afer\n0.0006\n0.0024\n\n\nTeleogryllus lemur\n0.0012\n0.0029\n\n\n\n\n\n\n\nWe seem to have a winner so far! The Gryllus madagascariensis are clearly the crickets with the highest per-day growth rate from stage L1 to L5. It is reasonable to expect that when we multiply our posterior sample from our two models together, Gryllus madagascariensis will be our winner. But there’s only one way to be sure, and that’s putting our final results together."
  },
  {
    "objectID": "projects/crickets/crickets.html#putting-the-models-together",
    "href": "projects/crickets/crickets.html#putting-the-models-together",
    "title": "Cricket Crackers: A Bayesian Approach",
    "section": "Putting the models together",
    "text": "Putting the models together\nSince we ran both models for exactly 5000 iterations, we can combine their results by simply multiplying them together row-wise. This has the effect of asking “for a given simulated number of eggs and a simulated growth rate, how much total weight will that many crickets generate that day?” That’s our research question, so let’s find out!\n\n\nClick here for code\nmelt_growth &lt;- growth_tib |&gt;\n  pivot_longer(cols = everything(), names_to = \"species\", values_to = \"growth\") \n\nmelt_eggs &lt;- eggs_tib |&gt;\n  pivot_longer(cols = everything(), names_to = \"species\", values_to = \"eggs\") \n\npost_meat_per_day &lt;- tibble(\n  species = melt_growth$species,\n  growth = melt_growth$growth,\n  eggs = melt_eggs$eggs,\n  meat_per_day = growth * eggs\n)\n\npost_meat_per_day |&gt;\n  ggplot(aes(x = meat_per_day, fill = species)) +\n  geom_histogram(bins = 50) +\n  scale_fill_wsj() +\n  facet_wrap(~species, nrow = 2) +\n  theme(legend.position = \"none\", axis.title.x = element_text()) +\n  labs(\n    x = \"Meat production per day\",\n    y = paste0(\"Count (n = \", ITERS, \")\"),\n    title = \"Posterior distribution of meat production per day\",\n    subtitle = \"Meat production values are shown per breeding pair.\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nClick here for code\nci_meat_per_day &lt;- post_meat_per_day %&gt;%\n  group_by(species) %&gt;%\n  summarise(\n    lower = round(quantile(meat_per_day, 0.025, na.rm = TRUE), 2),\n    upper = round(quantile(meat_per_day, 0.975, na.rm = TRUE), 2)\n  )\n\nci_table &lt;- left_join(ci_growth, ci_eggs, by = \"species\") |&gt;\n  mutate(\n    lower.z = ci_meat_per_day$lower,\n    upper.z = ci_meat_per_day$upper\n  ) |&gt;\n  gt(rowname_col = \"species\") |&gt;\n  tab_spanner(\n    label = \"Growth rate\\n(per-cricket)\", \n    columns = c(lower.x, upper.x)\n  ) |&gt;\n  tab_spanner(label = \"Eggs hatched\", columns = c(lower.y, upper.y)) |&gt;\n  tab_spanner(label = \"Meat per day\", columns = c(lower.z, upper.z)) |&gt;\n  cols_label(\n    lower.x = \"Lower\",\n    lower.y = \"Lower\",\n    lower.z = \"Lower\",\n    upper.x = \"Upper\",\n    upper.y = \"Upper\",\n    upper.z = \"Upper\"\n  ) |&gt;\n  tab_header(md(\"**Posterior 95% confidence intervals**\")) |&gt;\n  data_color(c(2,3,6,7), palette = \"#ededed\") |&gt;\n  tab_style(\n    locations = cells_body(columns = c(6,7)),\n    style = list(cell_text(weight = \"bold\"))\n  ) |&gt;\n  tab_options(\n    footnotes.padding = 3,\n    footnotes.font.size = 13\n  )\nci_table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosterior 95% confidence intervals\n\n\n\n\nGrowth rate (per-cricket)\n\n\nEggs hatched\n\n\nMeat per day\n\n\n\nLower\nUpper\nLower\nUpper\nLower\nUpper\n\n\n\n\nGryllodes sigillatus\n0.0009\n0.0016\n67.79\n73.84\n0.06\n0.12\n\n\nGryllus madagascariensis\n0.0126\n0.0144\n68.82\n75.01\n0.92\n1.03\n\n\nTeleogryllus afer\n0.0006\n0.0024\n68.79\n74.97\n0.08\n0.14\n\n\nTeleogryllus lemur\n0.0012\n0.0029\n68.79\n74.97\n0.12\n0.18"
  }
]