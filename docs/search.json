[
  {
    "objectID": "tutorials/math_stat_1.html",
    "href": "tutorials/math_stat_1.html",
    "title": "Welcome to Estimators! | Mathematical Statistics 1",
    "section": "",
    "text": "Say we have some data \\(\\mathbf{X}\\). It‚Äôs a vector, so just think of it like a list of \\(n\\) numbers. We want to learn something about how these data came to be. First, we will aggregate our data using a statistic.\n\n\n\n\n\n\nDefinition\n\n\n\nLet \\(X_1, \\cdots, X_n\\) be our data. A statistic is a function of that data. We will denote that statistic with \\(\\delta\\). Crucially, this function cannot contain anything that we don‚Äôt know. It is purely a function of known quantities.\n\n\n\n\nLet‚Äôs say that our data comes from a normal distribution (a ‚Äúbell curve‚Äù). We denote this with \\(X \\sim N(\\mu, \\sigma^2)\\), where \\(\\mu\\) is the mean of the distribution and \\(\\sigma^2\\) is the variance. Also, to make our life easier, say we know the variance \\(\\sigma^2\\). In practice, this will basically never be the case, but it will simplify our math for now.\nWe have infinitely many options for statistics that we can choose. For example, we could use \\(X_1\\) (that is, the first data point in our vector). While we leave some data on the table in that case, it is certainly a statistic since \\(\\delta = X_1\\) is a function of our data, and there are no unknowns.\nAlternatively, we could use the observed mean of our data. We will call it \\(\\overline{X}\\) (pronounced ‚Äú\\(X\\) bar‚Äù), and it is denoted with \\[\n\\delta(\\mathbf{X}) = \\overline{X} = \\frac{1}{n}\\sum_{i=1}^n X_i.\n\\]\nNotice that this is also a statistic! Although it looks much more complicated, we are still using our data and no unknowns. Here, \\(n\\) is the number of data points that we have, which we know. And we know every \\(X_i\\) because each is part of our data vector \\(\\mathbf{X}\\).\n\n\n\n\n\n\nNote\n\n\n\nConstants (e.g., 7) are also statistics, although no data are involved in the calculation. If it feels like you‚Äôre just guessing at random if you do this, you‚Äôre right.\n\n\nNow let‚Äôs look at an example of a function that is not a statistic: \\[\n\\delta(\\mathbf{X}) = T = \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}}.\n\\] This function will come back in future articles, but for now, recall that we said that we already know the variance \\(\\sigma^2\\). So that means we already know \\(\\sigma\\). We also know \\(n\\), as we mentioned earlier. But \\(\\mu\\) is unknown to us. Because we have an unknown value \\(\\mu\\) in the numerator, \\(T\\) is not a statistic."
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Here is a place to browse my tutorials! Feel free to browse or sort by category on the right side of the page.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to Estimators! | Mathematical Statistics 1\n\n\n\nStatistics\n\n\n\n\n\n\n\nMitch Harrison\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "This is where my projects will be listed‚Ä¶ soon!\n\n\n\n\n\n\n\n\n\n\n\n\nDummy project\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs/demo_blog/demo_blog.html",
    "href": "blogs/demo_blog/demo_blog.html",
    "title": "Demo blog post",
    "section": "",
    "text": "This is where I may do some writing."
  },
  {
    "objectID": "blogs/demo_blog/demo_blog.html#demo-blog",
    "href": "blogs/demo_blog/demo_blog.html#demo-blog",
    "title": "Demo blog post",
    "section": "",
    "text": "This is where I may do some writing."
  },
  {
    "objectID": "projects/dummy_project/dummy_project.html",
    "href": "projects/dummy_project/dummy_project.html",
    "title": "Dummy project",
    "section": "",
    "text": "Here is a sample code block!\n\n\nClick here for code\nlibrary(tidyverse)\ndata &lt;- read_csv(paste0(getwd(), \"/data/data.csv\"))\ndata |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_line(color = \"red\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I‚Äôm Mitch. üëã",
    "section": "",
    "text": "After leaving the Navy in 2022, I moved to North Carolina to study data science and political science at Duke University. Finishing my degree cost the military about $60,000 in aid and depleted my savings. Even my parents, who probably thought they could start spending their ‚Äúput their kid through college‚Äù fund on fun things, had to chip in to help. My education was world-class but utterly unattainable for the majority of us. This website is my attempt to turn my course notes, homework, and projects into comprehensible articles for free. If nothing else, future Duke students struggling through their data science degree can look here for a different perspective.\nIf you want my personal, non-academic analysis projects, head to the Projects tab. If you are here to learn, the Tutorials tab is for you! And if you want to sit back and read some plain-language blog posts, there‚Äôs a Blog tab just for you. Of course, if you are curious about the website‚Äôs structure, it is built with R and Quarto, and the code is available by clicking on the GitHub icon at the bottom-left of every page (or click here).\nAccess to information should always be free, so every article here is, and always will be, at no cost. If you want to show financial support, you can buy me a coffee! But I won‚Äôt ever make donor-exclusive educational content, so don‚Äôt feel like you‚Äôre missing out by not donating. It‚Äôs just one way to show thanks.\nI hope you enjoy the site, and feel free to reach out via GitHub issues to make suggestions for articles. Thanks for reading!"
  },
  {
    "objectID": "index.html#the-about-me-page.",
    "href": "index.html#the-about-me-page.",
    "title": "About me",
    "section": "",
    "text": "This is where readers will see information about me, Mitch Harrison."
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blog",
    "section": "",
    "text": "We can browse blog posts here. For example, will take you do a dummy blog post full of nothing (yet).\n\n\n\n\n\n\n\n\n\n\n\n\nDemo blog post\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/math_stat_1.html#definition",
    "href": "tutorials/math_stat_1.html#definition",
    "title": "Welcome to Estimators | Mathematical Statistics 1",
    "section": "",
    "text": "This is a note block that you can use to highlight important information."
  },
  {
    "objectID": "tutorials/math_stat_1.html#statistics",
    "href": "tutorials/math_stat_1.html#statistics",
    "title": "Welcome to Estimators! | Mathematical Statistics 1",
    "section": "",
    "text": "Let‚Äôs say that our data comes from a normal distribution (a ‚Äúbell curve‚Äù). We denote this with \\(X \\sim N(\\mu, \\sigma^2)\\), where \\(\\mu\\) is the mean of the distribution and \\(\\sigma^2\\) is the variance. Also, to make our life easier, say we know the variance \\(\\sigma^2\\). In practice, this will basically never be the case, but it will simplify our math for now.\nWe have infinitely many options for statistics that we can choose. For example, we could use \\(X_1\\) (that is, the first data point in our vector). While we leave some data on the table in that case, it is certainly a statistic since \\(\\delta = X_1\\) is a function of our data, and there are no unknowns.\nAlternatively, we could use the observed mean of our data. We will call it \\(\\overline{X}\\) (pronounced ‚Äú\\(X\\) bar‚Äù), and it is denoted with \\[\n\\delta(\\mathbf{X}) = \\overline{X} = \\frac{1}{n}\\sum_{i=1}^n X_i.\n\\]\nNotice that this is also a statistic! Although it looks much more complicated, we are still using our data and no unknowns. Here, \\(n\\) is the number of data points that we have, which we know. And we know every \\(X_i\\) because each is part of our data vector \\(\\mathbf{X}\\).\n\n\n\n\n\n\nNote\n\n\n\nConstants (e.g., 7) are also statistics, although no data are involved in the calculation. If it feels like you‚Äôre just guessing at random if you do this, you‚Äôre right.\n\n\nNow let‚Äôs look at an example of a function that is not a statistic: \\[\n\\delta(\\mathbf{X}) = T = \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}}.\n\\] This function will come back in future articles, but for now, recall that we said that we already know the variance \\(\\sigma^2\\). So that means we already know \\(\\sigma\\). We also know \\(n\\), as we mentioned earlier. But \\(\\mu\\) is unknown to us. Because we have an unknown value \\(\\mu\\) in the numerator, \\(T\\) is not a statistic."
  },
  {
    "objectID": "tutorials/math_stat_1.html#point-estimator",
    "href": "tutorials/math_stat_1.html#point-estimator",
    "title": "Welcome to Estimators | Mathematical Statistics 1",
    "section": "Point estimator",
    "text": "Point estimator"
  },
  {
    "objectID": "tutorials/math_stat_1.html#point-estimator-example",
    "href": "tutorials/math_stat_1.html#point-estimator-example",
    "title": "Welcome to Estimators! | Mathematical Statistics 1",
    "section": "Point estimator example",
    "text": "Point estimator example\nLet‚Äôs keep going with our data, which comes from a normal distribution. But, to get used to using \\(\\theta\\), say that \\(X \\sim N(\\theta, \\sigma^2)\\). One possible estimator is the example mean \\(\\overline{X}\\) from earlier (i.e., the mean of the observed data). Alternatively, we can use a constant: say 5. Intuitively, it feels like \\(\\hat{\\theta} = \\overline{X}\\) would be a better guess than a simple \\(\\hat{\\theta} = 5\\), because it is actually informed by the data. But how do we quantify that intuition? We will calculate and compare both bias and precision for each.\n\nBias\nBias tells us how often, on average, we get the correct value of our unknown parameter \\(\\theta\\). Mathematically, we hope that the following quantity is as small as possible: \\[\n\\mathbb{E}[\\delta(\\mathbf{X}) | \\theta] - \\theta.\n\\]\nThe confusing-looking term \\(\\mathbb{E}[\\cdot]\\) is the expected value of our estimator, given the value of the unknown parameter \\(\\theta\\). Basically, this is the expected value of \\(\\hat{\\theta}\\). If our estimator \\(\\hat{\\theta}\\) is expected to be exactly correct on average, then this whole term will be 0, which is the smallest possible bias.\n\n\nVariance\nVariance describes the variability of our estimator. Ideally, variance is also small. Intuitively we are less ‚Äúsure‚Äù about our estimate if we have a wider variance. We denote variance with \\(Var(\\delta(\\mathbf{X})|\\theta)\\).\nHowever, notice that both bias and variance are conditional on the true value of our unknown parameter \\(\\theta\\). Thus, we cannot calculate these quantities directly. To deal with this, we will introduce the concept of loss in the next article!"
  }
]