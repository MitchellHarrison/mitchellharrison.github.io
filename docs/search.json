[
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Here is a place to browse my tutorials! Feel free to browse or sort by category on the right side of the page. If you have any questions or ideas for new topics, let me know on Discord!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHello, statistics. | Mathematical Statistics 0\n\n\n\nStatistics\n\n\nMathematical Statistics\n\n\n\n\n\n\n\nMitch Harrison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to Estimators! | Mathematical Statistics 1\n\n\n\nStatistics\n\n\nMathematical Statistics\n\n\n\n\n\n\n\nMitch Harrison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Bias-Variance Tradeoff | Mathematical Statistics 2\n\n\n\nStatistics\n\n\nMathematical Statistics\n\n\n\n\n\n\n\nMitch Harrison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur World in Emissions | TidyTutorial\n\n\n\nData Viz\n\n\n\n\n\n\n\nMitch Harrison\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/math_stat_2.html",
    "href": "tutorials/math_stat_2.html",
    "title": "The Bias-Variance Tradeoff | Mathematical Statistics 2",
    "section": "",
    "text": "Last time, we noted that we cannot precisely calculate bias or variance since they are conditional on the parameter we are estimating in the first place. If we knew that value, we wouldn’t need an estimator. Ideally, our estimators would be as close as possible to the true value of our unknown parameter \\(\\theta\\), but there may be infinitely many possible values for \\(\\theta\\). Instead, we will seek to be close to a range of possible values of \\(\\theta\\).\n\n\n\n\n\n\nDefinition\n\n\n\nCloseness describes how much we are willing to “pay” to be some “distance” away from the true value of \\(\\theta\\). We will measure this distance with a function (called a loss function), which is denoted as:\n\\[\n\\ell(\\theta, a).\n\\] This notation is somewhat confusing, because we are now using \\(a\\) to denote our estimate instead of \\(\\hat{\\theta}\\). So you can think of it as \\(\\ell(\\theta, \\hat{\\theta})\\) if that is more helpful (as it is for me).\n\n\nAs an example, if we choose a loss function,\n\\[\n\\ell(\\theta, \\hat{\\theta}) = (\\theta - \\hat{\\theta})^2,\n\\] then we have arrived at the squared-error loss function. Observe that the difference between our guess and the true value is the error, and we are squaring that value. If we take the mean of that function, we arrive at a critical value in mathematical statistics that you may have heard of: the mean-squared error (MSE).\n\n\n\n\n\n\nDefinition\n\n\n\nRisk is the expected loss for a given loss function \\(\\ell\\). Mathematically, it is denoted as \\[\nR_{\\delta}(\\theta) = \\mathbb{E}_{X|\\theta}[\\ell(\\theta, \\delta(\\mathbf{X}))],\n\\] where \\(\\delta(\\mathbf{X})\\) is the function that gives us our estimator. In effect, the risk is the expected loss given some loss function \\(\\ell(\\theta,\\hat{\\theta})\\).\n\n\nWe will get back to loss next time."
  },
  {
    "objectID": "tutorials/math_stat_2.html#worked-example-bias",
    "href": "tutorials/math_stat_2.html#worked-example-bias",
    "title": "The Bias-Variance Tradeoff | Mathematical Statistics 2",
    "section": "Worked example: bias",
    "text": "Worked example: bias\nLet’s work on our first example problem to nail some things down. Let our data \\(X_1, \\cdots, X_n \\sim N(\\mu, \\sigma^2)\\), and let each data point be independent and identically distributed (iid). Like last time, \\(\\sigma^2\\) is fixed and known. We are interested in estimating \\(\\mu\\). Recall our two estimators from the previous article:\n\\[\n\\begin{align*}\n\\delta_1(\\mathbf{X}) &= \\overline{X} \\\\\n\\delta_2(\\mathbf{X}) &= 5.\n\\end{align*}\n\\]\nThe expected value of \\(\\delta_2\\) is 5. Let’s find the expected value of \\(\\delta_1\\). Recall our generic formula for the expectation of an estimator is \\(\\mathbb{E}(\\delta(\\mathbf{X}) | \\theta)\\) for some estimator \\(\\delta\\) and true value \\(\\theta\\). In this case, our estimator is \\(\\overline{X}\\) and our parameter is \\(\\mu\\). Thus: \\[\n\\begin{align*}\n\\mathbb{E}(\\delta_1(\\mathbf{X})|\\theta) &= \\mathbb{E}(\\overline{X} | \\mu) \\\\\n&= \\mathbb{E}\\left[\\frac{\\sum_{i=1}^nX_i}{n}\\right]\n& \\text{expanding out }\\overline{X} \\\\\n&= \\frac{1}{n}\\sum_{i=1}^n\\mathbb{E}(X_i|\\mu)\n& \\text{expectation can be distributed into sums} \\\\\n&= \\frac{1}{n}\\sum_{i=1}^n\\mu\n& \\text{because } \\mathbb{E}(X_i) = \\mu \\text{ for any }i\\\\\n&= \\frac{n\\mu}{n} \\\\\n&= \\mu\n\\end{align*}\n\\]\nWe have a bias of zero! That means that the expected value of our estimator is precisely equivalent to our unknown parameter. It feels like we have solved statistics, right? Well… no. But we will get to that in a moment. For now, let’s visualize bias.\nLet’s say that the true (but unknown to us) value of \\(\\mu\\) is 1. One of our estimators was the constant 5. If we let 5 be our guess for the mean, we are saying, “I think the distribution is centered on 5,” when in reality, it is centered on 1. That difference is shown below.\n\n\nClick here for code\nlibrary(tidyverse)\n\nSIGMA &lt;- 1 # this is our fixed and known standard deviation\nMU &lt;- 1 # this is our true but unknown value\nMU_HAT &lt;- 5 # this is our estimator\n\nggplot() + \n  \n  # plot the true distribution\n  stat_function(\n    fun = dnorm,\n    args = list(mean = MU, sd = SIGMA),\n    color = \"coral3\",\n    linewidth = 2\n  ) +\n  geom_vline(xintercept = MU, color = \"coral3\", size = 1) +\n  annotate(\n    geom = \"text\",\n    hjust = \"right\",\n    x = -.1,\n    y = .3,\n    label = latex2exp::TeX(\"$N(1, \\\\sigma^2)$\"),\n    color = \"coral3\"\n  ) +\n  \n  # plot the estimated distribution\n  stat_function(\n    fun = dnorm,\n    args = list(mean = MU_HAT, sd = SIGMA),\n    color = \"cyan4\",\n    linewidth = 2\n  ) +\n  geom_vline(xintercept = MU_HAT, color = \"cyan4\", size = 1) +\n  annotate(\n    geom = \"text\",\n    hjust = \"right\",\n    x = 6.8,\n    y = .3,\n    label = latex2exp::TeX(\"$N(5, \\\\sigma^2)$\"),\n    color = \"cyan4\"\n  ) +\n  \n  # annotation arrow\n  annotate(\n    geom = \"segment\",\n    x = MU,\n    xend = MU_HAT,\n    y = .19,\n    yend = .19,\n    arrow = arrow(length = unit(0.3, \"cm\"), ends = \"both\", type = \"closed\")\n  ) +\n  annotate(\n    geom = \"text\",\n    x = 3,\n    y = 0.2,\n    label = \"bias\",\n    fontface = \"bold\"\n  ) +\n  \n  # aesthetic fixes\n  theme_minimal() +\n  labs(x = latex2exp::TeX(\"\\\\mu\")) +\n  scale_x_continuous(limits = c(-2, 8), breaks = -3:8) +\n  theme(\n    axis.title.y = element_blank(),\n    axis.line.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank(),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    axis.line.x = element_line()\n  )\n\n\n\n\n\n\n\n\n\nAs we can see, the smaller our bias, the closer our guess becomes to the true value of our parameter. We can also see that a bias of zero is ideal. But there is more to an estimator than bias. Let’s move on to variance."
  },
  {
    "objectID": "tutorials/math_stat_2.html#worked-example-variance",
    "href": "tutorials/math_stat_2.html#worked-example-variance",
    "title": "The Bias-Variance Tradeoff | Mathematical Statistics 2",
    "section": "Worked example: variance",
    "text": "Worked example: variance\nFormally, the variance of an estimator is the expected value of the squared sampling deviations. But that explanation might not be conceptually helpful. So, let’s see an example.\nSay a city has a population of 1,000,000. We want to get the average height of everyone in town. So we sample 1,000 people, measure their height, and use some function to calculate an estimator. We have used the same two estimators so far, \\(\\delta_1 = \\overline{X}\\) and \\(\\delta_2 = 5\\), so we will continue with those.\nDepending on the height of everyone in our 1,000-person sample, we will get a different \\(\\overline{X}\\) each time we take a sample. If we take 100 samples of 1,000 people per sample, we will (probably) arrive at 100 distinct values of \\(\\overline{X}\\). But if we use 5 as our estimator (perhaps to mean 5 feet tall), there will be no such variation in our estimator.\nThe measure of how much our estimator fluctuates as we take more and more samples is a helpful, intuitive understanding of variance. Ideally, we want to have as little variance as possible. We know our estimator has no variance if we use a constant (a number is always just that number), but what about \\(\\overline{X}\\)? Let’s calculate.\n\\[\n\\begin{align*}\nVar[\\delta_1(\\mathbf{X})|\\mu] &= Var(\\overline{X}) \\\\\n&= Var\\left[\\frac{1}{n}\\sum_{i=1}^nX_i\\right]\n& \\text{expanding out } \\overline{X} \\\\\n&= \\frac{1}{n^2}\\sum_{i=1}^nVar(X_i)\n& \\text{must square constants when factoring out of variance} \\\\\n&= \\frac{1}{n^2}\\sum_{i=1}^n\\sigma^2\n& Var(X_i) = \\sigma^2 \\text{ is fixed and known} \\\\\n&= \\frac{n\\sigma^2}{n^2} \\\\\n&= \\frac{\\sigma^2}{n}\n\\end{align*}\n\\]\nNotice that this value is a positive number, which means the variance of \\(\\delta_1 = \\overline{X}\\) is higher than the variance of \\(\\delta_2 = 5\\), which is zero."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "These are my independent projects! They can be quick visualizations, full analyses with write-ups, and even interactive data exploration dashboards. Feel free to use the tags on the right side of the page to navigate, and reach out to me on Discord with any questions. Thanks!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur World in Emissions | TidyTuesday\n\n\n\nData Viz\n\n\nTidyTuesday\n\n\n\n\n\n\n\nMitch Harrison\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/math_stat_2.html#the-tradeoff",
    "href": "tutorials/math_stat_2.html#the-tradeoff",
    "title": "The Bias-Variance Tradeoff | Mathematical Statistics 2",
    "section": "The Tradeoff",
    "text": "The Tradeoff\nThis phenomenon, where one estimator has better bias and the other has better variance, is an example of a central balancing act that statisticians have to perform: the bias-variance tradeoff. We have seen an estimator with no bias but positive variance (\\(\\overline{X}\\)) and another with no variance but non-zero bias (5). How do we know which is better? We will explore that question i n the following article when we explore the MSE more deeply."
  }
]