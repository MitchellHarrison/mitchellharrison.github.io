---
title: "Sufficiency | Mathematical Statistics 4"
author: "Mitch Harrison"
categories:
  - "Statistics"
  - "Mathematical Statistics"
image: "../images/thumbnails/mathematical_statistics/4.png"
---

# Efficiency

Now that we are getting comfortable with estimators and bias, we can develop
some intuition. Say we are estimating the mean of a normal distribution like we 
did a few articles ago. Like last time, we will let the variance $\sigma^2$ be 
known. We have seen two different unbiased estimators before:

\begin{align*}
\hat{\theta}_1 &= \overline{X} \\
\hat{\theta}_2 &= X_1
\end{align*}

Intuitively, it feels like $\hat{\theta}_1$ would be a better estimator because 
it uses more of the data. We have previously shown that both of these estimators 
are unbiased, but $Var(\hat{\theta}_1) < Var(\hat{\theta}_2)$. Because the 
biases are equivalent (zero), we say that the estimator with a smaller variance 
is more **efficient**.

# Sufficiency

:::{.callout-important}
### Definition

**Sufficiency**: Let $T = T(\mathbf{X})$ be a statistic where $X_i$ is a random
sample from a distribution with an unknown parameter $\theta$. $T$ is a
**sufficient statistic** for this distribution if $\mathbb{P}(\mathbf{X}|T)$
does not depend on $\theta$. In other words,
$$
\mathbb{P}(\mathbf{X} | T, \theta) = \mathbb{P}(\mathbf{X} | T).
$$
:::

It turns out, the reason that $\hat{\theta}_2$ felt like a worse estimator than
$\hat{\theta}_1$ is because $\hat{\theta}_2$ is insufficient and
$\hat{\theta}_1$ *is* sufficient.

## Example

Let $\mathbf{X} \overset{\mathrm{iid}}{\sim} Poisson(\theta)$. Because the sum
of Poisson random variables is also Poisson (if you didn't know this, it's a 
helpful trick to memorize), we can let

$$
T = \sum_{i=1}^n X_i \sim Poisson(n\theta).
$$

be our statistic. For an individual $X_i$, we know (from memory or from
[Wikipedia](https://en.wikipedia.org/wiki/Poisson_distribution)) that the PDF is

$$
\mathbb{P}(X_i= x|\theta) = \prod_{i=1}^n \frac{exp(-\theta)\theta^{x_i}}{x_i!}.
$$

We can then find $\mathbb{P}_T(\cdot)$, replacing $\theta$ with $n\theta$ and
$x_i$ with $t$, because $T$ follows a $Poisson(n\theta)$ instead of a 
$Poisson(\theta)$ distribution. Then, we arrive at our PDF:

$$
\mathbb{P}_T(T=t|\theta) = \frac{exp\{-n\theta\}(n\theta)^t}{t!}.
$$

When we solve for 